{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "222cac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import eval_metrics as em\n",
    "import wandb\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89eddae",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "006b2c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/users1/liqe/TeamLab_phonetics/TeamLab/wandb/run-20250705_172759-ushjesg5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake/runs/ushjesg5' target=\"_blank\">Training_08_A01</a></strong> to <a href='https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake' target=\"_blank\">https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake/runs/ushjesg5' target=\"_blank\">https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake/runs/ushjesg5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project = \"teamlab_deepfake\",\n",
    "    name = \"Training_08_A01\",     #Training_XX\n",
    "    notes = None,\n",
    "    tags = [\"ALL_FEATURE\", \"COMBINED_MODEL\", \"HNR\", \"PITCH\", \"JITTER&SHIMMER\", \"MFCC\"],\n",
    "    config={\n",
    "        #NOTE: set manually\n",
    "        \"model\": \"SpoofEnsemble\",   #   SpoofEnsemble/LSTM_FFN_classifier/CNN_classifier/SpoofEnsemble_attention\n",
    "        \"dataset\": \"ASVSpoof19_LA\",    \n",
    "        \"feature\": \"MFCC&Prosody\",\n",
    "        \"attack_type\": \"A01\",   # all/A01/A02/A03/A04/A05/A06\n",
    "        \"loss_function\": \"weighted_CE\",\n",
    "        #\n",
    "        \"scheduler\": False,\n",
    "        \"scheduler_factor\": 0.5,\n",
    "        \"scheduler_patience\": 4,\n",
    "        \"epochs\": 70,\n",
    "        \"batch_size\": 32,\n",
    "        \"oversampling\": True,\n",
    "        \"learning_rate\": 5e-4,\n",
    "        \"dropout_rate\": 0.3,\n",
    "        # lstm layer\n",
    "        \"lstm_input_dim\": 2,\n",
    "        \"lstm_hidden_dim\": 64,\n",
    "        \"bidirectional\": False,\n",
    "        \"lstm_n_layers\":1,\n",
    "        # fnn layer\n",
    "        \"ffn_dims\": [11, 64], # in, out -\n",
    "        # cnn layer\n",
    "        \"cnn_channels\": [1, 32, 64, 128],   #in, out -\n",
    "        \"conv_kernel\": (3,3),\n",
    "        \"pool_kernel\": (2,2),\n",
    "        \"cnn_padding\": 1,\n",
    "        # random seeds\n",
    "        \"seeds\": [0,7,42]\n",
    "    },\n",
    ")\n",
    "\n",
    "config = run.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970b71e6",
   "metadata": {},
   "source": [
    ">NOTE: attack types are evenly distributed in training and dev dataset, and each has higher number than genuine voices, so no further balancing is needed>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "621b14ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       AUDIO_ID  LABEL ATTACK_TYPE  \\\n",
      "1  LA_T_1000406      1           -   \n",
      "2  LA_T_1000648      0         A01   \n",
      "6  LA_T_1001169      0         A01   \n",
      "7  LA_T_1001718      0         A01   \n",
      "9  LA_T_1002656      0         A01   \n",
      "\n",
      "                                               PITCH  \\\n",
      "1  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "2  [nan, nan, nan, nan, nan, 0.35835335, 0.350411...   \n",
      "6  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "7  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "9  [nan, nan, nan, nan, nan, 0.3446952, 0.3116533...   \n",
      "\n",
      "                                                 HNR  \\\n",
      "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7987432, 0.79...   \n",
      "6  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "7  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "9  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7937161, 0.81...   \n",
      "\n",
      "                                              JITTER  \\\n",
      "1  [0.1494679, 0.09972349, 0.086890295, 0.0791289...   \n",
      "2  [0.13972145, 0.05122372, 0.123514704, 0.096691...   \n",
      "6  [0.11878602, 0.047274902, 0.10791813, 0.076425...   \n",
      "7  [0.13606435, 0.049893107, 0.13377233, 0.102136...   \n",
      "9  [0.120143734, 0.054571845, 0.10887862, 0.09621...   \n",
      "\n",
      "                                             SHIMMER  \\\n",
      "1  [0.13012205, 0.15872449, 0.059600886, 0.081321...   \n",
      "2  [0.28141773, 0.3631905, 0.12680757, 0.17373542...   \n",
      "6  [0.2015925, 0.28466532, 0.07975244, 0.10248045...   \n",
      "7  [0.2511304, 0.27908075, 0.10077607, 0.14693537...   \n",
      "9  [0.2601562, 0.33056188, 0.12206272, 0.16642591...   \n",
      "\n",
      "                                                MFCC  \n",
      "1  [[0.30532825, 0.3215196, 0.31294125, 0.3068853...  \n",
      "2  [[0.22124906, 0.34767622, 0.5816188, 0.6628496...  \n",
      "6  [[0.2689295, 0.26384014, 0.2554089, 0.2559836,...  \n",
      "7  [[0.2070771, 0.20564632, 0.20497796, 0.2006736...  \n",
      "9  [[0.2583435, 0.29383314, 0.45285627, 0.5424404...  \n",
      "             AUDIO_ID  LABEL  PITCH   HNR  JITTER  SHIMMER  MFCC\n",
      "ATTACK_TYPE                                                     \n",
      "-                2580   2580   2580  2580    2580     2580  2580\n",
      "A01              3800   3800   3800  3800    3800     3800  3800\n",
      "\n",
      "\n",
      "        AUDIO_ID  LABEL ATTACK_TYPE  \\\n",
      "0   LA_D_7341689      1           -   \n",
      "2   LA_D_1275044      1           -   \n",
      "5   LA_D_4673595      0         A01   \n",
      "7   LA_D_7594804      0         A01   \n",
      "10  LA_D_2417193      1           -   \n",
      "\n",
      "                                                PITCH  \\\n",
      "0   [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "2   [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "5   [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "7   [nan, nan, nan, nan, nan, nan, nan, 0.31156045...   \n",
      "10  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "\n",
      "                                                  HNR  \\\n",
      "0   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "2   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "5   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "7   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.78905135...   \n",
      "10  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "\n",
      "                                               JITTER  \\\n",
      "0   [0.13253999, 0.056896314, 0.11020656, 0.075886...   \n",
      "2   [0.11645243, 0.0571051, 0.1166473, 0.08530104,...   \n",
      "5   [0.14577158, 0.065038234, 0.12784907, 0.103999...   \n",
      "7   [0.117040604, 0.0451014, 0.11148719, 0.0901735...   \n",
      "10  [0.15527266, 0.057062894, 0.1497324, 0.1076965...   \n",
      "\n",
      "                                              SHIMMER  \\\n",
      "0   [0.14911835, 0.2014152, 0.06324554, 0.08393805...   \n",
      "2   [0.1140816, 0.15433928, 0.0761387, 0.08161701,...   \n",
      "5   [0.2596176, 0.29413572, 0.11952713, 0.16643901...   \n",
      "7   [0.22324513, 0.28933012, 0.09750613, 0.1201312...   \n",
      "10  [0.107291475, 0.16611362, 0.075242005, 0.07916...   \n",
      "\n",
      "                                                 MFCC  \n",
      "0   [[0.22630103, 0.25814316, 0.26443368, 0.245855...  \n",
      "2   [[0.2550575, 0.27148598, 0.23670186, 0.2161811...  \n",
      "5   [[0.21984264, 0.4053417, 0.51509863, 0.5124076...  \n",
      "7   [[0.25146994, 0.40092656, 0.6281243, 0.7156452...  \n",
      "10  [[0.25216043, 0.2571961, 0.25724986, 0.2542430...  \n",
      "             AUDIO_ID  LABEL  PITCH   HNR  JITTER  SHIMMER  MFCC\n",
      "ATTACK_TYPE                                                     \n",
      "-                2548   2548   2548  2548    2548     2548  2548\n",
      "A01              3716   3716   3716  3716    3716     3716  3716\n"
     ]
    }
   ],
   "source": [
    "PITCH_COLUMN = 'PITCH'\n",
    "HNR_COLUMN = 'HNR'\n",
    "JITTER_COLUMN = 'JITTER'\n",
    "SHIMMER_COLUMN = 'SHIMMER'\n",
    "MFCC_COLUMN = 'MFCC'\n",
    "LABEL_COLUMN = 'LABEL'      \n",
    "                           \n",
    "NAN_REPLACEMENT_VALUE = 0.0  \n",
    "PADDING_VALUE = 0.0         \n",
    "LABEL_BONAFIDE = 1\n",
    "LABEL_SPOOF = 0\n",
    "\n",
    "train_features_path = '/home/users1/liqe/TeamLab_phonetics/merged_train_com.pkl'\n",
    "dev_features_path = '/home/users1/liqe/TeamLab_phonetics/merged_dev_com.pkl'\n",
    "\n",
    "df_train = pd.read_pickle(train_features_path)\n",
    "df_dev = pd.read_pickle(dev_features_path)\n",
    "\n",
    "# NOTE: if training on a specific attack type\n",
    "if config.attack_type != \"all\":\n",
    "    df_train = df_train[df_train['ATTACK_TYPE'].isin([config.attack_type,'-'])]\n",
    "    df_dev = df_dev[df_dev['ATTACK_TYPE'].isin([config.attack_type,'-'])]\n",
    "elif config.attack_type == \"all\":\n",
    "    pass\n",
    "elif config.attack_type != (\"A01\" or \"A02\" or \"A03\" or \"A04\" or \"A05\" or \"A06\"):\n",
    "    print(\"WARNING: invalid attack type.\")\n",
    "\n",
    "# inspect\n",
    "print(df_train.head())\n",
    "print(df_train.groupby('ATTACK_TYPE').count())\n",
    "\n",
    "print(\"\\n\")\n",
    "print(df_dev.head())\n",
    "print(df_dev.groupby('ATTACK_TYPE').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82fb4c6",
   "metadata": {},
   "source": [
    "#### Set the random seeds for replicability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5173b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def set_seed(seed):\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed_all(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     random.seed(seed)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "#     torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c9ec96",
   "metadata": {},
   "source": [
    "### Training Data Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f385ade",
   "metadata": {},
   "source": [
    ">NOTE: training audio & labels are matched, dev are not (Solved: excessive rows are deleted beforehand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d12dd51b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The target 'y' needs to have more than 1 class. Got 1 class instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m y \u001b[38;5;241m=\u001b[39m df_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLABEL\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m over \u001b[38;5;241m=\u001b[39m RandomOverSampler(random_state\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mseeds[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m----> 6\u001b[0m X_resampled_np, y_resampled_np \u001b[38;5;241m=\u001b[39m \u001b[43mover\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      8\u001b[0m X_resampled_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X_resampled_np, columns\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m      9\u001b[0m y_resampled_series \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(y_resampled_np, name\u001b[38;5;241m=\u001b[39my\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch0/lib/python3.11/site-packages/imblearn/base.py:202\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit_resample\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m    182\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03m        The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch0/lib/python3.11/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch0/lib/python3.11/site-packages/imblearn/base.py:101\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m     98\u001b[0m arrays_transformer \u001b[38;5;241m=\u001b[39m ArraysTransformer(X, y)\n\u001b[1;32m     99\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X_y(X, y)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_sampling_strategy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_strategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampling_type\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_resample(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    107\u001b[0m y_ \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m     label_binarize(output[\u001b[38;5;241m1\u001b[39m], classes\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39munique(y)) \u001b[38;5;28;01mif\u001b[39;00m binarize_y \u001b[38;5;28;01melse\u001b[39;00m output[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    109\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/pytorch0/lib/python3.11/site-packages/imblearn/utils/_validation.py:537\u001b[0m, in \u001b[0;36mcheck_sampling_strategy\u001b[0;34m(sampling_strategy, y, sampling_type, **kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msampling_type\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSAMPLING_KIND\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msampling_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    534\u001b[0m     )\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39munique(y)\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 537\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    538\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe target \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m needs to have more than 1 class. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    539\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39munique(y)\u001b[38;5;241m.\u001b[39msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m class instead\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    540\u001b[0m     )\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sampling_type \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensemble\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbypass\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sampling_strategy\n",
      "\u001b[0;31mValueError\u001b[0m: The target 'y' needs to have more than 1 class. Got 1 class instead"
     ]
    }
   ],
   "source": [
    "if config.oversampling:\n",
    "    X = df_train.drop('LABEL', axis=1)\n",
    "    y = df_train['LABEL']\n",
    "\n",
    "    over = RandomOverSampler(random_state=config.seeds[0])\n",
    "    X_resampled_np, y_resampled_np = over.fit_resample(X, y) \n",
    "\n",
    "    X_resampled_df = pd.DataFrame(X_resampled_np, columns=X.columns)\n",
    "    y_resampled_series = pd.Series(y_resampled_np, name=y.name)\n",
    "\n",
    "    print(\"\\nResampled X (DataFrame) head:\")\n",
    "    print(X_resampled_df.head())\n",
    "    print(\"\\nResampled y (Series) head:\")\n",
    "    print(y_resampled_series.head())\n",
    "    print(\"\\nResampled class distribution (from y_resampled_series):\")\n",
    "    print(Counter(y_resampled_series))\n",
    "\n",
    "    df_train = pd.concat([X_resampled_df, y_resampled_series], axis=1)\n",
    "\n",
    "    print(\"\\nCombined Resampled DataFrame head:\")\n",
    "    print(df_train.head())\n",
    "    print(\"\\nCombined Resampled DataFrame info:\")\n",
    "    df_train.info()\n",
    "    print(\"\\nCombined Resampled DataFrame class distribution:\")\n",
    "    print(Counter(df_train['LABEL'])) # Verify target column in the new DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057ec143",
   "metadata": {},
   "source": [
    "### Padding and Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbc2366",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASVDataset(Dataset):\n",
    "    def __init__(self, dataframe, pitch_col, hnr_col, jitter_col, shimmer_col, mfcc_col, label_col, nan_replacement=NAN_REPLACEMENT_VALUE):\n",
    "        \n",
    "        self.labels = []\n",
    "        self.processed_pitchhnr = []\n",
    "        self.global_features = []\n",
    "        self.processed_mfcc = []\n",
    "        \n",
    "        print(f\"Attempting to process {len(dataframe)} entries from DataFrame\")\n",
    "        found_count = 0\n",
    "        # Iterate through the DataFrame, process and pad the features\n",
    "        for index, row in dataframe.iterrows():  \n",
    "            if not np.isnan(row[label_col]):\n",
    "                self.labels.append(row[label_col])\n",
    "\n",
    "                pitch_sequence_raw = row[pitch_col]\n",
    "                processed_pitch = np.nan_to_num(pitch_sequence_raw, nan=nan_replacement)\n",
    "                \n",
    "                hnr_sequence_raw = row[hnr_col]\n",
    "                processed_hnr = np.nan_to_num(hnr_sequence_raw, nan=nan_replacement)\n",
    "\n",
    "                ### NOTE:need to pad the two sequences to the same length\n",
    "                max_length = max(len(processed_pitch), len(processed_hnr))\n",
    "                if len(processed_pitch) > len(processed_hnr):\n",
    "                    padding = np.zeros(max_length - len(processed_hnr), dtype=processed_hnr.dtype)\n",
    "                    processed_hnr = np.concatenate((processed_hnr, padding))\n",
    "                else:\n",
    "                    padding = np.zeros(max_length - len(processed_pitch), dtype=processed_pitch.dtype)\n",
    "                    processed_pitch = np.concatenate((processed_pitch, padding))\n",
    "\n",
    "                combined_features = np.stack((processed_pitch, processed_hnr), axis=-1) \n",
    "                self.processed_pitchhnr.append(torch.tensor(combined_features, dtype=torch.float32))\n",
    "\n",
    "                # process and combine jitter and shimmer to one sequence\n",
    "                processed_jitter = np.nan_to_num(row[jitter_col], nan=nan_replacement)\n",
    "                processed_shimmer = np.nan_to_num(row[shimmer_col], nan=nan_replacement)\n",
    "                jitter_shimmer = np.concatenate((processed_jitter, processed_shimmer))\n",
    "                self.global_features.append(torch.tensor(jitter_shimmer, dtype=torch.float32))\n",
    "                \n",
    "                # process mfcc\n",
    "                mfcc = row[mfcc_col]\n",
    "                # NOTE: need transpose for padding (time, feature_dim)\n",
    "                self.processed_mfcc.append(torch.tensor(mfcc, dtype=torch.float32).T)\n",
    "\n",
    "                found_count += 1\n",
    "        \n",
    "        self.labels = torch.tensor(self.labels, dtype=torch.long) \n",
    "        print(f\"Successfully processed {found_count} samples out of {len(dataframe)} DataFrame entries.\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of matched samples in the dataset.\"\"\"\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns one sample from the dataset: a preprocessed pitch sequence and its label.\n",
    "        \"\"\"\n",
    "        label = self.labels[idx]\n",
    "        pitch_hnr = self.processed_pitchhnr[idx]\n",
    "        global_feature = self.global_features[idx]\n",
    "        mfcc = self.processed_mfcc[idx]\n",
    "        return label, pitch_hnr, global_feature, mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f4a79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Custom Collate Function for Dynamic Padding  ---\n",
    "def collate_fn(batch, padding_value=PADDING_VALUE):\n",
    "    \"\"\"\n",
    "    Pads sequences within a batch to the same length.\n",
    "    \"\"\"\n",
    "    labels = [item[0] for item in batch]\n",
    "    pitch_hnrs = [item[1] for item in batch]\n",
    "    global_features = [item[2] for item in batch]\n",
    "    mfccs = [item[3] for item in batch]\n",
    "\n",
    "    labels = torch.stack(labels)\n",
    "\n",
    "    pitchhnr_lengths = torch.tensor([len(seq) for seq in pitch_hnrs], dtype=torch.long)\n",
    "    padded_pitchhnrs = pad_sequence(pitch_hnrs, batch_first=True, padding_value=padding_value)\n",
    "    if padded_pitchhnrs.ndim == 2:     # lstm expects: [batch_size, sequence_length, feature_size]\n",
    "        padded_pitchhnrs = padded_pitchhnrs.unsqueeze(2)\n",
    "\n",
    "    global_features = torch.stack(global_features)\n",
    "\n",
    "    padded_mfccs = pad_sequence(mfccs, batch_first=True, padding_value=padding_value)\n",
    "\n",
    "    return labels, pitchhnr_lengths, padded_pitchhnrs, global_features, padded_mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d92225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to process 45598 entries from DataFrame\n",
      "Successfully processed 45598 samples out of 45598 DataFrame entries.\n",
      "Attempting to process 24844 entries from DataFrame\n",
      "Successfully processed 24844 samples out of 24844 DataFrame entries.\n",
      "\n",
      "--- Batch 1 ---\n",
      "  Labels (first 5): tensor([1, 1, 0, 0, 1])\n",
      "  Padded Sequences Shape: torch.Size([32, 495, 2])\n",
      "  Original Lengths (first 5): tensor([168, 369, 403, 260, 347])\n",
      "  Global Shape: torch.Size([32, 11])\n",
      "  MFCC Shape: torch.Size([32, 156, 60])\n"
     ]
    }
   ],
   "source": [
    "pitch_dataset_train = ASVDataset(dataframe=df_train,   \n",
    "                                    pitch_col=PITCH_COLUMN,\n",
    "                                    hnr_col=HNR_COLUMN,\n",
    "                                    jitter_col=JITTER_COLUMN,\n",
    "                                    shimmer_col=SHIMMER_COLUMN,\n",
    "                                    mfcc_col=MFCC_COLUMN,\n",
    "                                    label_col=LABEL_COLUMN,\n",
    "                                    nan_replacement=NAN_REPLACEMENT_VALUE)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    pitch_dataset_train, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn, num_workers=8\n",
    ")\n",
    "\n",
    "pitch_dataset_dev = ASVDataset(dataframe=df_dev,   \n",
    "                                    pitch_col=PITCH_COLUMN,\n",
    "                                    hnr_col=HNR_COLUMN,\n",
    "                                    jitter_col=JITTER_COLUMN,\n",
    "                                    shimmer_col=SHIMMER_COLUMN,\n",
    "                                    mfcc_col=MFCC_COLUMN,\n",
    "                                    label_col=LABEL_COLUMN,\n",
    "                                    nan_replacement=NAN_REPLACEMENT_VALUE)\n",
    "\n",
    "dev_dataloader = DataLoader(\n",
    "    pitch_dataset_dev, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn, num_workers=8\n",
    ")\n",
    "\n",
    "## For inspection\n",
    "for i, batch_data in enumerate(train_dataloader):\n",
    "    # batch_data is a tuple\n",
    "    batch_labels, batch_lengths, batch_pitchhnr, batch_global, batch_mfcc = batch_data\n",
    "    print(f\"\\n--- Batch {i+1} ---\")\n",
    "    print(f\"  Labels (first 5): {batch_labels[:5]}\")\n",
    "    print(f\"  Padded Sequences Shape: {batch_pitchhnr.shape}\")\n",
    "    print(f\"  Original Lengths (first 5): {batch_lengths[:5]}\")\n",
    "    print(f\"  Global Shape: {batch_global.shape}\")\n",
    "    print(f\"  MFCC Shape: {batch_mfcc.shape}\")\n",
    "    \n",
    "\n",
    "    if i == 0: # Break after the first batch for inspection\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdbd83f",
   "metadata": {},
   "source": [
    "### Finding the weight (for weighted cross entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ac071d",
   "metadata": {},
   "source": [
    "is there different ways calculating weitghs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f67837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = df_train['LABEL']   \n",
    "total = len(labels)\n",
    "count_bonafide = labels.value_counts().get(LABEL_BONAFIDE, 0)\n",
    "count_spoof =  total - count_bonafide\n",
    "weight_bonafide = total / (count_bonafide * 2)\n",
    "weight_spoof = total / (count_spoof * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3d85b7",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8a24da",
   "metadata": {},
   "source": [
    "#### LSTM&FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eb3605",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_FFN_branch(nn.Module):\n",
    "    def __init__(self, lstm_input_dim, lstm_hidden_dim, lstm_n_layers, bidirectional, \n",
    "                 ffn_dims):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm_ffn_dim = (lstm_hidden_dim * 2 if bidirectional else lstm_hidden_dim) + ffn_dims[-1]\n",
    "        self.ffn_layers = nn.ModuleList()\n",
    "\n",
    "        # 1. lstm layer\n",
    "        self.lstm = nn.LSTM(lstm_input_dim, \n",
    "                            lstm_hidden_dim, \n",
    "                            num_layers=lstm_n_layers, \n",
    "                            bidirectional=bidirectional, \n",
    "                            batch_first=True) # Input/output tensors are (batch, seq, feature)\n",
    "        # BN layer for stabalization\n",
    "        self.bn_lstm = nn.BatchNorm1d(lstm_hidden_dim * 2 if bidirectional else lstm_hidden_dim)\n",
    "        \n",
    "        # 2. ffn layer\n",
    "        for i in range(len(ffn_dims) -1):\n",
    "            ffn_input_dim = ffn_dims[i]\n",
    "            ffn_hidden_dim = ffn_dims[i+1]\n",
    "            ffn_block = nn.Sequential(\n",
    "                nn.Linear(ffn_input_dim, ffn_hidden_dim),\n",
    "                nn.BatchNorm1d(ffn_hidden_dim),    # BN layer for stabalization\n",
    "                nn.ReLU())\n",
    "            self.ffn_layers.append(ffn_block)\n",
    "        \n",
    "        \n",
    "    def forward(self, pitch_hnrs, pitchhnr_lengths, global_features):\n",
    "      \n",
    "        # 1. Pack sequence\n",
    "        ### Compute actual data and ignore the padded values\n",
    "        packed_input = rnn_utils.pack_padded_sequence(pitch_hnrs, pitchhnr_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # 2. Pass packed sequence through LSTM\n",
    "        ### packed_output: Hidden states for every time step.\n",
    "        ### hidden: The final hidden state (summary) of the entire sequence.\n",
    "        ### cell: The final cell state (long-term memory) of the entire sequence.\n",
    "        packed_output, (lstm_hidden, cell) = self.lstm(packed_input)\n",
    "        \n",
    "        # 3. Concatenate the final forward and backward hidden states (if bidirectional)\n",
    "        if self.lstm.bidirectional:\n",
    "            lstm_hidden = torch.cat((lstm_hidden[-2,:,:], lstm_hidden[-1,:,:]), dim=1)\n",
    "        else:\n",
    "            lstm_hidden = lstm_hidden[-1,:,:]\n",
    "        lstm_hidden = self.bn_lstm(lstm_hidden)\n",
    "\n",
    "        # 4. Pass global features (jitter and shimmer) through the FFN\n",
    "        for layer in self.ffn_layers:\n",
    "            global_features = layer(global_features)\n",
    "        ffn_output = global_features\n",
    "\n",
    "        # 5. Concatenate the outputs from lstm and fnn\n",
    "        combined_output = torch.cat((lstm_hidden,ffn_output), dim=1)\n",
    "\n",
    "        return combined_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fda60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for LSTM_FFN training alone \n",
    "class LSTM_FFN_classifer(nn.Module):\n",
    "    def __init__(self, lstm_ffn_out, out_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm_ffn_layer = lstm_ffn_out\n",
    "        self.fc = nn.Linear(self.lstm_ffn_out.lstm_ffn_dim, out_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, pitch_hnrs, pitchhnr_lengths, global_features):\n",
    "\n",
    "        lstm_ffn_out = self.lstm_ffn_layer(pitch_hnrs, pitchhnr_lengths, global_features)\n",
    "        lstm_ffn_out = self.dropout(lstm_ffn_out)\n",
    "        output = self.fc(lstm_ffn_out)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba600f70",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290b852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_branch(nn.Module):\n",
    "    def __init__(self, cnn_channels, conv_kernel, pool_kernel, cnn_padding):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn_dim = cnn_channels[-1]\n",
    "\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "\n",
    "        for i in range(len(cnn_channels)-2):\n",
    "            cnn_in = cnn_channels[i]\n",
    "            cnn_out = cnn_channels[i+1]\n",
    "            conv_block = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=cnn_in, out_channels=cnn_out, kernel_size=conv_kernel, padding=cnn_padding),\n",
    "                nn.BatchNorm2d(cnn_out),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=pool_kernel))\n",
    "            self.conv_layers.append(conv_block)\n",
    "\n",
    "        # final layer of CNN\n",
    "        final_in = cnn_channels[-2]\n",
    "        final_out = cnn_channels[-1]\n",
    "\n",
    "        conv_final = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=final_in, out_channels=final_out, kernel_size=conv_kernel, padding=cnn_padding),\n",
    "            nn.BatchNorm2d(final_out),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool2d((1, 1))  # Output size: [batch, 64, 1, 1]\n",
    "        )\n",
    "        self.conv_layers.append(conv_final)\n",
    "        \n",
    "    def forward(self, mfccs):\n",
    "\n",
    "        # expected shape (batch_size, in_channel, height, width) -> unsqeeze\n",
    "        mfccs = mfccs.unsqueeze(1)\n",
    "\n",
    "        for layer in self.conv_layers:\n",
    "            mfccs = layer(mfccs)\n",
    "        cnn_out = mfccs.view(mfccs.size(0), -1)\n",
    "        \n",
    "        return cnn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d26e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for CNN training alone\n",
    "class CNN_classifer(nn.Module):\n",
    "    def __init__(self, cnn_out, out_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn_layer = cnn_out\n",
    "        self.fc = nn.Linear(self.cnn_out.cnn_dim, out_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, mfccs):\n",
    "\n",
    "        cnn_out = self.cnn_layer(mfccs)\n",
    "        cnn_out = self.dropout(cnn_out)\n",
    "        output = self.fc(cnn_out)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707abbf4",
   "metadata": {},
   "source": [
    "#### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e31c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BranchAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A module that learns to weigh two input branches.\n",
    "    \"\"\"\n",
    "    def __init__(self, branch1_dim, branch2_dim):\n",
    "        super().__init__()\n",
    "        self.attention_net = nn.Linear(branch1_dim + branch2_dim, 2)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.attention_net.bias.fill_(0)\n",
    "\n",
    "    def forward(self, branch1_out, branch2_out):\n",
    "        # 1. concatenate the raw outputs from both branches\n",
    "        combined_out = torch.cat((branch1_out, branch2_out), dim=1)\n",
    "        \n",
    "        # 2. Predict the score for each branch's importance\n",
    "        attention_scores = self.attention_net(combined_out)\n",
    "        \n",
    "        # 3. turn scores into weights that sum to 1 (e.g., [0.7, 0.3])\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)\n",
    "        \n",
    "        # 4. Get the individual weight for each branch\n",
    "        # .unsqueeze(1) is needed to make the dimensions compatible for multiplication\n",
    "        branch1_weight = attention_weights[:, 0].unsqueeze(1)\n",
    "        branch2_weight = attention_weights[:, 1].unsqueeze(1)\n",
    "        \n",
    "        # 5. Scale each branch's output by its learned weight\n",
    "        branch1_weighted = branch1_out * branch1_weight\n",
    "        branch2_weighted = branch2_out * branch2_weight\n",
    "        \n",
    "        # 6. Concatenate the *weighted* features to pass to the final classifier\n",
    "        weighted_combined_features = torch.cat((branch1_weighted, branch2_weighted), dim=1)\n",
    "        \n",
    "        # Return the combined features and the weights for inspection\n",
    "        return weighted_combined_features, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f942586f",
   "metadata": {},
   "source": [
    "#### Emsemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbba4a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpoofEnsemble(nn.Module):\n",
    "    def __init__(self, lstm_ffn_branch, cnn_branch, output_dim, dropout):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm_ffn_branch = lstm_ffn_branch\n",
    "        self.cnn_branch = cnn_branch\n",
    "\n",
    "        lstm_ffn_dim = lstm_ffn_branch.lstm_ffn_dim\n",
    "        cnn_dim = cnn_branch.cnn_dim\n",
    "        self.fc = nn.Linear(lstm_ffn_dim + cnn_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, pitch_hnrs, pitchhnr_lengths, global_features, mfccs):\n",
    "      \n",
    "        lstm_ffn_out = self.lstm_ffn_branch(pitch_hnrs, pitchhnr_lengths, global_features)\n",
    "        \n",
    "        # Get the output from the second branch\n",
    "        cnn_out = self.cnn_branch(mfccs)\n",
    "        \n",
    "        # Concatenate all features\n",
    "        combined_features = torch.cat((lstm_ffn_out, cnn_out), dim=1)\n",
    "\n",
    "        # Apply dropout\n",
    "        combined_features = self.dropout(combined_features)\n",
    "        \n",
    "        # Final classification\n",
    "        output = self.fc(combined_features)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318ef8a6",
   "metadata": {},
   "source": [
    "#### Ensemble with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ec8930",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpoofEnsemble_attention(nn.Module):\n",
    "    def __init__(self, lstm_ffn_branch, cnn_branch, output_dim, dropout):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm_ffn_branch = lstm_ffn_branch\n",
    "        self.cnn_branch = cnn_branch\n",
    "\n",
    "        lstm_ffn_dim = lstm_ffn_branch.lstm_ffn_dim\n",
    "        cnn_dim = cnn_branch.cnn_dim\n",
    "\n",
    "        # Instantiate the attention module\n",
    "        self.attention = BranchAttention(lstm_ffn_dim, cnn_dim)\n",
    "        \n",
    "        # This attribute will store the weights from the last forward pass\n",
    "        # for later analysis and interpretation.\n",
    "        self.attention_weights = None\n",
    "\n",
    "        self.fc = nn.Linear(lstm_ffn_dim + cnn_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, pitch_hnrs, pitchhnr_lengths, global_features, mfccs):\n",
    "      \n",
    "        lstm_ffn_out = self.lstm_ffn_branch(pitch_hnrs, pitchhnr_lengths, global_features)\n",
    "        \n",
    "        # Get the output from the second branch\n",
    "        cnn_out = self.cnn_branch(mfccs)\n",
    "        \n",
    "        # Pass the raw outputs through the attention mechanism\n",
    "        combined_features, self.attention_weights = self.attention(lstm_ffn_out, cnn_out)\n",
    "        \n",
    "        # Apply dropout\n",
    "        combined_features = self.dropout(combined_features)\n",
    "        \n",
    "        # Final classification\n",
    "        output = self.fc(combined_features)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0e0862",
   "metadata": {},
   "source": [
    "### Initiate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70b6bdf",
   "metadata": {},
   "source": [
    "#### find the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5ff2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device count: 4\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa91379",
   "metadata": {},
   "source": [
    "#### find the class weights for WCE & set the criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bcf403",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = torch.tensor([weight_bonafide, weight_spoof], dtype=torch.float32).to(DEVICE)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean', weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121e595d",
   "metadata": {},
   "source": [
    "#### Initiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e02bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_model():\n",
    "    lstm_ffn_out= LSTM_FFN_branch(lstm_input_dim=config.lstm_input_dim, lstm_hidden_dim=config.lstm_hidden_dim, lstm_n_layers=config.lstm_n_layers, bidirectional=config.bidirectional,\n",
    "                    ffn_dims=config.ffn_dims).to(DEVICE)\n",
    "    cnn_out = CNN_branch(cnn_channels=config.cnn_channels, conv_kernel=config.conv_kernel, pool_kernel=config.pool_kernel, cnn_padding=config.cnn_padding).to(DEVICE)\n",
    "\n",
    "    if config.model==\"SpoofEnsemble\":\n",
    "        model = SpoofEnsemble(lstm_ffn_branch=lstm_ffn_out, cnn_branch=cnn_out, output_dim=2, dropout=config.dropout_rate).to(DEVICE)\n",
    "    elif config.model==\"LSTM_FFN_classifier\":\n",
    "        model = LSTM_FFN_classifer(lstm_ffn_out=lstm_ffn_out, output_dim=2, dropout=config.dropout_rate).to(DEVICE)\n",
    "    elif config.model==\"CNN_classifier\":\n",
    "        model = CNN_classifer(cnn_out=cnn_out, out_dim=2, dropout=config.dropout_rate).to(DEVICE)\n",
    "    elif config.model==\"SpoofEnsemble_attention\":\n",
    "        model = SpoofEnsemble_attention(lstm_ffn_branch=lstm_ffn_out, cnn_branch=cnn_out, output_dim=2, dropout=config.dropout_rate).to(DEVICE)\n",
    "    else:\n",
    "        print(\"WARNING: invalid model name.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# print(f\"DEBUG: Initial FFN_Linear WEIGHTS:\\n{model.ffn_linear.weight.detach().cpu().numpy()}\")\n",
    "# print(f\"DEBUG: Initial FFN_Linear BIAS:\\n{model.ffn_linear.bias.detach().cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed0614e",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7882ce3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(data_loader, model, criterion):\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode (disables dropout, etc.)\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    scores_bonafide = []\n",
    "    scores_spoof = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculations during evaluation\n",
    "        for batch_labels, batch_lengths, batch_pitchhnr, batch_global, batch_mfcc in data_loader:\n",
    "            \n",
    "            batch_labels = batch_labels.to(DEVICE)\n",
    "            batch_pitchhnr = batch_pitchhnr.to(DEVICE)\n",
    "            batch_global = batch_global.to(DEVICE)\n",
    "            batch_mfcc = batch_mfcc.to(DEVICE)\n",
    "\n",
    "            # Forward pass: Get model outputs (logits)\n",
    "            logits = model(batch_pitchhnr, batch_lengths, batch_global, batch_mfcc)\n",
    "            \n",
    "            # Calculate loss for the current batch\n",
    "            loss = criterion(logits, batch_labels)\n",
    "            total_loss += loss.item() * batch_labels.size(0) # Accumulate loss, weighted by batch size\n",
    "\n",
    "            # for EER\n",
    "            probabilities = torch.softmax(logits, dim=1)\n",
    "            \n",
    "            for i in range(len(batch_labels)):\n",
    "                current_label = batch_labels[i]\n",
    "                current_score = probabilities[i]\n",
    "\n",
    "                if current_label == LABEL_BONAFIDE:\n",
    "                    scores_bonafide.append(current_score[LABEL_BONAFIDE].cpu())     # numpy is cpu only, need to move tensor from gpu\n",
    "                elif current_label == LABEL_SPOOF:\n",
    "                    scores_spoof.append(current_score[LABEL_BONAFIDE].cpu())\n",
    "            \n",
    "            total_samples += batch_labels.size(0) # Count number of samples in this batch\n",
    "\n",
    "    average_loss = total_loss / total_samples if total_samples > 0 else 0.0\n",
    "\n",
    "    scores_bonafide_np = np.array(scores_bonafide)    \n",
    "    scores_spoof_np = np.array(scores_spoof)\n",
    "    eer, threshold = em.compute_eer(scores_bonafide_np, scores_spoof_np)\n",
    "\n",
    "    all_scores = np.concatenate((scores_bonafide_np, scores_spoof_np))\n",
    "    labels_true = np.concatenate((np.ones_like(scores_bonafide_np), np.zeros_like(scores_spoof_np)))\n",
    "    labels_pred = (all_scores >= threshold).astype(int)\n",
    "    \n",
    "    return average_loss, eer, threshold, labels_true, labels_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05672159",
   "metadata": {},
   "source": [
    "### The training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dca569",
   "metadata": {},
   "source": [
    ">note: in wandb, scalers logs for every epoch, plots get overwritten (but still saved in artifacts?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10873ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(criterion, train_dataloader, dev_dataloader, num_epochs,\n",
    "                min_eer, best_model_filename):\n",
    "\n",
    "    model = initiate_model()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=config.scheduler_factor, patience=config.scheduler_patience)\n",
    "    print(f\"Training started on device: {DEVICE}\")\n",
    "    model.to(DEVICE) \n",
    "\n",
    "    # Initial metric dictionary for the progress bar\n",
    "    metric_dict = {'train_loss': 'N/A', 'val_loss': 'N/A', 'val_eer': 'N/A', 'val_threshold': 'N/A'}\n",
    "\n",
    "    # Evaluate on validation set first to get a baseline\n",
    "    print(\"Evaluating on validation set before training...\")\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    val_loss_initial, val_eer_initial, threshold_initial, labels_true, labels_pred = evaluate_classifier(dev_dataloader, model, criterion)\n",
    "    metric_dict.update({'val_loss': f'{val_loss_initial:.3f}', 'val_eer': f'{val_eer_initial*100:.2f}%', 'val_threshold': f'{threshold_initial*100:.2f}%'})\n",
    "    print(f\"Initial Validation - Loss: {val_loss_initial:.4f}, EER: {val_eer_initial*100:.2f}%, Threshold: {threshold_initial*100:.2f}%\")\n",
    "\n",
    "    # Progress bar setup\n",
    "    total_steps = num_epochs * len(train_dataloader)\n",
    "    pbar = tqdm(total=total_steps, initial=0, postfix=metric_dict, unit=\"batch\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode (enables dropout, etc.)\n",
    "        pbar.set_description(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        \n",
    "        running_train_loss = 0.0\n",
    "        num_train_batches = 0\n",
    "\n",
    "        for batch_labels, batch_lengths, batch_pitchhnr, batch_global, batch_mfcc in train_dataloader:\n",
    "            # Move data to the specified device\n",
    "            # batch_lengths are used by pack_padded_sequence which expects them on CPU\n",
    "            batch_labels = batch_labels.to(DEVICE)\n",
    "            batch_pitchhnr = batch_pitchhnr.to(DEVICE)\n",
    "            batch_global = batch_global.to(DEVICE)\n",
    "            batch_mfcc = batch_mfcc.to(DEVICE)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass: Get model outputs (logits)\n",
    "            logits = model(batch_pitchhnr, batch_lengths, batch_global, batch_mfcc)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(logits, batch_labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            # --- FOR GRADIENT CLIPPING ---\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update statistics for progress bar and logging\n",
    "            running_train_loss += loss.item()\n",
    "            num_train_batches += 1\n",
    "            \n",
    "            pbar.update(1) # Increment progress bar by one batch\n",
    "            metric_dict.update({'train_loss': f'{loss.item():.3f}'}) # Current batch loss\n",
    "            pbar.set_postfix(metric_dict)\n",
    "        \n",
    "        # Calculate average training loss for the epoch\n",
    "        avg_epoch_train_loss = running_train_loss / num_train_batches if num_train_batches > 0 else 0.0\n",
    "        metric_dict.update({'train_loss': f'{avg_epoch_train_loss:.3f}'}) # Average epoch loss\n",
    "        \n",
    "        # Evaluate on validation set after each epoch\n",
    "        avg_val_loss, val_eer, val_threshold, labels_true, labels_pred = evaluate_classifier(dev_dataloader, model, criterion)\n",
    "        \n",
    "        # for reduce on plateau\n",
    "        if config.scheduler:\n",
    "            scheduler.step(val_eer)\n",
    "\n",
    "        # Update with latest validation metrics\n",
    "        metric_dict.update({'val_loss': f'{avg_val_loss:.3f}', 'val_eer': f'{val_eer*100:.2f}%', 'val_threshold': f'{val_threshold*100:.2f}%'})\n",
    "        pbar.set_postfix(metric_dict)\n",
    "        \n",
    "        # Optional: Print epoch summary\n",
    "        print(f\"\\nEpoch {epoch+1} Summary: Avg Train Loss: {avg_epoch_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, EER: {val_eer*100:.2f}%, Threshold: {val_threshold*100:.2f}%\")\n",
    "\n",
    "        # log the train\\dev loss and the eer & threshold\n",
    "        run.log({\"train_loss\": avg_epoch_train_loss, \"dev_loss\": avg_val_loss, \n",
    "                   \"dev_eer\": val_eer, \"dev_threshold\":val_threshold, \"epoch\": epoch + 1})\n",
    "        \n",
    "        # update min eer and optimal model\n",
    "        if val_eer < min_eer:\n",
    "            min_eer = val_eer\n",
    "            torch.save(model.state_dict(), best_model_filename)\n",
    "            print(f\"Epoch {epoch+1}: New best model saved to '{best_model_filename}' with EER: {min_eer:.4f}\")\n",
    "\n",
    "            run.summary['best_validation_eer'] = min_eer\n",
    "            run.summary['best_eer_epoch'] = epoch + 1\n",
    "            run.summary['validation_loss_at_best_eer'] = avg_val_loss\n",
    "\n",
    "            # log the report and confusion matrix\n",
    "            class_names = ['SPOOF', 'BONAFIDE']     #NOTE: the order matters, need to match labels\n",
    "            report_columns =  [\"Class\", \"Precision\", \"Recall\", \"F1-score\", \"Support\"]\n",
    "            class_report = classification_report(labels_true, labels_pred, labels=[0, 1],\n",
    "                                        target_names=class_names).splitlines()\n",
    "            report_table = []\n",
    "            for line in class_report[2:(len(class_names)+2)]:\n",
    "                report_table.append(line.split())\n",
    "            run.log({\"Confusion Matix\": wandb.plot.confusion_matrix(y_true=labels_true, preds=labels_pred, class_names=class_names),\n",
    "                    \"Classification Report\": wandb.Table(data=report_table, columns=report_columns)})\n",
    "\n",
    "    pbar.close()\n",
    "    print(\"Training finished.\")\n",
    "    return min_eer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7357368b",
   "metadata": {},
   "source": [
    "#### Start the training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e00816c",
   "metadata": {},
   "source": [
    ">note: only partially deterministic for adaptivemaxpooling does not support the feature yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c89dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Trial with Seed: 0 ---\n",
      "Training started on device: cuda\n",
      "Evaluating on validation set before training...\n",
      "Initial Validation - Loss: 0.6465, EER: 59.73%, Threshold: 46.85%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "238020d26cd24e54aa9b7d3d8f292300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99750 [00:00<?, ?batch/s, train_loss=N/A, val_eer=59.73%, val_loss=0.646, val_threshold=46.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary: Avg Train Loss: 0.3243, Val Loss: 0.1743, EER: 6.44%, Threshold: 53.23%\n",
      "Epoch 1: New best model saved to 'best_model' with EER: 0.0644\n",
      "\n",
      "Epoch 2 Summary: Avg Train Loss: 0.1529, Val Loss: 0.0905, EER: 4.48%, Threshold: 40.18%\n",
      "Epoch 2: New best model saved to 'best_model' with EER: 0.0448\n",
      "\n",
      "Epoch 3 Summary: Avg Train Loss: 0.1173, Val Loss: 0.0689, EER: 4.36%, Threshold: 26.00%\n",
      "Epoch 3: New best model saved to 'best_model' with EER: 0.0436\n",
      "\n",
      "Epoch 4 Summary: Avg Train Loss: 0.1001, Val Loss: 0.0629, EER: 3.88%, Threshold: 26.28%\n",
      "Epoch 4: New best model saved to 'best_model' with EER: 0.0388\n",
      "\n",
      "Epoch 5 Summary: Avg Train Loss: 0.0844, Val Loss: 0.0512, EER: 3.65%, Threshold: 16.49%\n",
      "Epoch 5: New best model saved to 'best_model' with EER: 0.0365\n",
      "\n",
      "Epoch 6 Summary: Avg Train Loss: 0.0713, Val Loss: 0.0497, EER: 3.49%, Threshold: 18.02%\n",
      "Epoch 6: New best model saved to 'best_model' with EER: 0.0349\n",
      "\n",
      "Epoch 7 Summary: Avg Train Loss: 0.0622, Val Loss: 0.0461, EER: 3.07%, Threshold: 23.48%\n",
      "Epoch 7: New best model saved to 'best_model' with EER: 0.0307\n",
      "\n",
      "Epoch 8 Summary: Avg Train Loss: 0.0568, Val Loss: 0.0385, EER: 2.67%, Threshold: 20.19%\n",
      "Epoch 8: New best model saved to 'best_model' with EER: 0.0267\n",
      "\n",
      "Epoch 9 Summary: Avg Train Loss: 0.0481, Val Loss: 0.0328, EER: 2.52%, Threshold: 10.18%\n",
      "Epoch 9: New best model saved to 'best_model' with EER: 0.0252\n",
      "\n",
      "Epoch 10 Summary: Avg Train Loss: 0.0425, Val Loss: 0.0350, EER: 2.35%, Threshold: 2.85%\n",
      "Epoch 10: New best model saved to 'best_model' with EER: 0.0235\n",
      "\n",
      "Epoch 11 Summary: Avg Train Loss: 0.0367, Val Loss: 0.0298, EER: 2.08%, Threshold: 20.50%\n",
      "Epoch 11: New best model saved to 'best_model' with EER: 0.0208\n",
      "\n",
      "Epoch 12 Summary: Avg Train Loss: 0.0321, Val Loss: 0.0264, EER: 2.04%, Threshold: 8.24%\n",
      "Epoch 12: New best model saved to 'best_model' with EER: 0.0204\n",
      "\n",
      "Epoch 13 Summary: Avg Train Loss: 0.0289, Val Loss: 0.0237, EER: 1.81%, Threshold: 10.31%\n",
      "Epoch 13: New best model saved to 'best_model' with EER: 0.0181\n",
      "\n",
      "Epoch 14 Summary: Avg Train Loss: 0.0251, Val Loss: 0.0258, EER: 1.61%, Threshold: 22.34%\n",
      "Epoch 14: New best model saved to 'best_model' with EER: 0.0161\n",
      "\n",
      "Epoch 15 Summary: Avg Train Loss: 0.0264, Val Loss: 0.0210, EER: 1.57%, Threshold: 8.11%\n",
      "Epoch 15: New best model saved to 'best_model' with EER: 0.0157\n",
      "\n",
      "Epoch 16 Summary: Avg Train Loss: 0.0214, Val Loss: 0.0195, EER: 1.49%, Threshold: 6.97%\n",
      "Epoch 16: New best model saved to 'best_model' with EER: 0.0149\n",
      "\n",
      "Epoch 17 Summary: Avg Train Loss: 0.0226, Val Loss: 0.0176, EER: 1.45%, Threshold: 10.64%\n",
      "Epoch 17: New best model saved to 'best_model' with EER: 0.0145\n",
      "\n",
      "Epoch 18 Summary: Avg Train Loss: 0.0182, Val Loss: 0.0203, EER: 1.46%, Threshold: 6.77%\n",
      "\n",
      "Epoch 19 Summary: Avg Train Loss: 0.0191, Val Loss: 0.0181, EER: 1.41%, Threshold: 3.82%\n",
      "Epoch 19: New best model saved to 'best_model' with EER: 0.0141\n",
      "\n",
      "Epoch 20 Summary: Avg Train Loss: 0.0165, Val Loss: 0.0296, EER: 1.25%, Threshold: 0.78%\n",
      "Epoch 20: New best model saved to 'best_model' with EER: 0.0125\n",
      "\n",
      "Epoch 21 Summary: Avg Train Loss: 0.0144, Val Loss: 0.0197, EER: 1.49%, Threshold: 12.24%\n",
      "\n",
      "Epoch 22 Summary: Avg Train Loss: 0.0142, Val Loss: 0.0313, EER: 1.66%, Threshold: 0.79%\n",
      "\n",
      "Epoch 23 Summary: Avg Train Loss: 0.0140, Val Loss: 0.0215, EER: 1.37%, Threshold: 6.62%\n",
      "\n",
      "Epoch 24 Summary: Avg Train Loss: 0.0125, Val Loss: 0.0281, EER: 1.45%, Threshold: 0.90%\n",
      "\n",
      "Epoch 25 Summary: Avg Train Loss: 0.0131, Val Loss: 0.0180, EER: 1.29%, Threshold: 1.85%\n",
      "\n",
      "Epoch 26 Summary: Avg Train Loss: 0.0078, Val Loss: 0.0291, EER: 1.22%, Threshold: 0.47%\n",
      "Epoch 26: New best model saved to 'best_model' with EER: 0.0122\n",
      "\n",
      "Epoch 27 Summary: Avg Train Loss: 0.0072, Val Loss: 0.0232, EER: 1.10%, Threshold: 0.71%\n",
      "Epoch 27: New best model saved to 'best_model' with EER: 0.0110\n",
      "\n",
      "Epoch 28 Summary: Avg Train Loss: 0.0068, Val Loss: 0.0223, EER: 1.03%, Threshold: 0.55%\n",
      "Epoch 28: New best model saved to 'best_model' with EER: 0.0103\n",
      "\n",
      "Epoch 29 Summary: Avg Train Loss: 0.0077, Val Loss: 0.0138, EER: 1.10%, Threshold: 4.51%\n",
      "\n",
      "Epoch 30 Summary: Avg Train Loss: 0.0065, Val Loss: 0.0192, EER: 0.98%, Threshold: 1.11%\n",
      "Epoch 30: New best model saved to 'best_model' with EER: 0.0098\n",
      "\n",
      "Epoch 31 Summary: Avg Train Loss: 0.0065, Val Loss: 0.0373, EER: 1.06%, Threshold: 0.10%\n",
      "\n",
      "Epoch 32 Summary: Avg Train Loss: 0.0059, Val Loss: 0.0177, EER: 0.97%, Threshold: 2.05%\n",
      "Epoch 32: New best model saved to 'best_model' with EER: 0.0097\n",
      "\n",
      "Epoch 33 Summary: Avg Train Loss: 0.0059, Val Loss: 0.0172, EER: 1.10%, Threshold: 1.90%\n",
      "\n",
      "Epoch 34 Summary: Avg Train Loss: 0.0053, Val Loss: 0.0212, EER: 1.13%, Threshold: 0.75%\n",
      "\n",
      "Epoch 35 Summary: Avg Train Loss: 0.0059, Val Loss: 0.0169, EER: 0.94%, Threshold: 2.03%\n",
      "Epoch 35: New best model saved to 'best_model' with EER: 0.0094\n",
      "\n",
      "Epoch 36 Summary: Avg Train Loss: 0.0053, Val Loss: 0.0285, EER: 1.15%, Threshold: 0.25%\n",
      "\n",
      "Epoch 37 Summary: Avg Train Loss: 0.0041, Val Loss: 0.0269, EER: 1.02%, Threshold: 0.34%\n",
      "\n",
      "Epoch 38 Summary: Avg Train Loss: 0.0054, Val Loss: 0.0194, EER: 0.98%, Threshold: 1.69%\n",
      "\n",
      "Epoch 39 Summary: Avg Train Loss: 0.0057, Val Loss: 0.0317, EER: 0.97%, Threshold: 0.13%\n",
      "\n",
      "Epoch 40 Summary: Avg Train Loss: 0.0040, Val Loss: 0.0152, EER: 1.02%, Threshold: 1.60%\n",
      "\n",
      "Epoch 41 Summary: Avg Train Loss: 0.0034, Val Loss: 0.0156, EER: 0.90%, Threshold: 1.76%\n",
      "Epoch 41: New best model saved to 'best_model' with EER: 0.0090\n",
      "\n",
      "Epoch 42 Summary: Avg Train Loss: 0.0034, Val Loss: 0.0277, EER: 0.98%, Threshold: 0.18%\n",
      "\n",
      "Epoch 43 Summary: Avg Train Loss: 0.0031, Val Loss: 0.0163, EER: 1.06%, Threshold: 1.46%\n",
      "\n",
      "Epoch 44 Summary: Avg Train Loss: 0.0027, Val Loss: 0.0266, EER: 0.90%, Threshold: 0.17%\n",
      "\n",
      "Epoch 45 Summary: Avg Train Loss: 0.0033, Val Loss: 0.0206, EER: 1.02%, Threshold: 0.37%\n",
      "\n",
      "Epoch 46 Summary: Avg Train Loss: 0.0028, Val Loss: 0.0261, EER: 0.86%, Threshold: 0.14%\n",
      "Epoch 46: New best model saved to 'best_model' with EER: 0.0086\n",
      "\n",
      "Epoch 47 Summary: Avg Train Loss: 0.0029, Val Loss: 0.0223, EER: 0.90%, Threshold: 0.38%\n",
      "\n",
      "Epoch 48 Summary: Avg Train Loss: 0.0029, Val Loss: 0.0145, EER: 0.86%, Threshold: 1.33%\n",
      "\n",
      "Epoch 49 Summary: Avg Train Loss: 0.0027, Val Loss: 0.0169, EER: 0.90%, Threshold: 0.89%\n",
      "\n",
      "Epoch 50 Summary: Avg Train Loss: 0.0033, Val Loss: 0.0200, EER: 0.90%, Threshold: 0.41%\n",
      "\n",
      "Epoch 51 Summary: Avg Train Loss: 0.0025, Val Loss: 0.0221, EER: 0.90%, Threshold: 0.35%\n",
      "\n",
      "Epoch 52 Summary: Avg Train Loss: 0.0022, Val Loss: 0.0204, EER: 0.90%, Threshold: 0.33%\n",
      "\n",
      "Epoch 53 Summary: Avg Train Loss: 0.0020, Val Loss: 0.0275, EER: 0.90%, Threshold: 0.11%\n",
      "\n",
      "Epoch 54 Summary: Avg Train Loss: 0.0024, Val Loss: 0.0202, EER: 0.91%, Threshold: 0.54%\n",
      "\n",
      "Epoch 55 Summary: Avg Train Loss: 0.0017, Val Loss: 0.0205, EER: 0.94%, Threshold: 0.35%\n",
      "\n",
      "Epoch 56 Summary: Avg Train Loss: 0.0022, Val Loss: 0.0193, EER: 0.83%, Threshold: 0.60%\n",
      "Epoch 56: New best model saved to 'best_model' with EER: 0.0083\n",
      "\n",
      "Epoch 57 Summary: Avg Train Loss: 0.0017, Val Loss: 0.0214, EER: 0.86%, Threshold: 0.44%\n",
      "\n",
      "Epoch 58 Summary: Avg Train Loss: 0.0019, Val Loss: 0.0262, EER: 0.94%, Threshold: 0.12%\n",
      "\n",
      "Epoch 59 Summary: Avg Train Loss: 0.0018, Val Loss: 0.0199, EER: 0.95%, Threshold: 0.38%\n",
      "\n",
      "Epoch 60 Summary: Avg Train Loss: 0.0019, Val Loss: 0.0298, EER: 0.83%, Threshold: 0.08%\n",
      "\n",
      "Epoch 61 Summary: Avg Train Loss: 0.0020, Val Loss: 0.0241, EER: 0.86%, Threshold: 0.17%\n",
      "\n",
      "Epoch 62 Summary: Avg Train Loss: 0.0017, Val Loss: 0.0202, EER: 0.90%, Threshold: 0.28%\n",
      "\n",
      "Epoch 63 Summary: Avg Train Loss: 0.0014, Val Loss: 0.0193, EER: 0.90%, Threshold: 0.38%\n",
      "\n",
      "Epoch 64 Summary: Avg Train Loss: 0.0015, Val Loss: 0.0260, EER: 0.91%, Threshold: 0.14%\n",
      "\n",
      "Epoch 65 Summary: Avg Train Loss: 0.0014, Val Loss: 0.0307, EER: 0.86%, Threshold: 0.06%\n",
      "\n",
      "Epoch 66 Summary: Avg Train Loss: 0.0020, Val Loss: 0.0295, EER: 0.82%, Threshold: 0.07%\n",
      "Epoch 66: New best model saved to 'best_model' with EER: 0.0082\n",
      "\n",
      "Epoch 67 Summary: Avg Train Loss: 0.0013, Val Loss: 0.0304, EER: 0.91%, Threshold: 0.08%\n",
      "\n",
      "Epoch 68 Summary: Avg Train Loss: 0.0015, Val Loss: 0.0210, EER: 0.86%, Threshold: 0.24%\n",
      "\n",
      "Epoch 69 Summary: Avg Train Loss: 0.0013, Val Loss: 0.0275, EER: 0.86%, Threshold: 0.10%\n",
      "\n",
      "Epoch 70 Summary: Avg Train Loss: 0.0012, Val Loss: 0.0322, EER: 0.86%, Threshold: 0.05%\n",
      "Training finished.\n",
      "\n",
      "--- Starting Trial with Seed: 7 ---\n",
      "Training started on device: cuda\n",
      "Evaluating on validation set before training...\n",
      "Initial Validation - Loss: 0.6560, EER: 66.41%, Threshold: 47.46%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f804110b5a34eaebaca95ad5d67705c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99750 [00:00<?, ?batch/s, train_loss=N/A, val_eer=66.41%, val_loss=0.656, val_threshold=47.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary: Avg Train Loss: 0.3259, Val Loss: 0.1923, EER: 5.97%, Threshold: 63.78%\n",
      "\n",
      "Epoch 2 Summary: Avg Train Loss: 0.1527, Val Loss: 0.1133, EER: 4.99%, Threshold: 43.87%\n",
      "\n",
      "Epoch 3 Summary: Avg Train Loss: 0.1166, Val Loss: 0.0987, EER: 4.79%, Threshold: 38.94%\n",
      "\n",
      "Epoch 4 Summary: Avg Train Loss: 0.0935, Val Loss: 0.0484, EER: 3.38%, Threshold: 16.32%\n",
      "\n",
      "Epoch 5 Summary: Avg Train Loss: 0.0799, Val Loss: 0.0493, EER: 3.34%, Threshold: 21.51%\n",
      "\n",
      "Epoch 6 Summary: Avg Train Loss: 0.0699, Val Loss: 0.0443, EER: 3.35%, Threshold: 12.24%\n",
      "\n",
      "Epoch 7 Summary: Avg Train Loss: 0.0608, Val Loss: 0.0360, EER: 2.71%, Threshold: 11.29%\n",
      "\n",
      "Epoch 8 Summary: Avg Train Loss: 0.0802, Val Loss: 0.0377, EER: 2.91%, Threshold: 5.46%\n",
      "\n",
      "Epoch 9 Summary: Avg Train Loss: 0.0495, Val Loss: 0.0363, EER: 2.67%, Threshold: 3.59%\n",
      "\n",
      "Epoch 10 Summary: Avg Train Loss: 0.0426, Val Loss: 0.0340, EER: 2.66%, Threshold: 13.41%\n",
      "\n",
      "Epoch 11 Summary: Avg Train Loss: 0.0372, Val Loss: 0.0326, EER: 2.52%, Threshold: 7.07%\n",
      "\n",
      "Epoch 12 Summary: Avg Train Loss: 0.0337, Val Loss: 0.0329, EER: 2.51%, Threshold: 13.13%\n",
      "\n",
      "Epoch 13 Summary: Avg Train Loss: 0.0291, Val Loss: 0.0429, EER: 2.47%, Threshold: 28.74%\n",
      "\n",
      "Epoch 14 Summary: Avg Train Loss: 0.0306, Val Loss: 0.0319, EER: 2.19%, Threshold: 20.56%\n",
      "\n",
      "Epoch 15 Summary: Avg Train Loss: 0.0256, Val Loss: 0.0256, EER: 1.99%, Threshold: 7.93%\n",
      "\n",
      "Epoch 16 Summary: Avg Train Loss: 0.0232, Val Loss: 0.0486, EER: 2.50%, Threshold: 0.33%\n",
      "\n",
      "Epoch 17 Summary: Avg Train Loss: 0.0223, Val Loss: 0.0292, EER: 2.16%, Threshold: 11.44%\n",
      "\n",
      "Epoch 18 Summary: Avg Train Loss: 0.0185, Val Loss: 0.0313, EER: 2.16%, Threshold: 1.43%\n",
      "\n",
      "Epoch 19 Summary: Avg Train Loss: 0.0185, Val Loss: 0.0381, EER: 2.87%, Threshold: 2.10%\n",
      "\n",
      "Epoch 20 Summary: Avg Train Loss: 0.0173, Val Loss: 0.0301, EER: 2.00%, Threshold: 2.45%\n",
      "\n",
      "Epoch 21 Summary: Avg Train Loss: 0.0128, Val Loss: 0.0400, EER: 1.84%, Threshold: 0.36%\n",
      "\n",
      "Epoch 22 Summary: Avg Train Loss: 0.0105, Val Loss: 0.0378, EER: 1.92%, Threshold: 0.43%\n",
      "\n",
      "Epoch 23 Summary: Avg Train Loss: 0.0095, Val Loss: 0.0299, EER: 1.64%, Threshold: 0.84%\n",
      "\n",
      "Epoch 24 Summary: Avg Train Loss: 0.0106, Val Loss: 0.0499, EER: 2.19%, Threshold: 0.14%\n",
      "\n",
      "Epoch 25 Summary: Avg Train Loss: 0.0094, Val Loss: 0.0344, EER: 1.81%, Threshold: 0.46%\n",
      "\n",
      "Epoch 26 Summary: Avg Train Loss: 0.0099, Val Loss: 0.0299, EER: 1.80%, Threshold: 0.80%\n",
      "\n",
      "Epoch 27 Summary: Avg Train Loss: 0.0084, Val Loss: 0.0281, EER: 1.73%, Threshold: 1.09%\n",
      "\n",
      "Epoch 28 Summary: Avg Train Loss: 0.0080, Val Loss: 0.0378, EER: 1.62%, Threshold: 0.37%\n",
      "\n",
      "Epoch 29 Summary: Avg Train Loss: 0.0087, Val Loss: 0.0232, EER: 1.69%, Threshold: 2.59%\n",
      "\n",
      "Epoch 30 Summary: Avg Train Loss: 0.0083, Val Loss: 0.0307, EER: 1.65%, Threshold: 0.52%\n",
      "\n",
      "Epoch 31 Summary: Avg Train Loss: 0.0072, Val Loss: 0.0276, EER: 1.85%, Threshold: 1.24%\n",
      "\n",
      "Epoch 32 Summary: Avg Train Loss: 0.0066, Val Loss: 0.0328, EER: 1.73%, Threshold: 0.44%\n",
      "\n",
      "Epoch 33 Summary: Avg Train Loss: 0.0071, Val Loss: 0.0332, EER: 1.65%, Threshold: 0.53%\n",
      "\n",
      "Epoch 34 Summary: Avg Train Loss: 0.0048, Val Loss: 0.0246, EER: 1.57%, Threshold: 1.39%\n",
      "\n",
      "Epoch 35 Summary: Avg Train Loss: 0.0045, Val Loss: 0.0328, EER: 1.45%, Threshold: 0.32%\n",
      "\n",
      "Epoch 36 Summary: Avg Train Loss: 0.0045, Val Loss: 0.0230, EER: 1.57%, Threshold: 1.67%\n",
      "\n",
      "Epoch 37 Summary: Avg Train Loss: 0.0047, Val Loss: 0.0346, EER: 1.45%, Threshold: 0.35%\n",
      "\n",
      "Epoch 38 Summary: Avg Train Loss: 0.0039, Val Loss: 0.0286, EER: 1.41%, Threshold: 0.62%\n",
      "\n",
      "Epoch 39 Summary: Avg Train Loss: 0.0048, Val Loss: 0.0278, EER: 1.61%, Threshold: 0.71%\n",
      "\n",
      "Epoch 40 Summary: Avg Train Loss: 0.0041, Val Loss: 0.0324, EER: 1.61%, Threshold: 0.34%\n",
      "\n",
      "Epoch 41 Summary: Avg Train Loss: 0.0045, Val Loss: 0.0247, EER: 1.61%, Threshold: 1.34%\n",
      "\n",
      "Epoch 42 Summary: Avg Train Loss: 0.0038, Val Loss: 0.0302, EER: 1.53%, Threshold: 0.47%\n",
      "\n",
      "Epoch 43 Summary: Avg Train Loss: 0.0039, Val Loss: 0.0305, EER: 1.53%, Threshold: 0.38%\n",
      "\n",
      "Epoch 44 Summary: Avg Train Loss: 0.0040, Val Loss: 0.0328, EER: 1.45%, Threshold: 0.27%\n",
      "\n",
      "Epoch 45 Summary: Avg Train Loss: 0.0033, Val Loss: 0.0311, EER: 1.48%, Threshold: 0.30%\n",
      "\n",
      "Epoch 46 Summary: Avg Train Loss: 0.0032, Val Loss: 0.0387, EER: 1.49%, Threshold: 0.13%\n",
      "\n",
      "Epoch 47 Summary: Avg Train Loss: 0.0028, Val Loss: 0.0276, EER: 1.64%, Threshold: 0.75%\n",
      "\n",
      "Epoch 48 Summary: Avg Train Loss: 0.0029, Val Loss: 0.0339, EER: 1.49%, Threshold: 0.22%\n",
      "\n",
      "Epoch 49 Summary: Avg Train Loss: 0.0030, Val Loss: 0.0250, EER: 1.34%, Threshold: 0.74%\n",
      "\n",
      "Epoch 50 Summary: Avg Train Loss: 0.0025, Val Loss: 0.0306, EER: 1.49%, Threshold: 0.30%\n",
      "\n",
      "Epoch 51 Summary: Avg Train Loss: 0.0029, Val Loss: 0.0305, EER: 1.53%, Threshold: 0.29%\n",
      "\n",
      "Epoch 52 Summary: Avg Train Loss: 0.0028, Val Loss: 0.0288, EER: 1.41%, Threshold: 0.39%\n",
      "\n",
      "Epoch 53 Summary: Avg Train Loss: 0.0029, Val Loss: 0.0308, EER: 1.37%, Threshold: 0.27%\n",
      "\n",
      "Epoch 54 Summary: Avg Train Loss: 0.0021, Val Loss: 0.0347, EER: 1.45%, Threshold: 0.16%\n",
      "\n",
      "Epoch 55 Summary: Avg Train Loss: 0.0022, Val Loss: 0.0289, EER: 1.41%, Threshold: 0.37%\n",
      "\n",
      "Epoch 56 Summary: Avg Train Loss: 0.0025, Val Loss: 0.0378, EER: 1.56%, Threshold: 0.10%\n",
      "\n",
      "Epoch 57 Summary: Avg Train Loss: 0.0021, Val Loss: 0.0357, EER: 1.45%, Threshold: 0.16%\n",
      "\n",
      "Epoch 58 Summary: Avg Train Loss: 0.0021, Val Loss: 0.0339, EER: 1.45%, Threshold: 0.18%\n",
      "\n",
      "Epoch 59 Summary: Avg Train Loss: 0.0020, Val Loss: 0.0343, EER: 1.42%, Threshold: 0.16%\n",
      "\n",
      "Epoch 60 Summary: Avg Train Loss: 0.0022, Val Loss: 0.0389, EER: 1.49%, Threshold: 0.10%\n",
      "\n",
      "Epoch 61 Summary: Avg Train Loss: 0.0021, Val Loss: 0.0323, EER: 1.41%, Threshold: 0.21%\n",
      "\n",
      "Epoch 62 Summary: Avg Train Loss: 0.0021, Val Loss: 0.0379, EER: 1.53%, Threshold: 0.11%\n",
      "\n",
      "Epoch 63 Summary: Avg Train Loss: 0.0024, Val Loss: 0.0383, EER: 1.45%, Threshold: 0.11%\n",
      "\n",
      "Epoch 64 Summary: Avg Train Loss: 0.0022, Val Loss: 0.0337, EER: 1.45%, Threshold: 0.19%\n",
      "\n",
      "Epoch 65 Summary: Avg Train Loss: 0.0020, Val Loss: 0.0390, EER: 1.45%, Threshold: 0.10%\n",
      "\n",
      "Epoch 66 Summary: Avg Train Loss: 0.0020, Val Loss: 0.0321, EER: 1.41%, Threshold: 0.22%\n",
      "\n",
      "Epoch 67 Summary: Avg Train Loss: 0.0023, Val Loss: 0.0356, EER: 1.45%, Threshold: 0.15%\n",
      "\n",
      "Epoch 68 Summary: Avg Train Loss: 0.0021, Val Loss: 0.0340, EER: 1.45%, Threshold: 0.17%\n",
      "\n",
      "Epoch 69 Summary: Avg Train Loss: 0.0019, Val Loss: 0.0338, EER: 1.45%, Threshold: 0.18%\n",
      "\n",
      "Epoch 70 Summary: Avg Train Loss: 0.0021, Val Loss: 0.0328, EER: 1.42%, Threshold: 0.21%\n",
      "Training finished.\n",
      "\n",
      "--- Starting Trial with Seed: 42 ---\n",
      "Training started on device: cuda\n",
      "Evaluating on validation set before training...\n",
      "Initial Validation - Loss: 0.6820, EER: 57.02%, Threshold: 49.23%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d0eeb706eed499086210673dcd1ff4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99750 [00:00<?, ?batch/s, train_loss=N/A, val_eer=57.02%, val_loss=0.682, val_threshold=49.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary: Avg Train Loss: 0.2994, Val Loss: 0.1180, EER: 5.77%, Threshold: 39.67%\n",
      "\n",
      "Epoch 2 Summary: Avg Train Loss: 0.1440, Val Loss: 0.1150, EER: 4.66%, Threshold: 51.83%\n",
      "\n",
      "Epoch 3 Summary: Avg Train Loss: 0.1062, Val Loss: 0.0652, EER: 4.15%, Threshold: 27.45%\n",
      "\n",
      "Epoch 4 Summary: Avg Train Loss: 0.0830, Val Loss: 0.0590, EER: 3.69%, Threshold: 27.66%\n",
      "\n",
      "Epoch 5 Summary: Avg Train Loss: 0.0683, Val Loss: 0.0470, EER: 3.38%, Threshold: 17.65%\n",
      "\n",
      "Epoch 6 Summary: Avg Train Loss: 0.0588, Val Loss: 0.0436, EER: 3.14%, Threshold: 16.11%\n",
      "\n",
      "Epoch 7 Summary: Avg Train Loss: 0.0491, Val Loss: 0.0410, EER: 2.98%, Threshold: 8.99%\n",
      "\n",
      "Epoch 8 Summary: Avg Train Loss: 0.0430, Val Loss: 0.0341, EER: 2.56%, Threshold: 8.78%\n",
      "\n",
      "Epoch 9 Summary: Avg Train Loss: 0.0368, Val Loss: 0.0347, EER: 2.63%, Threshold: 6.35%\n",
      "\n",
      "Epoch 10 Summary: Avg Train Loss: 0.0340, Val Loss: 0.0328, EER: 2.20%, Threshold: 21.10%\n",
      "\n",
      "Epoch 11 Summary: Avg Train Loss: 0.0300, Val Loss: 0.0772, EER: 2.43%, Threshold: 64.04%\n",
      "\n",
      "Epoch 12 Summary: Avg Train Loss: 0.0271, Val Loss: 0.0280, EER: 2.16%, Threshold: 6.04%\n",
      "\n",
      "Epoch 13 Summary: Avg Train Loss: 0.0241, Val Loss: 0.0299, EER: 1.96%, Threshold: 19.93%\n",
      "\n",
      "Epoch 14 Summary: Avg Train Loss: 0.0238, Val Loss: 0.0317, EER: 1.84%, Threshold: 1.23%\n",
      "\n",
      "Epoch 15 Summary: Avg Train Loss: 0.0193, Val Loss: 0.0253, EER: 1.89%, Threshold: 3.74%\n",
      "\n",
      "Epoch 16 Summary: Avg Train Loss: 0.0173, Val Loss: 0.0253, EER: 1.76%, Threshold: 3.54%\n",
      "\n",
      "Epoch 17 Summary: Avg Train Loss: 0.0169, Val Loss: 0.0249, EER: 1.85%, Threshold: 3.13%\n",
      "\n",
      "Epoch 18 Summary: Avg Train Loss: 0.0172, Val Loss: 0.0230, EER: 1.57%, Threshold: 1.94%\n",
      "\n",
      "Epoch 19 Summary: Avg Train Loss: 0.0131, Val Loss: 0.0242, EER: 1.42%, Threshold: 1.21%\n",
      "\n",
      "Epoch 20 Summary: Avg Train Loss: 0.0132, Val Loss: 0.0377, EER: 1.70%, Threshold: 0.28%\n",
      "\n",
      "Epoch 21 Summary: Avg Train Loss: 0.0135, Val Loss: 0.0200, EER: 1.57%, Threshold: 3.63%\n",
      "\n",
      "Epoch 22 Summary: Avg Train Loss: 0.0109, Val Loss: 0.0183, EER: 1.30%, Threshold: 3.07%\n",
      "\n",
      "Epoch 23 Summary: Avg Train Loss: 0.0115, Val Loss: 0.0223, EER: 1.53%, Threshold: 3.06%\n",
      "\n",
      "Epoch 24 Summary: Avg Train Loss: 0.0112, Val Loss: 0.0200, EER: 1.45%, Threshold: 2.73%\n",
      "\n",
      "Epoch 25 Summary: Avg Train Loss: 0.0106, Val Loss: 0.0167, EER: 1.26%, Threshold: 5.07%\n",
      "\n",
      "Epoch 26 Summary: Avg Train Loss: 0.0088, Val Loss: 0.0284, EER: 1.41%, Threshold: 0.51%\n",
      "\n",
      "Epoch 27 Summary: Avg Train Loss: 0.0090, Val Loss: 0.0249, EER: 1.41%, Threshold: 0.78%\n",
      "\n",
      "Epoch 28 Summary: Avg Train Loss: 0.0087, Val Loss: 0.0243, EER: 1.14%, Threshold: 0.72%\n",
      "\n",
      "Epoch 29 Summary: Avg Train Loss: 0.0084, Val Loss: 0.0287, EER: 1.25%, Threshold: 0.29%\n",
      "\n",
      "Epoch 30 Summary: Avg Train Loss: 0.0077, Val Loss: 0.0260, EER: 1.42%, Threshold: 0.63%\n",
      "\n",
      "Epoch 31 Summary: Avg Train Loss: 0.0081, Val Loss: 0.0202, EER: 1.18%, Threshold: 0.96%\n",
      "\n",
      "Epoch 32 Summary: Avg Train Loss: 0.0067, Val Loss: 0.0265, EER: 1.14%, Threshold: 0.41%\n",
      "\n",
      "Epoch 33 Summary: Avg Train Loss: 0.0070, Val Loss: 0.0393, EER: 1.38%, Threshold: 0.07%\n",
      "\n",
      "Epoch 34 Summary: Avg Train Loss: 0.0041, Val Loss: 0.0373, EER: 1.21%, Threshold: 0.08%\n",
      "\n",
      "Epoch 35 Summary: Avg Train Loss: 0.0035, Val Loss: 0.0170, EER: 0.94%, Threshold: 1.83%\n",
      "\n",
      "Epoch 36 Summary: Avg Train Loss: 0.0036, Val Loss: 0.0197, EER: 1.17%, Threshold: 1.71%\n",
      "\n",
      "Epoch 37 Summary: Avg Train Loss: 0.0039, Val Loss: 0.0230, EER: 1.06%, Threshold: 0.32%\n",
      "\n",
      "Epoch 38 Summary: Avg Train Loss: 0.0027, Val Loss: 0.0201, EER: 0.97%, Threshold: 0.63%\n",
      "\n",
      "Epoch 39 Summary: Avg Train Loss: 0.0028, Val Loss: 0.0244, EER: 1.09%, Threshold: 0.27%\n",
      "\n",
      "Epoch 40 Summary: Avg Train Loss: 0.0031, Val Loss: 0.0182, EER: 0.82%, Threshold: 0.52%\n",
      "\n",
      "Epoch 41 Summary: Avg Train Loss: 0.0022, Val Loss: 0.0192, EER: 0.94%, Threshold: 0.63%\n",
      "\n",
      "Epoch 42 Summary: Avg Train Loss: 0.0030, Val Loss: 0.0324, EER: 0.90%, Threshold: 0.08%\n",
      "\n",
      "Epoch 43 Summary: Avg Train Loss: 0.0029, Val Loss: 0.0247, EER: 0.98%, Threshold: 0.19%\n",
      "\n",
      "Epoch 44 Summary: Avg Train Loss: 0.0024, Val Loss: 0.0271, EER: 1.06%, Threshold: 0.19%\n",
      "\n",
      "Epoch 45 Summary: Avg Train Loss: 0.0031, Val Loss: 0.0188, EER: 1.13%, Threshold: 1.09%\n",
      "\n",
      "Epoch 46 Summary: Avg Train Loss: 0.0022, Val Loss: 0.0234, EER: 0.93%, Threshold: 0.19%\n",
      "\n",
      "Epoch 47 Summary: Avg Train Loss: 0.0017, Val Loss: 0.0290, EER: 0.87%, Threshold: 0.08%\n",
      "\n",
      "Epoch 48 Summary: Avg Train Loss: 0.0017, Val Loss: 0.0190, EER: 0.86%, Threshold: 0.50%\n",
      "\n",
      "Epoch 49 Summary: Avg Train Loss: 0.0017, Val Loss: 0.0239, EER: 0.79%, Threshold: 0.21%\n",
      "Epoch 49: New best model saved to 'best_model' with EER: 0.0079\n",
      "\n",
      "Epoch 50 Summary: Avg Train Loss: 0.0015, Val Loss: 0.0356, EER: 0.91%, Threshold: 0.04%\n",
      "\n",
      "Epoch 51 Summary: Avg Train Loss: 0.0016, Val Loss: 0.0291, EER: 0.83%, Threshold: 0.07%\n",
      "\n",
      "Epoch 52 Summary: Avg Train Loss: 0.0018, Val Loss: 0.0283, EER: 0.91%, Threshold: 0.10%\n",
      "\n",
      "Epoch 53 Summary: Avg Train Loss: 0.0016, Val Loss: 0.0450, EER: 0.86%, Threshold: 0.01%\n",
      "\n",
      "Epoch 54 Summary: Avg Train Loss: 0.0017, Val Loss: 0.0258, EER: 0.90%, Threshold: 0.14%\n",
      "\n",
      "Epoch 55 Summary: Avg Train Loss: 0.0010, Val Loss: 0.0246, EER: 0.82%, Threshold: 0.16%\n",
      "\n",
      "Epoch 56 Summary: Avg Train Loss: 0.0008, Val Loss: 0.0236, EER: 0.82%, Threshold: 0.15%\n",
      "\n",
      "Epoch 57 Summary: Avg Train Loss: 0.0009, Val Loss: 0.0239, EER: 0.86%, Threshold: 0.16%\n",
      "\n",
      "Epoch 58 Summary: Avg Train Loss: 0.0009, Val Loss: 0.0349, EER: 0.90%, Threshold: 0.03%\n",
      "\n",
      "Epoch 59 Summary: Avg Train Loss: 0.0010, Val Loss: 0.0356, EER: 0.87%, Threshold: 0.03%\n",
      "\n",
      "Epoch 60 Summary: Avg Train Loss: 0.0013, Val Loss: 0.0285, EER: 0.86%, Threshold: 0.08%\n",
      "\n",
      "Epoch 61 Summary: Avg Train Loss: 0.0009, Val Loss: 0.0313, EER: 0.86%, Threshold: 0.05%\n",
      "\n",
      "Epoch 62 Summary: Avg Train Loss: 0.0008, Val Loss: 0.0246, EER: 0.90%, Threshold: 0.10%\n",
      "\n",
      "Epoch 63 Summary: Avg Train Loss: 0.0007, Val Loss: 0.0298, EER: 0.86%, Threshold: 0.06%\n",
      "\n",
      "Epoch 64 Summary: Avg Train Loss: 0.0012, Val Loss: 0.0265, EER: 0.87%, Threshold: 0.09%\n",
      "\n",
      "Epoch 65 Summary: Avg Train Loss: 0.0008, Val Loss: 0.0299, EER: 0.86%, Threshold: 0.06%\n",
      "\n",
      "Epoch 66 Summary: Avg Train Loss: 0.0008, Val Loss: 0.0253, EER: 0.79%, Threshold: 0.12%\n",
      "\n",
      "Epoch 67 Summary: Avg Train Loss: 0.0006, Val Loss: 0.0253, EER: 0.78%, Threshold: 0.12%\n",
      "Epoch 67: New best model saved to 'best_model' with EER: 0.0078\n",
      "\n",
      "Epoch 68 Summary: Avg Train Loss: 0.0007, Val Loss: 0.0256, EER: 0.82%, Threshold: 0.10%\n",
      "\n",
      "Epoch 69 Summary: Avg Train Loss: 0.0008, Val Loss: 0.0308, EER: 0.82%, Threshold: 0.05%\n",
      "\n",
      "Epoch 70 Summary: Avg Train Loss: 0.0008, Val Loss: 0.0356, EER: 0.86%, Threshold: 0.03%\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = config.epochs\n",
    "min_eer = float('inf')\n",
    "best_model_filename = 'best_model'\n",
    "\n",
    "for seed in config.seeds:\n",
    "    print(f\"\\n--- Starting Trial with Seed: {seed} ---\")\n",
    "    # set_seed(seed)\n",
    "    # torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    min_eer = train_model(criterion, train_dataloader, dev_dataloader, NUM_EPOCHS, min_eer, best_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd7ece4",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2840319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging the best model (best_model) to W&B Artifacts...\n",
      "Best model logged as W&B Artifact.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>dev_eer</td><td>▃▂▂▂▂▁▁▁▁▁▆▆▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂█▄▄▃▃▂▂▁▁▂▁▁</td></tr><tr><td>dev_loss</td><td>▄▄▁▁▁▂▁▁▂▁▂▁█▄▃▂▂▂▃▂▂▂▃▃▃▂▂▂▁▂▃▂▁▂▁▂▃▂▂▂</td></tr><tr><td>dev_threshold</td><td>█▄▃▄▂▂▁▁▁▁▁▁▁▁▁▄▃▂▃▁▁▁▁▁▁▁▁▁▁▃▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▄▄▄▅▅▇▇███▁▂▂▃▃▄▄▄▅▆▇▇█▁▁▃▃▃▄▅▆▇▇▇▇</td></tr><tr><td>train_loss</td><td>█▅▂▁▁▁▁▁▁▁█▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▅▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_eer_epoch</td><td>67</td></tr><tr><td>best_validation_eer</td><td>0.00785</td></tr><tr><td>dev_eer</td><td>0.00865</td></tr><tr><td>dev_loss</td><td>0.03562</td></tr><tr><td>dev_threshold</td><td>0.00028</td></tr><tr><td>epoch</td><td>70</td></tr><tr><td>train_loss</td><td>0.00082</td></tr><tr><td>validation_loss_at_best_eer</td><td>0.02527</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Training_10</strong> at: <a href='https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake/runs/8l5i2p10' target=\"_blank\">https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake/runs/8l5i2p10</a><br> View project at: <a href='https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake' target=\"_blank\">https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake</a><br>Synced 5 W&B file(s), 62 media file(s), 108 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250705_070438-8l5i2p10/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B run finished.\n"
     ]
    }
   ],
   "source": [
    "if min_eer != float('inf'):\n",
    "    print(f\"Logging the best model ({best_model_filename}) to W&B Artifacts...\")\n",
    "    best_model_artifact = wandb.Artifact(\n",
    "        name=f\"{run.id}-best-model\", # Using run ID for uniqueness\n",
    "        type=\"model\",\n",
    "        description=f\"Best model according to EER ({min_eer:.4f}) achieved at epoch {run.summary.get('best_eer_epoch', 'N/A')}.\",\n",
    "        metadata={\"best_eer\": min_eer, \"epoch_of_best_eer\": run.summary.get('best_eer_epoch', 'N/A')}\n",
    "    )\n",
    "    best_model_artifact.add_file(best_model_filename) # Add the saved file\n",
    "    wandb.run.log_artifact(best_model_artifact, aliases=[\"best_eer_model\"]) # Add an alias\n",
    "    print(\"Best model logged as W&B Artifact.\")\n",
    "else:\n",
    "    print(\"No model was saved as best_eer did not improve from its initial value.\")\n",
    "\n",
    "run.finish()\n",
    "\n",
    "print(\"W&B run finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc0200f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
