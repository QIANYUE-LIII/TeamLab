{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "222cac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import eval_metrics as em\n",
    "import wandb\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89eddae",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "006b2c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mqianyue\u001b[0m (\u001b[33mqianyue-university-of-stuttgart\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/users1/liqe/TeamLab_phonetics/TeamLab/wandb/run-20250706_100937-2ts3za1k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake/runs/2ts3za1k' target=\"_blank\">Training_08_CNN</a></strong> to <a href='https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake' target=\"_blank\">https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake/runs/2ts3za1k' target=\"_blank\">https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake/runs/2ts3za1k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project = \"teamlab_deepfake\",\n",
    "    name = \"Training_08_CNN\",     #Training_XX\n",
    "    notes = None,\n",
    "    tags = [\"ALL_FEATURE\", \"COMBINED_MODEL\", \"HNR\", \"PITCH\", \"JITTER&SHIMMER\", \"MFCC\"],\n",
    "    config={\n",
    "        #NOTE: set manually\n",
    "        \"model\": \"CNN_classifier\",   #   SpoofEnsemble/LSTM_FFN_classifier/CNN_classifier/SpoofEnsemble_attention\n",
    "        \"dataset\": \"ASVSpoof19_LA\",    \n",
    "        \"feature\": \"MFCC&Prosody\",\n",
    "        \"attack_type\": \"all\",   # all/A01/A02/A03/A04/A05/A06\n",
    "        \"loss_function\": \"weighted_CE\",\n",
    "        #\n",
    "        \"scheduler\": False,\n",
    "        \"scheduler_factor\": 0.5,\n",
    "        \"scheduler_patience\": 4,\n",
    "        \"epochs\": 70,\n",
    "        \"batch_size\": 32,\n",
    "        \"oversampling\": True,\n",
    "        \"learning_rate\": 5e-4,\n",
    "        \"dropout_rate\": 0.3,\n",
    "        # lstm layer\n",
    "        \"lstm_input_dim\": 2,\n",
    "        \"lstm_hidden_dim\": 64,\n",
    "        \"bidirectional\": False,\n",
    "        \"lstm_n_layers\":1,\n",
    "        # fnn layer\n",
    "        \"ffn_dims\": [11, 64], # in, out -\n",
    "        # cnn layer\n",
    "        \"cnn_channels\": [1, 32, 64, 128],   #in, out -\n",
    "        \"conv_kernel\": (3,3),\n",
    "        \"pool_kernel\": (2,2),\n",
    "        \"cnn_padding\": 1,\n",
    "        # random seeds\n",
    "        \"seeds\": [0,7,42]\n",
    "    },\n",
    ")\n",
    "\n",
    "config = run.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970b71e6",
   "metadata": {},
   "source": [
    ">NOTE: attack types are evenly distributed in training and dev dataset, and each has higher number than genuine voices, so no further balancing is needed>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "621b14ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "PITCH_COLUMN = 'PITCH'\n",
    "HNR_COLUMN = 'HNR'\n",
    "JITTER_COLUMN = 'JITTER'\n",
    "SHIMMER_COLUMN = 'SHIMMER'\n",
    "MFCC_COLUMN = 'MFCC'\n",
    "LABEL_COLUMN = 'LABEL'      \n",
    "                           \n",
    "NAN_REPLACEMENT_VALUE = 0.0  \n",
    "PADDING_VALUE = 0.0         \n",
    "LABEL_BONAFIDE = 1\n",
    "LABEL_SPOOF = 0\n",
    "\n",
    "train_features_path = '/home/users1/liqe/TeamLab_phonetics/merged_train_com.pkl'\n",
    "dev_features_path = '/home/users1/liqe/TeamLab_phonetics/merged_dev_com.pkl'\n",
    "\n",
    "df_train = pd.read_pickle(train_features_path)\n",
    "df_dev = pd.read_pickle(dev_features_path)\n",
    "\n",
    "# NOTE: if training on a specific attack type\n",
    "if config.attack_type != \"all\":\n",
    "    df_train = df_train[df_train['ATTACK_TYPE'].isin([config.attack_type,'-'])]\n",
    "    df_dev = df_dev[df_dev['ATTACK_TYPE'].isin([config.attack_type,'-'])]\n",
    "elif config.attack_type == \"all\":\n",
    "    pass\n",
    "elif config.attack_type != (\"A01\" or \"A02\" or \"A03\" or \"A04\" or \"A05\" or \"A06\"):\n",
    "    print(\"WARNING: invalid attack type.\")\n",
    "\n",
    "# # inspect\n",
    "# print(df_train.head())\n",
    "# print(df_train.groupby('ATTACK_TYPE').count())\n",
    "\n",
    "# print(\"\\n\")\n",
    "# print(df_dev.head())\n",
    "# print(df_dev.groupby('ATTACK_TYPE').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82fb4c6",
   "metadata": {},
   "source": [
    "#### Set the random seeds for replicability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5173b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def set_seed(seed):\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed_all(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     random.seed(seed)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "#     torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c9ec96",
   "metadata": {},
   "source": [
    "### Training Data Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f385ade",
   "metadata": {},
   "source": [
    ">NOTE: training audio & labels are matched, dev are not (Solved: excessive rows are deleted beforehand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d12dd51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resampled X (DataFrame) head:\n",
      "       AUDIO_ID ATTACK_TYPE  \\\n",
      "0  LA_T_1000137         A04   \n",
      "1  LA_T_1000406           -   \n",
      "2  LA_T_1000648         A01   \n",
      "3  LA_T_1000824         A04   \n",
      "4  LA_T_1001074         A03   \n",
      "\n",
      "                                               PITCH  \\\n",
      "0  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "1  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "2  [nan, nan, nan, nan, nan, 0.35835335, 0.350411...   \n",
      "3  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "4  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "\n",
      "                                                 HNR  \\\n",
      "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7987432, 0.79...   \n",
      "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "\n",
      "                                              JITTER  \\\n",
      "0  [0.30073947, 0.23022015, 0.26707897, 0.2538646...   \n",
      "1  [0.1494679, 0.09972349, 0.086890295, 0.0791289...   \n",
      "2  [0.13972145, 0.05122372, 0.123514704, 0.096691...   \n",
      "3  [0.28089172, 0.2684494, 0.23122567, 0.24996407...   \n",
      "4  [0.14094485, 0.068655394, 0.07130514, 0.062871...   \n",
      "\n",
      "                                             SHIMMER  \\\n",
      "0  [0.27033818, 0.33233136, 0.11353744, 0.1519924...   \n",
      "1  [0.13012205, 0.15872449, 0.059600886, 0.081321...   \n",
      "2  [0.28141773, 0.3631905, 0.12680757, 0.17373542...   \n",
      "3  [0.2846507, 0.35503966, 0.13182917, 0.15244015...   \n",
      "4  [0.16095506, 0.21579938, 0.039854582, 0.081848...   \n",
      "\n",
      "                                                MFCC  \n",
      "0  [[0.14843875, 0.16200094, 0.18541528, 0.279557...  \n",
      "1  [[0.30532825, 0.3215196, 0.31294125, 0.3068853...  \n",
      "2  [[0.22124906, 0.34767622, 0.5816188, 0.6628496...  \n",
      "3  [[0.11835651, 0.12180926, 0.11639575, 0.117103...  \n",
      "4  [[0.19357309, 0.19469197, 0.19534206, 0.195999...  \n",
      "\n",
      "Resampled y (Series) head:\n",
      "0    0\n",
      "1    1\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: LABEL, dtype: int8\n",
      "\n",
      "Resampled class distribution (from y_resampled_series):\n",
      "Counter({0: 22799, 1: 22799})\n",
      "\n",
      "Combined Resampled DataFrame head:\n",
      "       AUDIO_ID ATTACK_TYPE  \\\n",
      "0  LA_T_1000137         A04   \n",
      "1  LA_T_1000406           -   \n",
      "2  LA_T_1000648         A01   \n",
      "3  LA_T_1000824         A04   \n",
      "4  LA_T_1001074         A03   \n",
      "\n",
      "                                               PITCH  \\\n",
      "0  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "1  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "2  [nan, nan, nan, nan, nan, 0.35835335, 0.350411...   \n",
      "3  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "4  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "\n",
      "                                                 HNR  \\\n",
      "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7987432, 0.79...   \n",
      "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "\n",
      "                                              JITTER  \\\n",
      "0  [0.30073947, 0.23022015, 0.26707897, 0.2538646...   \n",
      "1  [0.1494679, 0.09972349, 0.086890295, 0.0791289...   \n",
      "2  [0.13972145, 0.05122372, 0.123514704, 0.096691...   \n",
      "3  [0.28089172, 0.2684494, 0.23122567, 0.24996407...   \n",
      "4  [0.14094485, 0.068655394, 0.07130514, 0.062871...   \n",
      "\n",
      "                                             SHIMMER  \\\n",
      "0  [0.27033818, 0.33233136, 0.11353744, 0.1519924...   \n",
      "1  [0.13012205, 0.15872449, 0.059600886, 0.081321...   \n",
      "2  [0.28141773, 0.3631905, 0.12680757, 0.17373542...   \n",
      "3  [0.2846507, 0.35503966, 0.13182917, 0.15244015...   \n",
      "4  [0.16095506, 0.21579938, 0.039854582, 0.081848...   \n",
      "\n",
      "                                                MFCC  LABEL  \n",
      "0  [[0.14843875, 0.16200094, 0.18541528, 0.279557...      0  \n",
      "1  [[0.30532825, 0.3215196, 0.31294125, 0.3068853...      1  \n",
      "2  [[0.22124906, 0.34767622, 0.5816188, 0.6628496...      0  \n",
      "3  [[0.11835651, 0.12180926, 0.11639575, 0.117103...      0  \n",
      "4  [[0.19357309, 0.19469197, 0.19534206, 0.195999...      0  \n",
      "\n",
      "Combined Resampled DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45598 entries, 0 to 45597\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   AUDIO_ID     45598 non-null  object\n",
      " 1   ATTACK_TYPE  45598 non-null  object\n",
      " 2   PITCH        45598 non-null  object\n",
      " 3   HNR          45598 non-null  object\n",
      " 4   JITTER       45598 non-null  object\n",
      " 5   SHIMMER      45598 non-null  object\n",
      " 6   MFCC         45598 non-null  object\n",
      " 7   LABEL        45598 non-null  int8  \n",
      "dtypes: int8(1), object(7)\n",
      "memory usage: 2.5+ MB\n",
      "\n",
      "Combined Resampled DataFrame class distribution:\n",
      "Counter({0: 22799, 1: 22799})\n"
     ]
    }
   ],
   "source": [
    "if config.oversampling:\n",
    "    X = df_train.drop('LABEL', axis=1)\n",
    "    y = df_train['LABEL']\n",
    "\n",
    "    over = RandomOverSampler(random_state=config.seeds[0])\n",
    "    X_resampled_np, y_resampled_np = over.fit_resample(X, y) \n",
    "\n",
    "    X_resampled_df = pd.DataFrame(X_resampled_np, columns=X.columns)\n",
    "    y_resampled_series = pd.Series(y_resampled_np, name=y.name)\n",
    "\n",
    "    print(\"\\nResampled X (DataFrame) head:\")\n",
    "    print(X_resampled_df.head())\n",
    "    print(\"\\nResampled y (Series) head:\")\n",
    "    print(y_resampled_series.head())\n",
    "    print(\"\\nResampled class distribution (from y_resampled_series):\")\n",
    "    print(Counter(y_resampled_series))\n",
    "\n",
    "    df_train = pd.concat([X_resampled_df, y_resampled_series], axis=1)\n",
    "\n",
    "    print(\"\\nCombined Resampled DataFrame head:\")\n",
    "    print(df_train.head())\n",
    "    print(\"\\nCombined Resampled DataFrame info:\")\n",
    "    df_train.info()\n",
    "    print(\"\\nCombined Resampled DataFrame class distribution:\")\n",
    "    print(Counter(df_train['LABEL'])) # Verify target column in the new DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057ec143",
   "metadata": {},
   "source": [
    "### Padding and Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccbc2366",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASVDataset(Dataset):\n",
    "    def __init__(self, dataframe, pitch_col, hnr_col, jitter_col, shimmer_col, mfcc_col, label_col, nan_replacement=NAN_REPLACEMENT_VALUE):\n",
    "        \n",
    "        self.labels = []\n",
    "        self.processed_pitchhnr = []\n",
    "        self.global_features = []\n",
    "        self.processed_mfcc = []\n",
    "        \n",
    "        print(f\"Attempting to process {len(dataframe)} entries from DataFrame\")\n",
    "        found_count = 0\n",
    "        # Iterate through the DataFrame, process and pad the features\n",
    "        for index, row in dataframe.iterrows():  \n",
    "            if not np.isnan(row[label_col]):\n",
    "                self.labels.append(row[label_col])\n",
    "\n",
    "                pitch_sequence_raw = row[pitch_col]\n",
    "                processed_pitch = np.nan_to_num(pitch_sequence_raw, nan=nan_replacement)\n",
    "                \n",
    "                hnr_sequence_raw = row[hnr_col]\n",
    "                processed_hnr = np.nan_to_num(hnr_sequence_raw, nan=nan_replacement)\n",
    "\n",
    "                ### NOTE:need to pad the two sequences to the same length\n",
    "                max_length = max(len(processed_pitch), len(processed_hnr))\n",
    "                if len(processed_pitch) > len(processed_hnr):\n",
    "                    padding = np.zeros(max_length - len(processed_hnr), dtype=processed_hnr.dtype)\n",
    "                    processed_hnr = np.concatenate((processed_hnr, padding))\n",
    "                else:\n",
    "                    padding = np.zeros(max_length - len(processed_pitch), dtype=processed_pitch.dtype)\n",
    "                    processed_pitch = np.concatenate((processed_pitch, padding))\n",
    "\n",
    "                combined_features = np.stack((processed_pitch, processed_hnr), axis=-1) \n",
    "                self.processed_pitchhnr.append(torch.tensor(combined_features, dtype=torch.float32))\n",
    "\n",
    "                # process and combine jitter and shimmer to one sequence\n",
    "                processed_jitter = np.nan_to_num(row[jitter_col], nan=nan_replacement)\n",
    "                processed_shimmer = np.nan_to_num(row[shimmer_col], nan=nan_replacement)\n",
    "                jitter_shimmer = np.concatenate((processed_jitter, processed_shimmer))\n",
    "                self.global_features.append(torch.tensor(jitter_shimmer, dtype=torch.float32))\n",
    "                \n",
    "                # process mfcc\n",
    "                mfcc = row[mfcc_col]\n",
    "                # NOTE: need transpose for padding (time, feature_dim)\n",
    "                self.processed_mfcc.append(torch.tensor(mfcc, dtype=torch.float32).T)\n",
    "\n",
    "                found_count += 1\n",
    "        \n",
    "        self.labels = torch.tensor(self.labels, dtype=torch.long) \n",
    "        print(f\"Successfully processed {found_count} samples out of {len(dataframe)} DataFrame entries.\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of matched samples in the dataset.\"\"\"\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns one sample from the dataset: a preprocessed pitch sequence and its label.\n",
    "        \"\"\"\n",
    "        label = self.labels[idx]\n",
    "        pitch_hnr = self.processed_pitchhnr[idx]\n",
    "        global_feature = self.global_features[idx]\n",
    "        mfcc = self.processed_mfcc[idx]\n",
    "        return label, pitch_hnr, global_feature, mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0f4a79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- for Dynamic Padding  ---\n",
    "def collate_fn(batch, padding_value=PADDING_VALUE):\n",
    "    \"\"\"\n",
    "    Pads sequences within a batch to the same length.\n",
    "    \"\"\"\n",
    "    labels = [item[0] for item in batch]\n",
    "    pitch_hnrs = [item[1] for item in batch]\n",
    "    global_features = [item[2] for item in batch]\n",
    "    mfccs = [item[3] for item in batch]\n",
    "\n",
    "    labels = torch.stack(labels)\n",
    "\n",
    "    pitchhnr_lengths = torch.tensor([len(seq) for seq in pitch_hnrs], dtype=torch.long)\n",
    "    padded_pitchhnrs = pad_sequence(pitch_hnrs, batch_first=True, padding_value=padding_value)\n",
    "    if padded_pitchhnrs.ndim == 2:     # lstm expects: [batch_size, sequence_length, feature_size]\n",
    "        padded_pitchhnrs = padded_pitchhnrs.unsqueeze(2)\n",
    "\n",
    "    global_features = torch.stack(global_features)\n",
    "\n",
    "    padded_mfccs = pad_sequence(mfccs, batch_first=True, padding_value=padding_value)\n",
    "\n",
    "    return labels, pitchhnr_lengths, padded_pitchhnrs, global_features, padded_mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67d92225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to process 45598 entries from DataFrame\n",
      "Successfully processed 45598 samples out of 45598 DataFrame entries.\n",
      "Attempting to process 24844 entries from DataFrame\n",
      "Successfully processed 24844 samples out of 24844 DataFrame entries.\n",
      "\n",
      "--- Batch 1 ---\n",
      "  Labels (first 5): tensor([0, 1, 1, 0, 1])\n",
      "  Padded Sequences Shape: torch.Size([32, 568, 2])\n",
      "  Original Lengths (first 5): tensor([152, 270, 351, 568, 249])\n",
      "  Global Shape: torch.Size([32, 11])\n",
      "  MFCC Shape: torch.Size([32, 179, 60])\n"
     ]
    }
   ],
   "source": [
    "pitch_dataset_train = ASVDataset(dataframe=df_train,   \n",
    "                                    pitch_col=PITCH_COLUMN,\n",
    "                                    hnr_col=HNR_COLUMN,\n",
    "                                    jitter_col=JITTER_COLUMN,\n",
    "                                    shimmer_col=SHIMMER_COLUMN,\n",
    "                                    mfcc_col=MFCC_COLUMN,\n",
    "                                    label_col=LABEL_COLUMN,\n",
    "                                    nan_replacement=NAN_REPLACEMENT_VALUE)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    pitch_dataset_train, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn, num_workers=8\n",
    ")\n",
    "\n",
    "pitch_dataset_dev = ASVDataset(dataframe=df_dev,   \n",
    "                                    pitch_col=PITCH_COLUMN,\n",
    "                                    hnr_col=HNR_COLUMN,\n",
    "                                    jitter_col=JITTER_COLUMN,\n",
    "                                    shimmer_col=SHIMMER_COLUMN,\n",
    "                                    mfcc_col=MFCC_COLUMN,\n",
    "                                    label_col=LABEL_COLUMN,\n",
    "                                    nan_replacement=NAN_REPLACEMENT_VALUE)\n",
    "\n",
    "dev_dataloader = DataLoader(\n",
    "    pitch_dataset_dev, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn, num_workers=8\n",
    ")\n",
    "\n",
    "## For inspection\n",
    "for i, batch_data in enumerate(train_dataloader):\n",
    "    # batch_data is a tuple\n",
    "    batch_labels, batch_lengths, batch_pitchhnr, batch_global, batch_mfcc = batch_data\n",
    "    print(f\"\\n--- Batch {i+1} ---\")\n",
    "    print(f\"  Labels (first 5): {batch_labels[:5]}\")\n",
    "    print(f\"  Padded Sequences Shape: {batch_pitchhnr.shape}\")\n",
    "    print(f\"  Original Lengths (first 5): {batch_lengths[:5]}\")\n",
    "    print(f\"  Global Shape: {batch_global.shape}\")\n",
    "    print(f\"  MFCC Shape: {batch_mfcc.shape}\")\n",
    "    \n",
    "\n",
    "    if i == 0: # Break after the first batch for inspection\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdbd83f",
   "metadata": {},
   "source": [
    "### Finding the weight (for weighted cross entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ac071d",
   "metadata": {},
   "source": [
    "is there different ways calculating weitghs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f67837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = df_train['LABEL']   \n",
    "total = len(labels)\n",
    "count_bonafide = labels.value_counts().get(LABEL_BONAFIDE, 0)\n",
    "count_spoof =  total - count_bonafide\n",
    "weight_bonafide = total / (count_bonafide * 2)\n",
    "weight_spoof = total / (count_spoof * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3d85b7",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8a24da",
   "metadata": {},
   "source": [
    "#### LSTM&FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41eb3605",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_FFN_branch(nn.Module):\n",
    "    def __init__(self, lstm_input_dim, lstm_hidden_dim, lstm_n_layers, bidirectional, \n",
    "                 ffn_dims):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm_ffn_dim = (lstm_hidden_dim * 2 if bidirectional else lstm_hidden_dim) + ffn_dims[-1]\n",
    "        self.ffn_layers = nn.ModuleList()\n",
    "\n",
    "        # 1. lstm layer\n",
    "        self.lstm = nn.LSTM(lstm_input_dim, \n",
    "                            lstm_hidden_dim, \n",
    "                            num_layers=lstm_n_layers, \n",
    "                            bidirectional=bidirectional, \n",
    "                            batch_first=True) # Input/output tensors are (batch, seq, feature)\n",
    "        # BN layer for stabalization\n",
    "        self.bn_lstm = nn.BatchNorm1d(lstm_hidden_dim * 2 if bidirectional else lstm_hidden_dim)\n",
    "        \n",
    "        # 2. ffn layer\n",
    "        for i in range(len(ffn_dims) -1):\n",
    "            ffn_input_dim = ffn_dims[i]\n",
    "            ffn_hidden_dim = ffn_dims[i+1]\n",
    "            ffn_block = nn.Sequential(\n",
    "                nn.Linear(ffn_input_dim, ffn_hidden_dim),\n",
    "                nn.BatchNorm1d(ffn_hidden_dim),    # BN layer for stabalization\n",
    "                nn.ReLU())\n",
    "            self.ffn_layers.append(ffn_block)\n",
    "        \n",
    "        \n",
    "    def forward(self, pitch_hnrs, pitchhnr_lengths, global_features):\n",
    "      \n",
    "        # 1. Pack sequence\n",
    "        ### Compute actual data and ignore the padded values\n",
    "        packed_input = rnn_utils.pack_padded_sequence(pitch_hnrs, pitchhnr_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # 2. Pass packed sequence through LSTM\n",
    "        ### packed_output: Hidden states for every time step.\n",
    "        ### hidden: The final hidden state (summary) of the entire sequence.\n",
    "        ### cell: The final cell state (long-term memory) of the entire sequence.\n",
    "        packed_output, (lstm_hidden, cell) = self.lstm(packed_input)\n",
    "        \n",
    "        # 3. Concatenate the final forward and backward hidden states (if bidirectional)\n",
    "        if self.lstm.bidirectional:\n",
    "            lstm_hidden = torch.cat((lstm_hidden[-2,:,:], lstm_hidden[-1,:,:]), dim=1)\n",
    "        else:\n",
    "            lstm_hidden = lstm_hidden[-1,:,:]\n",
    "        lstm_hidden = self.bn_lstm(lstm_hidden)\n",
    "\n",
    "        # 4. Pass global features (jitter and shimmer) through the FFN\n",
    "        for layer in self.ffn_layers:\n",
    "            global_features = layer(global_features)\n",
    "        ffn_output = global_features\n",
    "\n",
    "        # 5. Concatenate the outputs from lstm and fnn\n",
    "        combined_output = torch.cat((lstm_hidden,ffn_output), dim=1)\n",
    "\n",
    "        return combined_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00fda60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for LSTM_FFN training alone \n",
    "class LSTM_FFN_classifer(nn.Module):\n",
    "    def __init__(self, lstm_ffn_out, output_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm_ffn_layer = lstm_ffn_out\n",
    "        self.fc = nn.Linear(lstm_ffn_out.lstm_ffn_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, pitch_hnrs, pitchhnr_lengths, global_features, _a):\n",
    "\n",
    "        lstm_ffn_out = self.lstm_ffn_layer(pitch_hnrs, pitchhnr_lengths, global_features)\n",
    "        lstm_ffn_out = self.dropout(lstm_ffn_out)\n",
    "        output = self.fc(lstm_ffn_out)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba600f70",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "290b852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_branch(nn.Module):\n",
    "    def __init__(self, cnn_channels, conv_kernel, pool_kernel, cnn_padding):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn_dim = cnn_channels[-1]\n",
    "\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "\n",
    "        for i in range(len(cnn_channels)-2):\n",
    "            cnn_in = cnn_channels[i]\n",
    "            cnn_out = cnn_channels[i+1]\n",
    "            conv_block = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=cnn_in, out_channels=cnn_out, kernel_size=conv_kernel, padding=cnn_padding),\n",
    "                nn.BatchNorm2d(cnn_out),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=pool_kernel))\n",
    "            self.conv_layers.append(conv_block)\n",
    "\n",
    "        # final layer of CNN\n",
    "        final_in = cnn_channels[-2]\n",
    "        final_out = cnn_channels[-1]\n",
    "\n",
    "        conv_final = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=final_in, out_channels=final_out, kernel_size=conv_kernel, padding=cnn_padding),\n",
    "            nn.BatchNorm2d(final_out),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool2d((1, 1))  # Output size: [batch, 64, 1, 1]\n",
    "        )\n",
    "        self.conv_layers.append(conv_final)\n",
    "        \n",
    "    def forward(self, mfccs):\n",
    "\n",
    "        # expected shape (batch_size, in_channel, height, width) -> unsqeeze\n",
    "        mfccs = mfccs.unsqueeze(1)\n",
    "\n",
    "        for layer in self.conv_layers:\n",
    "            mfccs = layer(mfccs)\n",
    "        cnn_out = mfccs.view(mfccs.size(0), -1)\n",
    "        \n",
    "        return cnn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49d26e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for CNN training alone\n",
    "class CNN_classifer(nn.Module):\n",
    "    def __init__(self, cnn_out, output_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn_layer = cnn_out\n",
    "        self.fc = nn.Linear(cnn_out.cnn_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, _a, _b, _c, mfccs):\n",
    "\n",
    "        cnn_out = self.cnn_layer(mfccs)\n",
    "        cnn_out = self.dropout(cnn_out)\n",
    "        output = self.fc(cnn_out)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707abbf4",
   "metadata": {},
   "source": [
    "#### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7e31c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BranchAttention(nn.Module):\n",
    "    def __init__(self, branch1_dim, branch2_dim):\n",
    "        super().__init__()\n",
    "        self.attention_net = nn.Linear(branch1_dim + branch2_dim, 2)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.attention_net.bias.fill_(0)\n",
    "\n",
    "    def forward(self, branch1_out, branch2_out):\n",
    "        # 1. concatenate the raw outputs from both branches\n",
    "        combined_out = torch.cat((branch1_out, branch2_out), dim=1)\n",
    "        \n",
    "        # 2. Predict the score for each branch's importance\n",
    "        attention_scores = self.attention_net(combined_out)\n",
    "        \n",
    "        # 3. turn scores into weights that sum to 1 (e.g., [0.7, 0.3])\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)\n",
    "        \n",
    "        # 4. Get the individual weight for each branch\n",
    "        # .unsqueeze(1) is needed to make the dimensions compatible for multiplication\n",
    "        branch1_weight = attention_weights[:, 0].unsqueeze(1)\n",
    "        branch2_weight = attention_weights[:, 1].unsqueeze(1)\n",
    "        \n",
    "        # 5. Scale each branch's output by its learned weight\n",
    "        branch1_weighted = branch1_out * branch1_weight\n",
    "        branch2_weighted = branch2_out * branch2_weight\n",
    "        \n",
    "        # 6. Concatenate the *weighted* features to pass to the final classifier\n",
    "        weighted_combined_features = torch.cat((branch1_weighted, branch2_weighted), dim=1)\n",
    "        \n",
    "        # Return the combined features and the weights for inspection\n",
    "        return weighted_combined_features, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f942586f",
   "metadata": {},
   "source": [
    "#### Emsemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbba4a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpoofEnsemble(nn.Module):\n",
    "    def __init__(self, lstm_ffn_branch, cnn_branch, output_dim, dropout):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm_ffn_branch = lstm_ffn_branch\n",
    "        self.cnn_branch = cnn_branch\n",
    "\n",
    "        lstm_ffn_dim = lstm_ffn_branch.lstm_ffn_dim\n",
    "        cnn_dim = cnn_branch.cnn_dim\n",
    "        self.fc = nn.Linear(lstm_ffn_dim + cnn_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, pitch_hnrs, pitchhnr_lengths, global_features, mfccs):\n",
    "      \n",
    "        lstm_ffn_out = self.lstm_ffn_branch(pitch_hnrs, pitchhnr_lengths, global_features)\n",
    "        \n",
    "        # Get the output from the second branch\n",
    "        cnn_out = self.cnn_branch(mfccs)\n",
    "        \n",
    "        # Concatenate all features\n",
    "        combined_features = torch.cat((lstm_ffn_out, cnn_out), dim=1)\n",
    "\n",
    "        # Apply dropout\n",
    "        combined_features = self.dropout(combined_features)\n",
    "        \n",
    "        # Final classification\n",
    "        output = self.fc(combined_features)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318ef8a6",
   "metadata": {},
   "source": [
    "#### Ensemble with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58ec8930",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpoofEnsemble_attention(nn.Module):\n",
    "    def __init__(self, lstm_ffn_branch, cnn_branch, output_dim, dropout):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm_ffn_branch = lstm_ffn_branch\n",
    "        self.cnn_branch = cnn_branch\n",
    "\n",
    "        lstm_ffn_dim = lstm_ffn_branch.lstm_ffn_dim\n",
    "        cnn_dim = cnn_branch.cnn_dim\n",
    "\n",
    "        # Instantiate the attention module\n",
    "        self.attention = BranchAttention(lstm_ffn_dim, cnn_dim)\n",
    "        \n",
    "        # This attribute will store the weights from the last forward pass\n",
    "        # for later analysis and interpretation.\n",
    "        self.attention_weights = None\n",
    "\n",
    "        self.fc = nn.Linear(lstm_ffn_dim + cnn_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, pitch_hnrs, pitchhnr_lengths, global_features, mfccs):\n",
    "      \n",
    "        lstm_ffn_out = self.lstm_ffn_branch(pitch_hnrs, pitchhnr_lengths, global_features)\n",
    "        \n",
    "        # Get the output from the second branch\n",
    "        cnn_out = self.cnn_branch(mfccs)\n",
    "        \n",
    "        # Pass the raw outputs through the attention mechanism\n",
    "        combined_features, self.attention_weights = self.attention(lstm_ffn_out, cnn_out)\n",
    "        \n",
    "        # Apply dropout\n",
    "        combined_features = self.dropout(combined_features)\n",
    "        \n",
    "        # Final classification\n",
    "        output = self.fc(combined_features)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0e0862",
   "metadata": {},
   "source": [
    "### Initiate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70b6bdf",
   "metadata": {},
   "source": [
    "#### find the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c5ff2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device: NVIDIA GeForce GTX TITAN X\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device_index = 1\n",
    "    torch.cuda.set_device(device_index)\n",
    "    DEVICE = torch.device('cuda')\n",
    "    print(f\"Using CUDA device: {torch.cuda.get_device_name(DEVICE)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "    DEVICE = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa91379",
   "metadata": {},
   "source": [
    "#### find the class weights for WCE & set the criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76bcf403",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = torch.tensor([weight_bonafide, weight_spoof], dtype=torch.float32).to(DEVICE)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean', weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121e595d",
   "metadata": {},
   "source": [
    "#### Initiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5e02bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_model():\n",
    "    lstm_ffn_out= LSTM_FFN_branch(lstm_input_dim=config.lstm_input_dim, lstm_hidden_dim=config.lstm_hidden_dim, lstm_n_layers=config.lstm_n_layers, bidirectional=config.bidirectional,\n",
    "                    ffn_dims=config.ffn_dims).to(DEVICE)\n",
    "    cnn_out = CNN_branch(cnn_channels=config.cnn_channels, conv_kernel=config.conv_kernel, pool_kernel=config.pool_kernel, cnn_padding=config.cnn_padding).to(DEVICE)\n",
    "\n",
    "    if config.model==\"SpoofEnsemble\":\n",
    "        model = SpoofEnsemble(lstm_ffn_branch=lstm_ffn_out, cnn_branch=cnn_out, output_dim=2, dropout=config.dropout_rate).to(DEVICE)\n",
    "    elif config.model==\"LSTM_FFN_classifier\":\n",
    "        model = LSTM_FFN_classifer(lstm_ffn_out=lstm_ffn_out, output_dim=2, dropout=config.dropout_rate).to(DEVICE)\n",
    "    elif config.model==\"CNN_classifier\":\n",
    "        model = CNN_classifer(cnn_out=cnn_out, output_dim=2, dropout=config.dropout_rate).to(DEVICE)\n",
    "    elif config.model==\"SpoofEnsemble_attention\":\n",
    "        model = SpoofEnsemble_attention(lstm_ffn_branch=lstm_ffn_out, cnn_branch=cnn_out, output_dim=2, dropout=config.dropout_rate).to(DEVICE)\n",
    "    else:\n",
    "        print(\"WARNING: invalid model name.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# print(f\"DEBUG: Initial FFN_Linear WEIGHTS:\\n{model.ffn_linear.weight.detach().cpu().numpy()}\")\n",
    "# print(f\"DEBUG: Initial FFN_Linear BIAS:\\n{model.ffn_linear.bias.detach().cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed0614e",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7882ce3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(data_loader, model, criterion):\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode (disables dropout, etc.)\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    scores_bonafide = []\n",
    "    scores_spoof = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculations during evaluation\n",
    "        for batch_labels, batch_lengths, batch_pitchhnr, batch_global, batch_mfcc in data_loader:\n",
    "            \n",
    "            batch_labels = batch_labels.to(DEVICE)\n",
    "            batch_pitchhnr = batch_pitchhnr.to(DEVICE)\n",
    "            batch_global = batch_global.to(DEVICE)\n",
    "            batch_mfcc = batch_mfcc.to(DEVICE)\n",
    "\n",
    "            # Forward pass: Get model outputs (logits)\n",
    "            logits = model(batch_pitchhnr, batch_lengths, batch_global, batch_mfcc)\n",
    "            \n",
    "            # Calculate loss for the current batch\n",
    "            loss = criterion(logits, batch_labels)\n",
    "            total_loss += loss.item() * batch_labels.size(0) # Accumulate loss, weighted by batch size\n",
    "\n",
    "            # for EER\n",
    "            probabilities = torch.softmax(logits, dim=1)\n",
    "            \n",
    "            for i in range(len(batch_labels)):\n",
    "                current_label = batch_labels[i]\n",
    "                current_score = probabilities[i]\n",
    "\n",
    "                if current_label == LABEL_BONAFIDE:\n",
    "                    scores_bonafide.append(current_score[LABEL_BONAFIDE].cpu())     # numpy is cpu only, need to move tensor from gpu\n",
    "                elif current_label == LABEL_SPOOF:\n",
    "                    scores_spoof.append(current_score[LABEL_BONAFIDE].cpu())\n",
    "            \n",
    "            total_samples += batch_labels.size(0) # Count number of samples in this batch\n",
    "\n",
    "    average_loss = total_loss / total_samples if total_samples > 0 else 0.0\n",
    "\n",
    "    scores_bonafide_np = np.array(scores_bonafide)    \n",
    "    scores_spoof_np = np.array(scores_spoof)\n",
    "    eer, threshold = em.compute_eer(scores_bonafide_np, scores_spoof_np)\n",
    "\n",
    "    all_scores = np.concatenate((scores_bonafide_np, scores_spoof_np))\n",
    "    labels_true = np.concatenate((np.ones_like(scores_bonafide_np), np.zeros_like(scores_spoof_np)))\n",
    "    labels_pred = (all_scores >= threshold).astype(int)\n",
    "    \n",
    "    return average_loss, eer, threshold, labels_true, labels_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05672159",
   "metadata": {},
   "source": [
    "### The training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dca569",
   "metadata": {},
   "source": [
    ">note: in wandb, scalers logs for every epoch, plots get overwritten (but still saved in artifacts?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c10873ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(criterion, train_dataloader, dev_dataloader, num_epochs,\n",
    "                min_eer, best_model_filename):\n",
    "\n",
    "    model = initiate_model()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=config.scheduler_factor, patience=config.scheduler_patience)\n",
    "    print(f\"Training started on device: {DEVICE}\")\n",
    "    model.to(DEVICE) \n",
    "\n",
    "    # Initial metric dictionary for the progress bar\n",
    "    metric_dict = {'train_loss': 'N/A', 'val_loss': 'N/A', 'val_eer': 'N/A', 'val_threshold': 'N/A'}\n",
    "\n",
    "    # Evaluate on validation set first to get a baseline\n",
    "    print(\"Evaluating on validation set before training...\")\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    val_loss_initial, val_eer_initial, threshold_initial, labels_true, labels_pred = evaluate_classifier(dev_dataloader, model, criterion)\n",
    "    metric_dict.update({'val_loss': f'{val_loss_initial:.3f}', 'val_eer': f'{val_eer_initial*100:.2f}%', 'val_threshold': f'{threshold_initial*100:.2f}%'})\n",
    "    print(f\"Initial Validation - Loss: {val_loss_initial:.4f}, EER: {val_eer_initial*100:.2f}%, Threshold: {threshold_initial*100:.2f}%\")\n",
    "\n",
    "    # Progress bar setup\n",
    "    total_steps = num_epochs * len(train_dataloader)\n",
    "    pbar = tqdm(total=total_steps, initial=0, postfix=metric_dict, unit=\"batch\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode (enables dropout, etc.)\n",
    "        pbar.set_description(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        \n",
    "        running_train_loss = 0.0\n",
    "        num_train_batches = 0\n",
    "\n",
    "        for batch_labels, batch_lengths, batch_pitchhnr, batch_global, batch_mfcc in train_dataloader:\n",
    "            # Move data to the specified device\n",
    "            # batch_lengths are used by pack_padded_sequence which expects them on CPU\n",
    "            batch_labels = batch_labels.to(DEVICE)\n",
    "            batch_pitchhnr = batch_pitchhnr.to(DEVICE)\n",
    "            batch_global = batch_global.to(DEVICE)\n",
    "            batch_mfcc = batch_mfcc.to(DEVICE)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass: Get model outputs (logits)\n",
    "            logits = model(batch_pitchhnr, batch_lengths, batch_global, batch_mfcc)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(logits, batch_labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            # --- FOR GRADIENT CLIPPING ---\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update statistics for progress bar and logging\n",
    "            running_train_loss += loss.item()\n",
    "            num_train_batches += 1\n",
    "            \n",
    "            pbar.update(1) # Increment progress bar by one batch\n",
    "            metric_dict.update({'train_loss': f'{loss.item():.3f}'}) # Current batch loss\n",
    "            pbar.set_postfix(metric_dict)\n",
    "        \n",
    "        # Calculate average training loss for the epoch\n",
    "        avg_epoch_train_loss = running_train_loss / num_train_batches if num_train_batches > 0 else 0.0\n",
    "        metric_dict.update({'train_loss': f'{avg_epoch_train_loss:.3f}'}) # Average epoch loss\n",
    "        \n",
    "        # Evaluate on validation set after each epoch\n",
    "        avg_val_loss, val_eer, val_threshold, labels_true, labels_pred = evaluate_classifier(dev_dataloader, model, criterion)\n",
    "        \n",
    "        # for reduce on plateau\n",
    "        if config.scheduler:\n",
    "            scheduler.step(val_eer)\n",
    "\n",
    "        # Update with latest validation metrics\n",
    "        metric_dict.update({'val_loss': f'{avg_val_loss:.3f}', 'val_eer': f'{val_eer*100:.2f}%', 'val_threshold': f'{val_threshold*100:.2f}%'})\n",
    "        pbar.set_postfix(metric_dict)\n",
    "        \n",
    "        # Optional: Print epoch summary\n",
    "        print(f\"\\nEpoch {epoch+1} Summary: Avg Train Loss: {avg_epoch_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, EER: {val_eer*100:.2f}%, Threshold: {val_threshold*100:.2f}%\")\n",
    "\n",
    "        # log the train\\dev loss and the eer & threshold\n",
    "        run.log({\"train_loss\": avg_epoch_train_loss, \"dev_loss\": avg_val_loss, \n",
    "                   \"dev_eer\": val_eer, \"dev_threshold\":val_threshold, \"epoch\": epoch + 1})\n",
    "        \n",
    "        # update min eer and optimal model\n",
    "        if val_eer < min_eer:\n",
    "            min_eer = val_eer\n",
    "            torch.save(model.state_dict(), best_model_filename)\n",
    "            print(f\"Epoch {epoch+1}: New best model saved to '{best_model_filename}' with EER: {min_eer:.4f}\")\n",
    "\n",
    "            run.summary['best_validation_eer'] = min_eer\n",
    "            run.summary['best_eer_epoch'] = epoch + 1\n",
    "            run.summary['validation_loss_at_best_eer'] = avg_val_loss\n",
    "\n",
    "            # log the report and confusion matrix\n",
    "            class_names = ['SPOOF', 'BONAFIDE']     #NOTE: the order matters, need to match labels\n",
    "            report_columns =  [\"Class\", \"Precision\", \"Recall\", \"F1-score\", \"Support\"]\n",
    "            class_report = classification_report(labels_true, labels_pred, labels=[0, 1],\n",
    "                                        target_names=class_names).splitlines()\n",
    "            report_table = []\n",
    "            for line in class_report[2:(len(class_names)+2)]:\n",
    "                report_table.append(line.split())\n",
    "            run.log({\"Confusion Matix\": wandb.plot.confusion_matrix(y_true=labels_true, preds=labels_pred, class_names=class_names),\n",
    "                    \"Classification Report\": wandb.Table(data=report_table, columns=report_columns)})\n",
    "\n",
    "    pbar.close()\n",
    "    print(\"Training finished.\")\n",
    "    return min_eer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7357368b",
   "metadata": {},
   "source": [
    "#### Start the training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e00816c",
   "metadata": {},
   "source": [
    ">note: only partially deterministic for adaptivemaxpooling does not support the feature yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2c89dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Trial with Seed: 0 ---\n",
      "Training started on device: cuda\n",
      "Evaluating on validation set before training...\n",
      "Initial Validation - Loss: 0.7381, EER: 59.81%, Threshold: 52.69%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8066154f6b84f7ea5c25dfd9d96df9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99750 [00:00<?, ?batch/s, train_loss=N/A, val_eer=59.81%, val_loss=0.738, val_threshold=52.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary: Avg Train Loss: 0.2975, Val Loss: 0.1130, EER: 5.65%, Threshold: 36.42%\n",
      "Epoch 1: New best model saved to 'best_model' with EER: 0.0565\n",
      "\n",
      "Epoch 2 Summary: Avg Train Loss: 0.1129, Val Loss: 0.0630, EER: 3.72%, Threshold: 21.34%\n",
      "Epoch 2: New best model saved to 'best_model' with EER: 0.0372\n",
      "\n",
      "Epoch 3 Summary: Avg Train Loss: 0.0655, Val Loss: 0.0476, EER: 2.43%, Threshold: 3.63%\n",
      "Epoch 3: New best model saved to 'best_model' with EER: 0.0243\n",
      "\n",
      "Epoch 4 Summary: Avg Train Loss: 0.0460, Val Loss: 0.0337, EER: 2.12%, Threshold: 20.08%\n",
      "Epoch 4: New best model saved to 'best_model' with EER: 0.0212\n",
      "\n",
      "Epoch 5 Summary: Avg Train Loss: 0.0291, Val Loss: 0.0552, EER: 1.92%, Threshold: 56.95%\n",
      "Epoch 5: New best model saved to 'best_model' with EER: 0.0192\n",
      "\n",
      "Epoch 6 Summary: Avg Train Loss: 0.0241, Val Loss: 0.4135, EER: 2.24%, Threshold: 99.11%\n",
      "\n",
      "Epoch 7 Summary: Avg Train Loss: 0.0195, Val Loss: 0.0271, EER: 1.45%, Threshold: 1.93%\n",
      "Epoch 7: New best model saved to 'best_model' with EER: 0.0145\n",
      "\n",
      "Epoch 8 Summary: Avg Train Loss: 0.0149, Val Loss: 0.0469, EER: 2.07%, Threshold: 0.88%\n",
      "\n",
      "Epoch 9 Summary: Avg Train Loss: 0.0146, Val Loss: 0.1399, EER: 1.38%, Threshold: 96.56%\n",
      "Epoch 9: New best model saved to 'best_model' with EER: 0.0138\n",
      "\n",
      "Epoch 10 Summary: Avg Train Loss: 0.0086, Val Loss: 0.0335, EER: 1.52%, Threshold: 0.76%\n",
      "\n",
      "Epoch 11 Summary: Avg Train Loss: 0.0091, Val Loss: 0.0218, EER: 1.15%, Threshold: 1.32%\n",
      "Epoch 11: New best model saved to 'best_model' with EER: 0.0115\n",
      "\n",
      "Epoch 12 Summary: Avg Train Loss: 0.0095, Val Loss: 0.0205, EER: 1.14%, Threshold: 2.21%\n",
      "Epoch 12: New best model saved to 'best_model' with EER: 0.0114\n",
      "\n",
      "Epoch 13 Summary: Avg Train Loss: 0.0093, Val Loss: 0.0548, EER: 1.22%, Threshold: 0.09%\n",
      "\n",
      "Epoch 14 Summary: Avg Train Loss: 0.0064, Val Loss: 0.0752, EER: 1.37%, Threshold: 0.04%\n",
      "\n",
      "Epoch 15 Summary: Avg Train Loss: 0.0053, Val Loss: 0.0563, EER: 1.02%, Threshold: 0.02%\n",
      "Epoch 15: New best model saved to 'best_model' with EER: 0.0102\n",
      "\n",
      "Epoch 16 Summary: Avg Train Loss: 0.0052, Val Loss: 0.0280, EER: 1.42%, Threshold: 26.35%\n",
      "\n",
      "Epoch 17 Summary: Avg Train Loss: 0.0036, Val Loss: 0.0308, EER: 1.22%, Threshold: 0.25%\n",
      "\n",
      "Epoch 18 Summary: Avg Train Loss: 0.0065, Val Loss: 0.0349, EER: 1.06%, Threshold: 0.12%\n",
      "\n",
      "Epoch 19 Summary: Avg Train Loss: 0.0034, Val Loss: 0.0231, EER: 1.26%, Threshold: 3.69%\n",
      "\n",
      "Epoch 20 Summary: Avg Train Loss: 0.0047, Val Loss: 0.0470, EER: 1.41%, Threshold: 0.05%\n",
      "\n",
      "Epoch 21 Summary: Avg Train Loss: 0.0024, Val Loss: 0.0275, EER: 1.38%, Threshold: 0.98%\n",
      "\n",
      "Epoch 22 Summary: Avg Train Loss: 0.0046, Val Loss: 0.0308, EER: 1.23%, Threshold: 0.23%\n",
      "\n",
      "Epoch 23 Summary: Avg Train Loss: 0.0028, Val Loss: 0.0222, EER: 0.86%, Threshold: 0.38%\n",
      "Epoch 23: New best model saved to 'best_model' with EER: 0.0086\n",
      "\n",
      "Epoch 24 Summary: Avg Train Loss: 0.0026, Val Loss: 0.1087, EER: 1.52%, Threshold: 0.00%\n",
      "\n",
      "Epoch 25 Summary: Avg Train Loss: 0.0031, Val Loss: 0.0247, EER: 1.37%, Threshold: 1.72%\n",
      "\n",
      "Epoch 26 Summary: Avg Train Loss: 0.0031, Val Loss: 0.0327, EER: 1.07%, Threshold: 0.10%\n",
      "\n",
      "Epoch 27 Summary: Avg Train Loss: 0.0028, Val Loss: 0.0409, EER: 0.99%, Threshold: 0.06%\n",
      "\n",
      "Epoch 28 Summary: Avg Train Loss: 0.0063, Val Loss: 0.0343, EER: 1.22%, Threshold: 0.12%\n",
      "\n",
      "Epoch 29 Summary: Avg Train Loss: 0.0012, Val Loss: 0.0383, EER: 1.15%, Threshold: 0.07%\n",
      "\n",
      "Epoch 30 Summary: Avg Train Loss: 0.0017, Val Loss: 0.0892, EER: 1.34%, Threshold: 0.00%\n",
      "\n",
      "Epoch 31 Summary: Avg Train Loss: 0.0029, Val Loss: 0.0194, EER: 0.75%, Threshold: 0.49%\n",
      "Epoch 31: New best model saved to 'best_model' with EER: 0.0075\n",
      "\n",
      "Epoch 32 Summary: Avg Train Loss: 0.0010, Val Loss: 0.0170, EER: 0.91%, Threshold: 2.60%\n",
      "\n",
      "Epoch 33 Summary: Avg Train Loss: 0.0026, Val Loss: 0.1367, EER: 1.34%, Threshold: 0.00%\n",
      "\n",
      "Epoch 34 Summary: Avg Train Loss: 0.0012, Val Loss: 0.0665, EER: 1.57%, Threshold: 0.00%\n",
      "\n",
      "Epoch 35 Summary: Avg Train Loss: 0.0011, Val Loss: 0.0212, EER: 0.98%, Threshold: 2.38%\n",
      "\n",
      "Epoch 36 Summary: Avg Train Loss: 0.0025, Val Loss: 0.0291, EER: 0.94%, Threshold: 0.04%\n",
      "\n",
      "Epoch 37 Summary: Avg Train Loss: 0.0011, Val Loss: 0.0297, EER: 0.74%, Threshold: 0.03%\n",
      "Epoch 37: New best model saved to 'best_model' with EER: 0.0074\n",
      "\n",
      "Epoch 38 Summary: Avg Train Loss: 0.0019, Val Loss: 0.0231, EER: 0.71%, Threshold: 0.07%\n",
      "Epoch 38: New best model saved to 'best_model' with EER: 0.0071\n",
      "\n",
      "Epoch 39 Summary: Avg Train Loss: 0.0002, Val Loss: 0.0465, EER: 0.86%, Threshold: 0.00%\n",
      "\n",
      "Epoch 40 Summary: Avg Train Loss: 0.0032, Val Loss: 0.0704, EER: 1.06%, Threshold: 0.00%\n",
      "\n",
      "Epoch 41 Summary: Avg Train Loss: 0.0004, Val Loss: 0.0235, EER: 0.79%, Threshold: 0.04%\n",
      "\n",
      "Epoch 42 Summary: Avg Train Loss: 0.0023, Val Loss: 0.0395, EER: 3.11%, Threshold: 4.74%\n",
      "\n",
      "Epoch 43 Summary: Avg Train Loss: 0.0017, Val Loss: 0.0216, EER: 0.86%, Threshold: 0.12%\n",
      "\n",
      "Epoch 44 Summary: Avg Train Loss: 0.0001, Val Loss: 0.0407, EER: 0.82%, Threshold: 0.00%\n",
      "\n",
      "Epoch 45 Summary: Avg Train Loss: 0.0030, Val Loss: 0.0387, EER: 1.33%, Threshold: 0.05%\n",
      "\n",
      "Epoch 46 Summary: Avg Train Loss: 0.0002, Val Loss: 0.0313, EER: 0.86%, Threshold: 0.02%\n",
      "\n",
      "Epoch 47 Summary: Avg Train Loss: 0.0027, Val Loss: 0.0292, EER: 1.25%, Threshold: 0.09%\n",
      "\n",
      "Epoch 48 Summary: Avg Train Loss: 0.0009, Val Loss: 0.0383, EER: 1.22%, Threshold: 0.03%\n",
      "\n",
      "Epoch 49 Summary: Avg Train Loss: 0.0013, Val Loss: 0.0248, EER: 1.01%, Threshold: 0.20%\n",
      "\n",
      "Epoch 50 Summary: Avg Train Loss: 0.0001, Val Loss: 0.0488, EER: 0.98%, Threshold: 0.00%\n",
      "\n",
      "Epoch 51 Summary: Avg Train Loss: 0.0015, Val Loss: 0.0468, EER: 1.15%, Threshold: 0.01%\n",
      "\n",
      "Epoch 52 Summary: Avg Train Loss: 0.0023, Val Loss: 0.0240, EER: 0.82%, Threshold: 0.07%\n",
      "\n",
      "Epoch 53 Summary: Avg Train Loss: 0.0002, Val Loss: 0.0616, EER: 0.82%, Threshold: 0.00%\n",
      "\n",
      "Epoch 54 Summary: Avg Train Loss: 0.0017, Val Loss: 0.0619, EER: 0.75%, Threshold: 0.00%\n",
      "\n",
      "Epoch 55 Summary: Avg Train Loss: 0.0001, Val Loss: 0.0400, EER: 0.74%, Threshold: 0.00%\n",
      "\n",
      "Epoch 56 Summary: Avg Train Loss: 0.0016, Val Loss: 0.0689, EER: 0.86%, Threshold: 0.00%\n",
      "\n",
      "Epoch 57 Summary: Avg Train Loss: 0.0015, Val Loss: 0.0328, EER: 0.86%, Threshold: 0.03%\n",
      "\n",
      "Epoch 58 Summary: Avg Train Loss: 0.0012, Val Loss: 0.1085, EER: 1.06%, Threshold: 0.00%\n",
      "\n",
      "Epoch 59 Summary: Avg Train Loss: 0.0017, Val Loss: 0.0423, EER: 1.18%, Threshold: 0.04%\n",
      "\n",
      "Epoch 60 Summary: Avg Train Loss: 0.0016, Val Loss: 0.0586, EER: 1.02%, Threshold: 0.00%\n",
      "\n",
      "Epoch 61 Summary: Avg Train Loss: 0.0017, Val Loss: 0.0252, EER: 1.22%, Threshold: 0.37%\n",
      "\n",
      "Epoch 62 Summary: Avg Train Loss: 0.0001, Val Loss: 0.0539, EER: 1.10%, Threshold: 0.00%\n",
      "\n",
      "Epoch 63 Summary: Avg Train Loss: 0.0000, Val Loss: 0.0566, EER: 1.06%, Threshold: 0.00%\n",
      "\n",
      "Epoch 64 Summary: Avg Train Loss: 0.0022, Val Loss: 0.0287, EER: 0.90%, Threshold: 0.04%\n",
      "\n",
      "Epoch 65 Summary: Avg Train Loss: 0.0004, Val Loss: 0.0538, EER: 0.94%, Threshold: 0.00%\n",
      "\n",
      "Epoch 66 Summary: Avg Train Loss: 0.0018, Val Loss: 0.0185, EER: 1.09%, Threshold: 1.95%\n",
      "\n",
      "Epoch 67 Summary: Avg Train Loss: 0.0011, Val Loss: 0.0213, EER: 0.71%, Threshold: 0.06%\n",
      "Epoch 67: New best model saved to 'best_model' with EER: 0.0071\n",
      "\n",
      "Epoch 68 Summary: Avg Train Loss: 0.0005, Val Loss: 0.0349, EER: 0.78%, Threshold: 0.02%\n",
      "\n",
      "Epoch 69 Summary: Avg Train Loss: 0.0003, Val Loss: 0.0210, EER: 0.82%, Threshold: 0.05%\n",
      "\n",
      "Epoch 70 Summary: Avg Train Loss: 0.0000, Val Loss: 0.0939, EER: 0.71%, Threshold: 0.00%\n",
      "Training finished.\n",
      "\n",
      "--- Starting Trial with Seed: 7 ---\n",
      "Training started on device: cuda\n",
      "Evaluating on validation set before training...\n",
      "Initial Validation - Loss: 0.7751, EER: 31.01%, Threshold: 54.91%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f9b67797da54a0cb288243f79324197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99750 [00:00<?, ?batch/s, train_loss=N/A, val_eer=31.01%, val_loss=0.775, val_threshold=54.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary: Avg Train Loss: 0.3307, Val Loss: 0.0888, EER: 5.45%, Threshold: 13.42%\n",
      "\n",
      "Epoch 2 Summary: Avg Train Loss: 0.1260, Val Loss: 0.0483, EER: 2.94%, Threshold: 12.49%\n",
      "\n",
      "Epoch 3 Summary: Avg Train Loss: 0.0642, Val Loss: 0.0352, EER: 2.36%, Threshold: 15.35%\n",
      "\n",
      "Epoch 4 Summary: Avg Train Loss: 0.0452, Val Loss: 0.0283, EER: 1.88%, Threshold: 6.64%\n",
      "\n",
      "Epoch 5 Summary: Avg Train Loss: 0.0287, Val Loss: 0.0294, EER: 1.53%, Threshold: 32.84%\n",
      "\n",
      "Epoch 6 Summary: Avg Train Loss: 0.0241, Val Loss: 0.0304, EER: 1.37%, Threshold: 1.27%\n",
      "\n",
      "Epoch 7 Summary: Avg Train Loss: 0.0183, Val Loss: 0.0730, EER: 1.57%, Threshold: 82.23%\n",
      "\n",
      "Epoch 8 Summary: Avg Train Loss: 0.0144, Val Loss: 0.0432, EER: 1.34%, Threshold: 0.14%\n",
      "\n",
      "Epoch 9 Summary: Avg Train Loss: 0.0119, Val Loss: 0.0307, EER: 1.22%, Threshold: 0.39%\n",
      "\n",
      "Epoch 10 Summary: Avg Train Loss: 0.0114, Val Loss: 0.0339, EER: 1.53%, Threshold: 35.75%\n",
      "\n",
      "Epoch 11 Summary: Avg Train Loss: 0.0095, Val Loss: 0.0279, EER: 1.42%, Threshold: 0.61%\n",
      "\n",
      "Epoch 12 Summary: Avg Train Loss: 0.0094, Val Loss: 0.0185, EER: 1.22%, Threshold: 1.85%\n",
      "\n",
      "Epoch 13 Summary: Avg Train Loss: 0.0053, Val Loss: 0.0449, EER: 1.37%, Threshold: 0.20%\n",
      "\n",
      "Epoch 14 Summary: Avg Train Loss: 0.0080, Val Loss: 0.0387, EER: 1.40%, Threshold: 0.22%\n",
      "\n",
      "Epoch 15 Summary: Avg Train Loss: 0.0052, Val Loss: 0.0271, EER: 1.18%, Threshold: 0.29%\n",
      "\n",
      "Epoch 16 Summary: Avg Train Loss: 0.0047, Val Loss: 0.0330, EER: 1.13%, Threshold: 0.08%\n",
      "\n",
      "Epoch 17 Summary: Avg Train Loss: 0.0058, Val Loss: 0.1535, EER: 0.90%, Threshold: 0.00%\n",
      "\n",
      "Epoch 18 Summary: Avg Train Loss: 0.0038, Val Loss: 0.0339, EER: 0.94%, Threshold: 0.11%\n",
      "\n",
      "Epoch 19 Summary: Avg Train Loss: 0.0034, Val Loss: 0.0342, EER: 1.37%, Threshold: 0.24%\n",
      "\n",
      "Epoch 20 Summary: Avg Train Loss: 0.0033, Val Loss: 0.0347, EER: 1.34%, Threshold: 0.07%\n",
      "\n",
      "Epoch 21 Summary: Avg Train Loss: 0.0046, Val Loss: 0.0303, EER: 1.29%, Threshold: 0.19%\n",
      "\n",
      "Epoch 22 Summary: Avg Train Loss: 0.0033, Val Loss: 0.0289, EER: 1.50%, Threshold: 0.36%\n",
      "\n",
      "Epoch 23 Summary: Avg Train Loss: 0.0022, Val Loss: 0.0246, EER: 1.68%, Threshold: 1.06%\n",
      "\n",
      "Epoch 24 Summary: Avg Train Loss: 0.0022, Val Loss: 0.0169, EER: 1.13%, Threshold: 6.47%\n",
      "\n",
      "Epoch 25 Summary: Avg Train Loss: 0.0044, Val Loss: 0.0249, EER: 1.10%, Threshold: 0.16%\n",
      "\n",
      "Epoch 26 Summary: Avg Train Loss: 0.0016, Val Loss: 0.0191, EER: 0.98%, Threshold: 0.36%\n",
      "\n",
      "Epoch 27 Summary: Avg Train Loss: 0.0017, Val Loss: 0.0255, EER: 1.18%, Threshold: 0.08%\n",
      "\n",
      "Epoch 28 Summary: Avg Train Loss: 0.0023, Val Loss: 0.0474, EER: 1.06%, Threshold: 0.01%\n",
      "\n",
      "Epoch 29 Summary: Avg Train Loss: 0.0029, Val Loss: 0.0179, EER: 1.18%, Threshold: 4.37%\n",
      "\n",
      "Epoch 30 Summary: Avg Train Loss: 0.0028, Val Loss: 0.0322, EER: 1.17%, Threshold: 0.15%\n",
      "\n",
      "Epoch 31 Summary: Avg Train Loss: 0.0006, Val Loss: 0.0193, EER: 0.98%, Threshold: 0.41%\n",
      "\n",
      "Epoch 32 Summary: Avg Train Loss: 0.0036, Val Loss: 0.0244, EER: 1.14%, Threshold: 0.32%\n",
      "\n",
      "Epoch 33 Summary: Avg Train Loss: 0.0009, Val Loss: 0.0259, EER: 0.86%, Threshold: 0.11%\n",
      "\n",
      "Epoch 34 Summary: Avg Train Loss: 0.0015, Val Loss: 0.0380, EER: 0.98%, Threshold: 0.01%\n",
      "\n",
      "Epoch 35 Summary: Avg Train Loss: 0.0017, Val Loss: 0.0327, EER: 1.26%, Threshold: 28.46%\n",
      "\n",
      "Epoch 36 Summary: Avg Train Loss: 0.0016, Val Loss: 0.0253, EER: 1.07%, Threshold: 0.39%\n",
      "\n",
      "Epoch 37 Summary: Avg Train Loss: 0.0035, Val Loss: 0.0438, EER: 1.23%, Threshold: 0.04%\n",
      "\n",
      "Epoch 38 Summary: Avg Train Loss: 0.0006, Val Loss: 0.0308, EER: 1.01%, Threshold: 0.03%\n",
      "\n",
      "Epoch 39 Summary: Avg Train Loss: 0.0019, Val Loss: 0.0183, EER: 0.94%, Threshold: 0.82%\n",
      "\n",
      "Epoch 40 Summary: Avg Train Loss: 0.0008, Val Loss: 0.1187, EER: 1.18%, Threshold: 0.00%\n",
      "\n",
      "Epoch 41 Summary: Avg Train Loss: 0.0015, Val Loss: 0.0310, EER: 1.02%, Threshold: 0.02%\n",
      "\n",
      "Epoch 42 Summary: Avg Train Loss: 0.0008, Val Loss: 0.0376, EER: 1.22%, Threshold: 0.01%\n",
      "\n",
      "Epoch 43 Summary: Avg Train Loss: 0.0020, Val Loss: 0.0698, EER: 1.22%, Threshold: 0.00%\n",
      "\n",
      "Epoch 44 Summary: Avg Train Loss: 0.0004, Val Loss: 0.0171, EER: 1.06%, Threshold: 1.77%\n",
      "\n",
      "Epoch 45 Summary: Avg Train Loss: 0.0017, Val Loss: 0.0286, EER: 1.26%, Threshold: 0.17%\n",
      "\n",
      "Epoch 46 Summary: Avg Train Loss: 0.0008, Val Loss: 0.0333, EER: 0.98%, Threshold: 0.01%\n",
      "\n",
      "Epoch 47 Summary: Avg Train Loss: 0.0023, Val Loss: 0.0623, EER: 1.36%, Threshold: 0.00%\n",
      "\n",
      "Epoch 48 Summary: Avg Train Loss: 0.0008, Val Loss: 0.0537, EER: 0.98%, Threshold: 0.00%\n",
      "\n",
      "Epoch 49 Summary: Avg Train Loss: 0.0011, Val Loss: 0.0248, EER: 1.49%, Threshold: 1.17%\n",
      "\n",
      "Epoch 50 Summary: Avg Train Loss: 0.0011, Val Loss: 0.0335, EER: 0.86%, Threshold: 0.02%\n",
      "\n",
      "Epoch 51 Summary: Avg Train Loss: 0.0001, Val Loss: 0.0297, EER: 0.75%, Threshold: 0.01%\n",
      "\n",
      "Epoch 52 Summary: Avg Train Loss: 0.0028, Val Loss: 0.0388, EER: 1.37%, Threshold: 0.02%\n",
      "\n",
      "Epoch 53 Summary: Avg Train Loss: 0.0008, Val Loss: 0.0206, EER: 1.14%, Threshold: 0.41%\n",
      "\n",
      "Epoch 54 Summary: Avg Train Loss: 0.0010, Val Loss: 0.0181, EER: 1.02%, Threshold: 0.76%\n",
      "\n",
      "Epoch 55 Summary: Avg Train Loss: 0.0012, Val Loss: 0.0358, EER: 0.86%, Threshold: 0.01%\n",
      "\n",
      "Epoch 56 Summary: Avg Train Loss: 0.0008, Val Loss: 0.1138, EER: 0.90%, Threshold: 0.00%\n",
      "\n",
      "Epoch 57 Summary: Avg Train Loss: 0.0018, Val Loss: 0.0269, EER: 0.71%, Threshold: 0.01%\n",
      "\n",
      "Epoch 58 Summary: Avg Train Loss: 0.0006, Val Loss: 0.0222, EER: 0.67%, Threshold: 0.03%\n",
      "Epoch 58: New best model saved to 'best_model' with EER: 0.0067\n",
      "\n",
      "Epoch 59 Summary: Avg Train Loss: 0.0016, Val Loss: 0.0373, EER: 1.15%, Threshold: 0.01%\n",
      "\n",
      "Epoch 60 Summary: Avg Train Loss: 0.0006, Val Loss: 0.0411, EER: 1.26%, Threshold: 0.00%\n",
      "\n",
      "Epoch 61 Summary: Avg Train Loss: 0.0007, Val Loss: 0.0361, EER: 1.14%, Threshold: 0.01%\n",
      "\n",
      "Epoch 62 Summary: Avg Train Loss: 0.0012, Val Loss: 0.0491, EER: 1.26%, Threshold: 0.00%\n",
      "\n",
      "Epoch 63 Summary: Avg Train Loss: 0.0002, Val Loss: 0.0443, EER: 1.14%, Threshold: 0.00%\n",
      "\n",
      "Epoch 64 Summary: Avg Train Loss: 0.0009, Val Loss: 0.0535, EER: 1.22%, Threshold: 0.00%\n",
      "\n",
      "Epoch 65 Summary: Avg Train Loss: 0.0017, Val Loss: 0.0488, EER: 1.11%, Threshold: 0.00%\n",
      "\n",
      "Epoch 66 Summary: Avg Train Loss: 0.0006, Val Loss: 0.0199, EER: 1.06%, Threshold: 0.27%\n",
      "\n",
      "Epoch 67 Summary: Avg Train Loss: 0.0015, Val Loss: 0.0278, EER: 0.97%, Threshold: 0.03%\n",
      "\n",
      "Epoch 68 Summary: Avg Train Loss: 0.0004, Val Loss: 0.0350, EER: 0.98%, Threshold: 0.01%\n",
      "\n",
      "Epoch 69 Summary: Avg Train Loss: 0.0006, Val Loss: 0.0355, EER: 0.83%, Threshold: 0.00%\n",
      "\n",
      "Epoch 70 Summary: Avg Train Loss: 0.0021, Val Loss: 0.0764, EER: 0.94%, Threshold: 0.00%\n",
      "Training finished.\n",
      "\n",
      "--- Starting Trial with Seed: 42 ---\n",
      "Training started on device: cuda\n",
      "Evaluating on validation set before training...\n",
      "Initial Validation - Loss: 0.6917, EER: 38.74%, Threshold: 49.94%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d47e0417599744b9a7cd122a52c0fc44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99750 [00:00<?, ?batch/s, train_loss=N/A, val_eer=38.74%, val_loss=0.692, val_threshold=49.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary: Avg Train Loss: 0.2619, Val Loss: 0.0776, EER: 5.14%, Threshold: 14.00%\n",
      "\n",
      "Epoch 2 Summary: Avg Train Loss: 0.0826, Val Loss: 0.0435, EER: 2.68%, Threshold: 21.80%\n",
      "\n",
      "Epoch 3 Summary: Avg Train Loss: 0.0407, Val Loss: 0.0339, EER: 1.96%, Threshold: 22.09%\n",
      "\n",
      "Epoch 4 Summary: Avg Train Loss: 0.0326, Val Loss: 0.1064, EER: 1.57%, Threshold: 87.66%\n",
      "\n",
      "Epoch 5 Summary: Avg Train Loss: 0.0241, Val Loss: 0.0312, EER: 1.73%, Threshold: 1.62%\n",
      "\n",
      "Epoch 6 Summary: Avg Train Loss: 0.0216, Val Loss: 0.0247, EER: 1.38%, Threshold: 1.69%\n",
      "\n",
      "Epoch 7 Summary: Avg Train Loss: 0.0152, Val Loss: 0.0245, EER: 1.57%, Threshold: 3.50%\n",
      "\n",
      "Epoch 8 Summary: Avg Train Loss: 0.0116, Val Loss: 0.0267, EER: 1.02%, Threshold: 0.52%\n",
      "\n",
      "Epoch 9 Summary: Avg Train Loss: 0.0089, Val Loss: 0.0227, EER: 1.14%, Threshold: 22.60%\n",
      "\n",
      "Epoch 10 Summary: Avg Train Loss: 0.0103, Val Loss: 0.0291, EER: 1.45%, Threshold: 1.44%\n",
      "\n",
      "Epoch 11 Summary: Avg Train Loss: 0.0058, Val Loss: 0.0288, EER: 1.06%, Threshold: 0.33%\n",
      "\n",
      "Epoch 12 Summary: Avg Train Loss: 0.0068, Val Loss: 0.1210, EER: 1.18%, Threshold: 0.00%\n",
      "\n",
      "Epoch 13 Summary: Avg Train Loss: 0.0079, Val Loss: 0.0205, EER: 1.01%, Threshold: 1.64%\n",
      "\n",
      "Epoch 14 Summary: Avg Train Loss: 0.0049, Val Loss: 0.0607, EER: 1.22%, Threshold: 82.96%\n",
      "\n",
      "Epoch 15 Summary: Avg Train Loss: 0.0049, Val Loss: 0.0276, EER: 1.22%, Threshold: 0.36%\n",
      "\n",
      "Epoch 16 Summary: Avg Train Loss: 0.0047, Val Loss: 0.0160, EER: 0.94%, Threshold: 6.43%\n",
      "\n",
      "Epoch 17 Summary: Avg Train Loss: 0.0035, Val Loss: 0.0493, EER: 0.98%, Threshold: 0.02%\n",
      "\n",
      "Epoch 18 Summary: Avg Train Loss: 0.0044, Val Loss: 0.0166, EER: 1.06%, Threshold: 3.47%\n",
      "\n",
      "Epoch 19 Summary: Avg Train Loss: 0.0033, Val Loss: 0.0381, EER: 1.10%, Threshold: 0.06%\n",
      "\n",
      "Epoch 20 Summary: Avg Train Loss: 0.0029, Val Loss: 0.0297, EER: 1.26%, Threshold: 0.10%\n",
      "\n",
      "Epoch 21 Summary: Avg Train Loss: 0.0036, Val Loss: 0.0390, EER: 1.34%, Threshold: 0.06%\n",
      "\n",
      "Epoch 22 Summary: Avg Train Loss: 0.0019, Val Loss: 0.0538, EER: 1.06%, Threshold: 0.01%\n",
      "\n",
      "Epoch 23 Summary: Avg Train Loss: 0.0034, Val Loss: 0.0408, EER: 2.00%, Threshold: 0.34%\n",
      "\n",
      "Epoch 24 Summary: Avg Train Loss: 0.0022, Val Loss: 0.0176, EER: 1.06%, Threshold: 1.93%\n",
      "\n",
      "Epoch 25 Summary: Avg Train Loss: 0.0027, Val Loss: 0.0345, EER: 0.90%, Threshold: 0.04%\n",
      "\n",
      "Epoch 26 Summary: Avg Train Loss: 0.0020, Val Loss: 0.0653, EER: 0.99%, Threshold: 0.00%\n",
      "\n",
      "Epoch 27 Summary: Avg Train Loss: 0.0019, Val Loss: 0.0228, EER: 0.71%, Threshold: 0.16%\n",
      "\n",
      "Epoch 28 Summary: Avg Train Loss: 0.0018, Val Loss: 0.0313, EER: 1.02%, Threshold: 0.07%\n",
      "\n",
      "Epoch 29 Summary: Avg Train Loss: 0.0024, Val Loss: 0.0336, EER: 0.86%, Threshold: 0.02%\n",
      "\n",
      "Epoch 30 Summary: Avg Train Loss: 0.0016, Val Loss: 0.0440, EER: 0.90%, Threshold: 0.01%\n",
      "\n",
      "Epoch 31 Summary: Avg Train Loss: 0.0015, Val Loss: 0.0253, EER: 0.75%, Threshold: 0.09%\n",
      "\n",
      "Epoch 32 Summary: Avg Train Loss: 0.0018, Val Loss: 0.0223, EER: 0.78%, Threshold: 0.29%\n",
      "\n",
      "Epoch 33 Summary: Avg Train Loss: 0.0006, Val Loss: 0.0209, EER: 1.06%, Threshold: 0.82%\n",
      "\n",
      "Epoch 34 Summary: Avg Train Loss: 0.0023, Val Loss: 0.0158, EER: 1.02%, Threshold: 4.91%\n",
      "\n",
      "Epoch 35 Summary: Avg Train Loss: 0.0014, Val Loss: 0.0283, EER: 0.90%, Threshold: 0.05%\n",
      "\n",
      "Epoch 36 Summary: Avg Train Loss: 0.0016, Val Loss: 0.0345, EER: 1.22%, Threshold: 0.02%\n",
      "\n",
      "Epoch 37 Summary: Avg Train Loss: 0.0018, Val Loss: 0.0540, EER: 1.07%, Threshold: 0.00%\n",
      "\n",
      "Epoch 38 Summary: Avg Train Loss: 0.0013, Val Loss: 0.0493, EER: 1.18%, Threshold: 0.00%\n",
      "\n",
      "Epoch 39 Summary: Avg Train Loss: 0.0008, Val Loss: 0.0339, EER: 0.97%, Threshold: 0.01%\n",
      "\n",
      "Epoch 40 Summary: Avg Train Loss: 0.0016, Val Loss: 0.0346, EER: 1.45%, Threshold: 0.29%\n",
      "\n",
      "Epoch 41 Summary: Avg Train Loss: 0.0009, Val Loss: 0.0247, EER: 0.86%, Threshold: 0.05%\n",
      "\n",
      "Epoch 42 Summary: Avg Train Loss: 0.0014, Val Loss: 0.0397, EER: 2.16%, Threshold: 0.29%\n",
      "\n",
      "Epoch 43 Summary: Avg Train Loss: 0.0006, Val Loss: 0.0519, EER: 1.18%, Threshold: 0.00%\n",
      "\n",
      "Epoch 44 Summary: Avg Train Loss: 0.0019, Val Loss: 0.0224, EER: 0.75%, Threshold: 0.08%\n",
      "\n",
      "Epoch 45 Summary: Avg Train Loss: 0.0006, Val Loss: 0.0288, EER: 0.94%, Threshold: 0.05%\n",
      "\n",
      "Epoch 46 Summary: Avg Train Loss: 0.0016, Val Loss: 0.0239, EER: 0.86%, Threshold: 0.10%\n",
      "\n",
      "Epoch 47 Summary: Avg Train Loss: 0.0004, Val Loss: 0.0415, EER: 0.91%, Threshold: 0.01%\n",
      "\n",
      "Epoch 48 Summary: Avg Train Loss: 0.0021, Val Loss: 0.0257, EER: 0.74%, Threshold: 0.03%\n",
      "\n",
      "Epoch 49 Summary: Avg Train Loss: 0.0006, Val Loss: 0.0411, EER: 1.18%, Threshold: 0.02%\n",
      "\n",
      "Epoch 50 Summary: Avg Train Loss: 0.0006, Val Loss: 0.0458, EER: 0.90%, Threshold: 0.02%\n",
      "\n",
      "Epoch 51 Summary: Avg Train Loss: 0.0015, Val Loss: 0.0832, EER: 0.90%, Threshold: 0.00%\n",
      "\n",
      "Epoch 52 Summary: Avg Train Loss: 0.0011, Val Loss: 0.0291, EER: 0.90%, Threshold: 0.04%\n",
      "\n",
      "Epoch 53 Summary: Avg Train Loss: 0.0006, Val Loss: 0.0453, EER: 1.41%, Threshold: 0.01%\n",
      "\n",
      "Epoch 54 Summary: Avg Train Loss: 0.0010, Val Loss: 0.0291, EER: 0.86%, Threshold: 0.03%\n",
      "\n",
      "Epoch 55 Summary: Avg Train Loss: 0.0008, Val Loss: 0.0304, EER: 0.82%, Threshold: 0.01%\n",
      "\n",
      "Epoch 56 Summary: Avg Train Loss: 0.0007, Val Loss: 0.0390, EER: 0.94%, Threshold: 0.00%\n",
      "\n",
      "Epoch 57 Summary: Avg Train Loss: 0.0014, Val Loss: 0.0396, EER: 0.78%, Threshold: 0.00%\n",
      "\n",
      "Epoch 58 Summary: Avg Train Loss: 0.0006, Val Loss: 0.0340, EER: 0.90%, Threshold: 0.01%\n",
      "\n",
      "Epoch 59 Summary: Avg Train Loss: 0.0012, Val Loss: 0.0328, EER: 0.86%, Threshold: 0.02%\n",
      "\n",
      "Epoch 60 Summary: Avg Train Loss: 0.0004, Val Loss: 0.0299, EER: 0.90%, Threshold: 0.03%\n",
      "\n",
      "Epoch 61 Summary: Avg Train Loss: 0.0020, Val Loss: 0.0324, EER: 0.78%, Threshold: 0.01%\n",
      "\n",
      "Epoch 62 Summary: Avg Train Loss: 0.0003, Val Loss: 0.0448, EER: 0.90%, Threshold: 0.00%\n",
      "\n",
      "Epoch 63 Summary: Avg Train Loss: 0.0006, Val Loss: 0.0300, EER: 0.75%, Threshold: 0.02%\n",
      "\n",
      "Epoch 64 Summary: Avg Train Loss: 0.0003, Val Loss: 0.0891, EER: 1.69%, Threshold: 82.22%\n",
      "\n",
      "Epoch 65 Summary: Avg Train Loss: 0.0022, Val Loss: 0.0383, EER: 0.83%, Threshold: 0.00%\n",
      "\n",
      "Epoch 66 Summary: Avg Train Loss: 0.0004, Val Loss: 0.0482, EER: 0.78%, Threshold: 0.00%\n",
      "\n",
      "Epoch 67 Summary: Avg Train Loss: 0.0002, Val Loss: 0.0493, EER: 1.02%, Threshold: 0.00%\n",
      "\n",
      "Epoch 68 Summary: Avg Train Loss: 0.0017, Val Loss: 0.0357, EER: 1.41%, Threshold: 0.02%\n",
      "\n",
      "Epoch 69 Summary: Avg Train Loss: 0.0010, Val Loss: 0.0565, EER: 1.03%, Threshold: 0.00%\n",
      "\n",
      "Epoch 70 Summary: Avg Train Loss: 0.0010, Val Loss: 0.0648, EER: 1.37%, Threshold: 0.00%\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = config.epochs\n",
    "min_eer = float('inf')\n",
    "best_model_filename = 'best_model'\n",
    "\n",
    "for seed in config.seeds:\n",
    "    print(f\"\\n--- Starting Trial with Seed: {seed} ---\")\n",
    "    # set_seed(seed)\n",
    "    # torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    min_eer = train_model(criterion, train_dataloader, dev_dataloader, NUM_EPOCHS, min_eer, best_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd7ece4",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2840319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging the best model (best_model) to W&B Artifacts...\n",
      "Best model logged as W&B Artifact.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>dev_eer</td><td>▃▂▂▂▂▁▂▁▁▂▁█▄▃▂▂▂▂▂▂▂▁▁▁▂▁▄▃▂▂▂▃▂▁▂▁▁▁▁▂</td></tr><tr><td>dev_loss</td><td>▂▁▂█▁▁▂▁▂▁▃▁▁▁▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂▁▂▂▁▁▂▁▂</td></tr><tr><td>dev_threshold</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇▁▁▁▁▁▁▁▁▁▁▁▁▇▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▃▄▄▅▅▆▆▇▇▇██▁▂▂▃▄▄▄▄▆▇▇██▁▂▂▃▃▄▄▅▅▅▆▆▆▇█</td></tr><tr><td>train_loss</td><td>▃▁▂▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_eer_epoch</td><td>58</td></tr><tr><td>best_validation_eer</td><td>0.00668</td></tr><tr><td>dev_eer</td><td>0.01373</td></tr><tr><td>dev_loss</td><td>0.06477</td></tr><tr><td>dev_threshold</td><td>0.0</td></tr><tr><td>epoch</td><td>70</td></tr><tr><td>train_loss</td><td>0.00101</td></tr><tr><td>validation_loss_at_best_eer</td><td>0.02216</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Training_08_CNN</strong> at: <a href='https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake/runs/2ts3za1k' target=\"_blank\">https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake/runs/2ts3za1k</a><br> View project at: <a href='https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake' target=\"_blank\">https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake</a><br>Synced 5 W&B file(s), 32 media file(s), 58 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250706_100937-2ts3za1k/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B run finished.\n"
     ]
    }
   ],
   "source": [
    "if min_eer != float('inf'):\n",
    "    print(f\"Logging the best model ({best_model_filename}) to W&B Artifacts...\")\n",
    "    best_model_artifact = wandb.Artifact(\n",
    "        name=f\"{run.id}-best-model\", # Using run ID for uniqueness\n",
    "        type=\"model\",\n",
    "        description=f\"Best model according to EER ({min_eer:.4f}) achieved at epoch {run.summary.get('best_eer_epoch', 'N/A')}.\",\n",
    "        metadata={\"best_eer\": min_eer, \"epoch_of_best_eer\": run.summary.get('best_eer_epoch', 'N/A')}\n",
    "    )\n",
    "    best_model_artifact.add_file(best_model_filename) # Add the saved file\n",
    "    wandb.run.log_artifact(best_model_artifact, aliases=[\"best_eer_model\"]) # Add an alias\n",
    "    print(\"Best model logged as W&B Artifact.\")\n",
    "else:\n",
    "    print(\"No model was saved as best_eer did not improve from its initial value.\")\n",
    "\n",
    "run.finish()\n",
    "\n",
    "print(\"W&B run finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc0200f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
