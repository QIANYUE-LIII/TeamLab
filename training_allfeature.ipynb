{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "222cac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import eval_metrics as em\n",
    "import wandb\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89eddae",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006b2c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_attack_type</strong> at: <a href='https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake/runs/owkke7mw' target=\"_blank\">https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake/runs/owkke7mw</a><br> View project at: <a href='https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake' target=\"_blank\">https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250630_194456-owkke7mw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/users1/liqe/TeamLab_phonetics/TeamLab/wandb/run-20250630_194825-f7u7bc82</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake/runs/f7u7bc82' target=\"_blank\">test_attack_type</a></strong> to <a href='https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake' target=\"_blank\">https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake/runs/f7u7bc82' target=\"_blank\">https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake/runs/f7u7bc82</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project = \"teamlab_deepfake\",\n",
    "    name = \"test_attack_type\",     #NOTE: set manually\n",
    "    notes = None,\n",
    "    tags = [\"ALL_FEATURE\", \"COMBINED_MODEL\", \"HNR\", \"PITCH\", \"JITTER&SHIMMER\", \"MFCC\"],\n",
    "    config={\n",
    "        #NOTE: set manually\n",
    "        \"model\": \"SpoofEnsemble\",   #   SpoofEnsemble/LSTM_FFN_classifier/CNN_classifier\n",
    "        \"dataset\": \"ASVSpoof19_LA_original\",    \n",
    "        \"feature\": \"MFCC&Prosody\",\n",
    "        \"attack_type\": \"all\",   # all/A01/A02/A03/A04/A05/A06\n",
    "        \"loss_function\": \"weighted_CE\",\n",
    "        #\n",
    "        \"epochs\": 1,\n",
    "        \"batch_size\": 32,\n",
    "        \"oversampling\": True,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"dropout_rate\": 0.1,\n",
    "        # lstm layer\n",
    "        \"lstm_input_dim\": 2,\n",
    "        \"lstm_hidden_dim\": 64,\n",
    "        \"bidirectional\": False,\n",
    "        \"lstm_n_layers\":1,\n",
    "        # fnn layer\n",
    "        \"ffn_input_dim\": 11,\n",
    "        \"ffn_hidden_dim\": 64,\n",
    "        # cnn layer\n",
    "        \"cnn_channels\": [1, 16, 32, 64],   #in, out, out, out\n",
    "        \"conv_kernel\": (3,3),\n",
    "        \"pool_kernel\": (2,2),\n",
    "        \"cnn_padding\": 1,\n",
    "        # random seeds\n",
    "        \"seeds\": [42]\n",
    "    },\n",
    ")\n",
    "\n",
    "config = run.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970b71e6",
   "metadata": {},
   "source": [
    ">NOTE: attack types are evenly distributed in training and dev dataset, and each has higher number than genuine voices, so no further balancing is needed>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621b14ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        AUDIO_ID  LABEL ATTACK_TYPE  \\\n",
      "2   LA_T_1000648      0         A01   \n",
      "6   LA_T_1001169      0         A01   \n",
      "7   LA_T_1001718      0         A01   \n",
      "9   LA_T_1002656      0         A01   \n",
      "11  LA_T_1004407      0         A01   \n",
      "\n",
      "                                                PITCH  \\\n",
      "2   [nan, nan, nan, nan, nan, 0.35835335, 0.350411...   \n",
      "6   [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "7   [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "9   [nan, nan, nan, nan, nan, 0.3446952, 0.3116533...   \n",
      "11  [nan, nan, nan, nan, nan, nan, nan, nan, 0.292...   \n",
      "\n",
      "                                                  HNR  \\\n",
      "2   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7987432, 0.79...   \n",
      "6   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "7   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "9   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7937161, 0.81...   \n",
      "11  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.786...   \n",
      "\n",
      "                                               JITTER  \\\n",
      "2   [0.13972145, 0.05122372, 0.123514704, 0.096691...   \n",
      "6   [0.11878602, 0.047274902, 0.10791813, 0.076425...   \n",
      "7   [0.13606435, 0.049893107, 0.13377233, 0.102136...   \n",
      "9   [0.120143734, 0.054571845, 0.10887862, 0.09621...   \n",
      "11  [0.16879041, 0.0786074, 0.14137433, 0.11028980...   \n",
      "\n",
      "                                              SHIMMER  \\\n",
      "2   [0.28141773, 0.3631905, 0.12680757, 0.17373542...   \n",
      "6   [0.2015925, 0.28466532, 0.07975244, 0.10248045...   \n",
      "7   [0.2511304, 0.27908075, 0.10077607, 0.14693537...   \n",
      "9   [0.2601562, 0.33056188, 0.12206272, 0.16642591...   \n",
      "11  [0.2647236, 0.31022537, 0.119792365, 0.1580609...   \n",
      "\n",
      "                                                 MFCC  \n",
      "2   [[0.22124906, 0.34767622, 0.5816188, 0.6628496...  \n",
      "6   [[0.2689295, 0.26384014, 0.2554089, 0.2559836,...  \n",
      "7   [[0.2070771, 0.20564632, 0.20497796, 0.2006736...  \n",
      "9   [[0.2583435, 0.29383314, 0.45285627, 0.5424404...  \n",
      "11  [[0.26524612, 0.26490054, 0.4673639, 0.6282413...  \n",
      "             AUDIO_ID  LABEL  PITCH   HNR  JITTER  SHIMMER  MFCC\n",
      "ATTACK_TYPE                                                     \n",
      "A01              3800   3800   3800  3800    3800     3800  3800\n",
      "\n",
      "\n",
      "        AUDIO_ID  LABEL ATTACK_TYPE  \\\n",
      "5   LA_D_4673595      0         A01   \n",
      "7   LA_D_7594804      0         A01   \n",
      "21  LA_D_2453330      0         A01   \n",
      "28  LA_D_4461485      0         A01   \n",
      "40  LA_D_1039057      0         A01   \n",
      "\n",
      "                                                PITCH  \\\n",
      "5   [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "7   [nan, nan, nan, nan, nan, nan, nan, 0.31156045...   \n",
      "21  [nan, nan, nan, nan, nan, nan, nan, 0.33043152...   \n",
      "28  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "40  [nan, nan, nan, nan, nan, 0.33690137, 0.334806...   \n",
      "\n",
      "                                                  HNR  \\\n",
      "5   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "7   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.78905135...   \n",
      "21  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.762...   \n",
      "28  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "40  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.78083813, 0.8...   \n",
      "\n",
      "                                               JITTER  \\\n",
      "5   [0.14577158, 0.065038234, 0.12784907, 0.103999...   \n",
      "7   [0.117040604, 0.0451014, 0.11148719, 0.0901735...   \n",
      "21  [0.1393618, 0.057644658, 0.10973163, 0.0955715...   \n",
      "28  [0.33953696, 0.21352124, 0.35141045, 0.2444977...   \n",
      "40  [0.13265179, 0.05147722, 0.12130696, 0.0939342...   \n",
      "\n",
      "                                              SHIMMER  \\\n",
      "5   [0.2596176, 0.29413572, 0.11952713, 0.16643901...   \n",
      "7   [0.22324513, 0.28933012, 0.09750613, 0.1201312...   \n",
      "21  [0.2983977, 0.36616358, 0.1278408, 0.1677203, ...   \n",
      "28  [0.2964161, 0.36230132, 0.1734279, 0.22491238,...   \n",
      "40  [0.31692308, 0.37621522, 0.11318527, 0.1732816...   \n",
      "\n",
      "                                                 MFCC  \n",
      "5   [[0.21984264, 0.4053417, 0.51509863, 0.5124076...  \n",
      "7   [[0.25146994, 0.40092656, 0.6281243, 0.7156452...  \n",
      "21  [[0.2747786, 0.30176488, 0.6020938, 0.763443, ...  \n",
      "28  [[0.23481716, 0.24130385, 0.48688486, 0.714682...  \n",
      "40  [[0.22919701, 0.36687177, 0.557574, 0.6225878,...  \n",
      "             AUDIO_ID  LABEL  PITCH   HNR  JITTER  SHIMMER  MFCC\n",
      "ATTACK_TYPE                                                     \n",
      "A01              3716   3716   3716  3716    3716     3716  3716\n"
     ]
    }
   ],
   "source": [
    "PITCH_COLUMN = 'PITCH'\n",
    "HNR_COLUMN = 'HNR'\n",
    "JITTER_COLUMN = 'JITTER'\n",
    "SHIMMER_COLUMN = 'SHIMMER'\n",
    "MFCC_COLUMN = 'MFCC'\n",
    "LABEL_COLUMN = 'LABEL'      \n",
    "                           \n",
    "NAN_REPLACEMENT_VALUE = 0.0  \n",
    "PADDING_VALUE = 0.0         \n",
    "LABEL_BONAFIDE = 1\n",
    "LABEL_SPOOF = 0\n",
    "\n",
    "train_features_path = '/home/users1/liqe/TeamLab_phonetics/merged_train_com.pkl'\n",
    "dev_features_path = '/home/users1/liqe/TeamLab_phonetics/merged_dev_com.pkl'\n",
    "\n",
    "df_train = pd.read_pickle(train_features_path)\n",
    "df_dev = pd.read_pickle(dev_features_path)\n",
    "\n",
    "# NOTE: if training on a specific attack type\n",
    "if config.attack_type != \"all\":\n",
    "    df_train = df_train[df_train['ATTACK_TYPE']==(config.attack_type)]\n",
    "    df_dev = df_dev[df_dev['ATTACK_TYPE']==(config.attack_type)]\n",
    "elif config.attack_type == \"all\":\n",
    "    pass\n",
    "elif config.attack_type != (\"A01\" or \"A02\" or \"A03\" or \"A04\" or \"A05\" or \"A06\"):\n",
    "    print(\"WARNING: invalid attack type.\")\n",
    "\n",
    "# inspect\n",
    "# print(df_train.head())\n",
    "# print(df_train.groupby('ATTACK_TYPE').count())\n",
    "\n",
    "# print(\"\\n\")\n",
    "# print(df_dev.head())\n",
    "# print(df_dev.groupby('ATTACK_TYPE').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82fb4c6",
   "metadata": {},
   "source": [
    "#### Set the random seeds for replicability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c5173b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c9ec96",
   "metadata": {},
   "source": [
    "### Training Data Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f385ade",
   "metadata": {},
   "source": [
    ">NOTE: training audio & labels are matched, dev are not (Solved: excessive rows are deleted beforehand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d12dd51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resampled X (DataFrame) head:\n",
      "       AUDIO_ID ATTACK_TYPE  \\\n",
      "0  LA_T_1000137         A04   \n",
      "1  LA_T_1000406           -   \n",
      "2  LA_T_1000648         A01   \n",
      "3  LA_T_1000824         A04   \n",
      "4  LA_T_1001074         A03   \n",
      "\n",
      "                                               PITCH  \\\n",
      "0  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "1  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "2  [nan, nan, nan, nan, nan, 0.35835335, 0.350411...   \n",
      "3  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "4  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "\n",
      "                                                 HNR  \\\n",
      "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7987432, 0.79...   \n",
      "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "\n",
      "                                              JITTER  \\\n",
      "0  [0.30073947, 0.23022015, 0.26707897, 0.2538646...   \n",
      "1  [0.1494679, 0.09972349, 0.086890295, 0.0791289...   \n",
      "2  [0.13972145, 0.05122372, 0.123514704, 0.096691...   \n",
      "3  [0.28089172, 0.2684494, 0.23122567, 0.24996407...   \n",
      "4  [0.14094485, 0.068655394, 0.07130514, 0.062871...   \n",
      "\n",
      "                                             SHIMMER  \\\n",
      "0  [0.27033818, 0.33233136, 0.11353744, 0.1519924...   \n",
      "1  [0.13012205, 0.15872449, 0.059600886, 0.081321...   \n",
      "2  [0.28141773, 0.3631905, 0.12680757, 0.17373542...   \n",
      "3  [0.2846507, 0.35503966, 0.13182917, 0.15244015...   \n",
      "4  [0.16095506, 0.21579938, 0.039854582, 0.081848...   \n",
      "\n",
      "                                                MFCC  \n",
      "0  [[0.14843875, 0.16200094, 0.18541528, 0.279557...  \n",
      "1  [[0.30532825, 0.3215196, 0.31294125, 0.3068853...  \n",
      "2  [[0.22124906, 0.34767622, 0.5816188, 0.6628496...  \n",
      "3  [[0.11835651, 0.12180926, 0.11639575, 0.117103...  \n",
      "4  [[0.19357309, 0.19469197, 0.19534206, 0.195999...  \n",
      "\n",
      "Resampled y (Series) head:\n",
      "0    0\n",
      "1    1\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: LABEL, dtype: int8\n",
      "\n",
      "Resampled class distribution (from y_resampled_series):\n",
      "Counter({0: 22799, 1: 22799})\n",
      "\n",
      "Combined Resampled DataFrame head:\n",
      "       AUDIO_ID ATTACK_TYPE  \\\n",
      "0  LA_T_1000137         A04   \n",
      "1  LA_T_1000406           -   \n",
      "2  LA_T_1000648         A01   \n",
      "3  LA_T_1000824         A04   \n",
      "4  LA_T_1001074         A03   \n",
      "\n",
      "                                               PITCH  \\\n",
      "0  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "1  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "2  [nan, nan, nan, nan, nan, 0.35835335, 0.350411...   \n",
      "3  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "4  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
      "\n",
      "                                                 HNR  \\\n",
      "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7987432, 0.79...   \n",
      "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "\n",
      "                                              JITTER  \\\n",
      "0  [0.30073947, 0.23022015, 0.26707897, 0.2538646...   \n",
      "1  [0.1494679, 0.09972349, 0.086890295, 0.0791289...   \n",
      "2  [0.13972145, 0.05122372, 0.123514704, 0.096691...   \n",
      "3  [0.28089172, 0.2684494, 0.23122567, 0.24996407...   \n",
      "4  [0.14094485, 0.068655394, 0.07130514, 0.062871...   \n",
      "\n",
      "                                             SHIMMER  \\\n",
      "0  [0.27033818, 0.33233136, 0.11353744, 0.1519924...   \n",
      "1  [0.13012205, 0.15872449, 0.059600886, 0.081321...   \n",
      "2  [0.28141773, 0.3631905, 0.12680757, 0.17373542...   \n",
      "3  [0.2846507, 0.35503966, 0.13182917, 0.15244015...   \n",
      "4  [0.16095506, 0.21579938, 0.039854582, 0.081848...   \n",
      "\n",
      "                                                MFCC  LABEL  \n",
      "0  [[0.14843875, 0.16200094, 0.18541528, 0.279557...      0  \n",
      "1  [[0.30532825, 0.3215196, 0.31294125, 0.3068853...      1  \n",
      "2  [[0.22124906, 0.34767622, 0.5816188, 0.6628496...      0  \n",
      "3  [[0.11835651, 0.12180926, 0.11639575, 0.117103...      0  \n",
      "4  [[0.19357309, 0.19469197, 0.19534206, 0.195999...      0  \n",
      "\n",
      "Combined Resampled DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45598 entries, 0 to 45597\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   AUDIO_ID     45598 non-null  object\n",
      " 1   ATTACK_TYPE  45598 non-null  object\n",
      " 2   PITCH        45598 non-null  object\n",
      " 3   HNR          45598 non-null  object\n",
      " 4   JITTER       45598 non-null  object\n",
      " 5   SHIMMER      45598 non-null  object\n",
      " 6   MFCC         45598 non-null  object\n",
      " 7   LABEL        45598 non-null  int8  \n",
      "dtypes: int8(1), object(7)\n",
      "memory usage: 2.5+ MB\n",
      "\n",
      "Combined Resampled DataFrame class distribution:\n",
      "Counter({0: 22799, 1: 22799})\n"
     ]
    }
   ],
   "source": [
    "if config.oversampling:\n",
    "    X = df_train.drop('LABEL', axis=1)\n",
    "    y = df_train['LABEL']\n",
    "\n",
    "    over = RandomOverSampler(random_state=config.seeds[0])\n",
    "    X_resampled_np, y_resampled_np = over.fit_resample(X, y) \n",
    "\n",
    "    X_resampled_df = pd.DataFrame(X_resampled_np, columns=X.columns)\n",
    "    y_resampled_series = pd.Series(y_resampled_np, name=y.name)\n",
    "\n",
    "    print(\"\\nResampled X (DataFrame) head:\")\n",
    "    print(X_resampled_df.head())\n",
    "    print(\"\\nResampled y (Series) head:\")\n",
    "    print(y_resampled_series.head())\n",
    "    print(\"\\nResampled class distribution (from y_resampled_series):\")\n",
    "    print(Counter(y_resampled_series))\n",
    "\n",
    "    df_train = pd.concat([X_resampled_df, y_resampled_series], axis=1)\n",
    "\n",
    "    print(\"\\nCombined Resampled DataFrame head:\")\n",
    "    print(df_train.head())\n",
    "    print(\"\\nCombined Resampled DataFrame info:\")\n",
    "    df_train.info()\n",
    "    print(\"\\nCombined Resampled DataFrame class distribution:\")\n",
    "    print(Counter(df_train['LABEL'])) # Verify target column in the new DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057ec143",
   "metadata": {},
   "source": [
    "### Padding and Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbc2366",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASVDataset(Dataset):\n",
    "    def __init__(self, dataframe, pitch_col, hnr_col, jitter_col, shimmer_col, mfcc_col, label_col, nan_replacement=NAN_REPLACEMENT_VALUE):\n",
    "        \n",
    "        self.labels = []\n",
    "        self.processed_pitchhnr = []\n",
    "        self.global_features = []\n",
    "        self.processed_mfcc = []\n",
    "        \n",
    "        print(f\"Attempting to process {len(dataframe)} entries from DataFrame\")\n",
    "        found_count = 0\n",
    "        # Iterate through the DataFrame, process and pad the features\n",
    "        for index, row in dataframe.iterrows():  \n",
    "            if not np.isnan(row[label_col]):\n",
    "                self.labels.append(row[label_col])\n",
    "\n",
    "                pitch_sequence_raw = row[pitch_col]\n",
    "                processed_pitch = np.nan_to_num(pitch_sequence_raw, nan=nan_replacement)\n",
    "                \n",
    "                hnr_sequence_raw = row[hnr_col]\n",
    "                processed_hnr = np.nan_to_num(hnr_sequence_raw, nan=nan_replacement)\n",
    "\n",
    "                ### NOTE:need to pad the two sequences to the same length\n",
    "                max_length = max(len(processed_pitch), len(processed_hnr))\n",
    "                if len(processed_pitch) > len(processed_hnr):\n",
    "                    padding = np.zeros(max_length - len(processed_hnr), dtype=processed_hnr.dtype)\n",
    "                    processed_hnr = np.concatenate((processed_hnr, padding))\n",
    "                else:\n",
    "                    padding = np.zeros(max_length - len(processed_pitch), dtype=processed_pitch.dtype)\n",
    "                    processed_pitch = np.concatenate((processed_pitch, padding))\n",
    "\n",
    "                combined_features = np.stack((processed_pitch, processed_hnr), axis=-1) \n",
    "                self.processed_pitchhnr.append(torch.tensor(combined_features, dtype=torch.float32))\n",
    "\n",
    "                # process and combine jitter and shimmer to one sequence\n",
    "                processed_jitter = np.nan_to_num(row[jitter_col], nan=nan_replacement)\n",
    "                processed_shimmer = np.nan_to_num(row[shimmer_col], nan=nan_replacement)\n",
    "                jitter_shimmer = np.concatenate((processed_jitter, processed_shimmer))\n",
    "                self.global_features.append(torch.tensor(jitter_shimmer, dtype=torch.float32))\n",
    "                \n",
    "                # process mfcc\n",
    "                mfcc = row[mfcc_col]\n",
    "                # NOTE: need transpose for padding (time, feature_dim)\n",
    "                self.processed_mfcc.append(torch.tensor(mfcc, dtype=torch.float32).T)\n",
    "\n",
    "                found_count += 1\n",
    "        \n",
    "        self.labels = torch.tensor(self.labels, dtype=torch.long) # Assuming labels are integers for classification\n",
    "        print(f\"Successfully processed {found_count} samples out of {len(dataframe)} DataFrame entries.\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of matched samples in the dataset.\"\"\"\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns one sample from the dataset: a preprocessed pitch sequence and its label.\n",
    "        \"\"\"\n",
    "        label = self.labels[idx]\n",
    "        pitch_hnr = self.processed_pitchhnr[idx]\n",
    "        global_feature = self.global_features[idx]\n",
    "        mfcc = self.processed_mfcc[idx]\n",
    "        return label, pitch_hnr, global_feature, mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e0f4a79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Custom Collate Function for Dynamic Padding  ---\n",
    "def collate_fn(batch, padding_value=PADDING_VALUE):\n",
    "    \"\"\"\n",
    "    Pads sequences within a batch to the same length.\n",
    "    \"\"\"\n",
    "    labels = [item[0] for item in batch]\n",
    "    pitch_hnrs = [item[1] for item in batch]\n",
    "    global_features = [item[2] for item in batch]\n",
    "    mfccs = [item[3] for item in batch]\n",
    "\n",
    "    labels = torch.stack(labels)\n",
    "\n",
    "    pitchhnr_lengths = torch.tensor([len(seq) for seq in pitch_hnrs], dtype=torch.long)\n",
    "    padded_pitchhnrs = pad_sequence(pitch_hnrs, batch_first=True, padding_value=padding_value)\n",
    "    if padded_pitchhnrs.ndim == 2:     # lstm expects: [batch_size, sequence_length, feature_size]\n",
    "        padded_pitchhnrs = padded_pitchhnrs.unsqueeze(2)\n",
    "\n",
    "    global_features = torch.stack(global_features)\n",
    "\n",
    "    padded_mfccs = pad_sequence(mfccs, batch_first=True, padding_value=padding_value)\n",
    "\n",
    "    return labels, pitchhnr_lengths, padded_pitchhnrs, global_features, padded_mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "67d92225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to process 45598 entries from DataFrame\n",
      "Successfully processed 45598 samples out of 45598 DataFrame entries.\n",
      "Attempting to process 24844 entries from DataFrame\n",
      "Successfully processed 24844 samples out of 24844 DataFrame entries.\n",
      "\n",
      "--- Batch 1 ---\n",
      "  Labels (first 5): tensor([0, 0, 0, 0, 0])\n",
      "  Padded Sequences Shape: torch.Size([32, 667, 2])\n",
      "  Original Lengths (first 5): tensor([329, 624, 377, 510, 117])\n",
      "  Global Shape: torch.Size([32, 11])\n",
      "  MFCC Shape: torch.Size([32, 210, 60])\n"
     ]
    }
   ],
   "source": [
    "pitch_dataset_train = ASVDataset(dataframe=df_train,   # NOTE: oversampled (change upon need)\n",
    "                                    pitch_col=PITCH_COLUMN,\n",
    "                                    hnr_col=HNR_COLUMN,\n",
    "                                    jitter_col=JITTER_COLUMN,\n",
    "                                    shimmer_col=SHIMMER_COLUMN,\n",
    "                                    mfcc_col=MFCC_COLUMN,\n",
    "                                    label_col=LABEL_COLUMN,\n",
    "                                    nan_replacement=NAN_REPLACEMENT_VALUE)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    pitch_dataset_train, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "pitch_dataset_dev = ASVDataset(dataframe=df_dev,   # NOTE: oversampled (change upon need)\n",
    "                                    pitch_col=PITCH_COLUMN,\n",
    "                                    hnr_col=HNR_COLUMN,\n",
    "                                    jitter_col=JITTER_COLUMN,\n",
    "                                    shimmer_col=SHIMMER_COLUMN,\n",
    "                                    mfcc_col=MFCC_COLUMN,\n",
    "                                    label_col=LABEL_COLUMN,\n",
    "                                    nan_replacement=NAN_REPLACEMENT_VALUE)\n",
    "\n",
    "dev_dataloader = DataLoader(\n",
    "    pitch_dataset_dev, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "## For inspection\n",
    "for i, batch_data in enumerate(train_dataloader):\n",
    "    # batch_data is a tuple\n",
    "    batch_labels, batch_lengths, batch_pitchhnr, batch_global, batch_mfcc = batch_data\n",
    "    print(f\"\\n--- Batch {i+1} ---\")\n",
    "    print(f\"  Labels (first 5): {batch_labels[:5]}\")\n",
    "    print(f\"  Padded Sequences Shape: {batch_pitchhnr.shape}\")\n",
    "    print(f\"  Original Lengths (first 5): {batch_lengths[:5]}\")\n",
    "    print(f\"  Global Shape: {batch_global.shape}\")\n",
    "    print(f\"  MFCC Shape: {batch_mfcc.shape}\")\n",
    "    \n",
    "\n",
    "    if i == 0: # Break after the first batch for inspection\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdbd83f",
   "metadata": {},
   "source": [
    "### Finding the weight (for weighted cross entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ac071d",
   "metadata": {},
   "source": [
    "is there different ways calculating weitghs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8f67837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = df_train['LABEL']   # NOTE:w/ oversampling\n",
    "#labels = df_train['LABEL']   # NOTE:w/o oversampling\n",
    "total = len(labels)\n",
    "count_bonafide = labels.value_counts().get(LABEL_BONAFIDE, 0)\n",
    "count_spoof =  total - count_bonafide\n",
    "weight_bonafide = total / (count_bonafide * 2)\n",
    "weight_spoof = total / (count_spoof * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3d85b7",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8a24da",
   "metadata": {},
   "source": [
    "#### LSTM&FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "41eb3605",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_FFN_branch(nn.Module):\n",
    "    def __init__(self, lstm_input_dim, lstm_hidden_dim, lstm_n_layers, bidirectional, \n",
    "                 ffn_input_dim, ffn_hidden_dim, dropout):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm_ffn_dim = (lstm_hidden_dim * 2 if bidirectional else lstm_hidden_dim) + ffn_hidden_dim\n",
    "\n",
    "        # 1. lstm layer\n",
    "        self.lstm = nn.LSTM(lstm_input_dim, \n",
    "                            lstm_hidden_dim, \n",
    "                            num_layers=lstm_n_layers, \n",
    "                            bidirectional=bidirectional, \n",
    "                            dropout=dropout if lstm_n_layers > 1 else 0,\n",
    "                            batch_first=True) # Input/output tensors are (batch, seq, feature)\n",
    "        # BN layer for stabalization\n",
    "        self.bn_lstm = nn.BatchNorm1d(lstm_hidden_dim)\n",
    "        \n",
    "        # 2. ffn layer\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(ffn_input_dim, ffn_hidden_dim),\n",
    "            nn.BatchNorm1d(ffn_hidden_dim),    # BN layer for stabalization\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout))\n",
    "        \n",
    "         # 3. Dropout Layer (for further regularization on the output of LSTM&FFN)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, pitch_hnrs, pitchhnr_lengths, global_features):\n",
    "      \n",
    "        # 1. Pack sequence\n",
    "        ### Compute actual data and ignore the padded values\n",
    "        packed_input = rnn_utils.pack_padded_sequence(pitch_hnrs, pitchhnr_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # 2. Pass packed sequence through LSTM\n",
    "        ### packed_output: Hidden states for every time step.\n",
    "        ### hidden: The final hidden state (summary) of the entire sequence.\n",
    "        ### cell: The final cell state (long-term memory) of the entire sequence.\n",
    "        packed_output, (lstm_hidden, cell) = self.lstm(packed_input)\n",
    "        \n",
    "        # 3. Concatenate the final forward and backward hidden states (if bidirectional)\n",
    "        if self.lstm.bidirectional:\n",
    "            lstm_hidden = self.dropout(torch.cat((lstm_hidden[-2,:,:], lstm_hidden[-1,:,:]), dim=1))\n",
    "        else:\n",
    "            lstm_hidden = self.dropout(lstm_hidden[-1,:,:])\n",
    "        lstm_hidden = self.bn_lstm(lstm_hidden)\n",
    "\n",
    "        # 4. Pass global features (jitter and shimmer) through the FFN\n",
    "        ffn_output = self.ffn(global_features)\n",
    "\n",
    "        # 5. Concatenate the outputs from lstm and fnn\n",
    "        combined_output = torch.cat((lstm_hidden,ffn_output), dim=1)\n",
    "\n",
    "        return combined_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "00fda60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for LSTM_FFN training alone \n",
    "class LSTM_FFN_classifer(nn.Module):\n",
    "    def __init__(self, lstm_ffn_out, out_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm_ffn_layer = lstm_ffn_out\n",
    "        self.fc = nn.Linear(self.lstm_ffn_out.lstm_ffn_dim, out_dim)\n",
    "\n",
    "    def forward(self, pitch_hnrs, pitchhnr_lengths, global_features):\n",
    "\n",
    "        lstm_ffn_out = self.lstm_ffn_layer(pitch_hnrs, pitchhnr_lengths, global_features)\n",
    "        output = self.fc(lstm_ffn_out)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba600f70",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "290b852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_branch(nn.Module):\n",
    "    def __init__(self, cnn_channels, conv_kernel, pool_kernel, cnn_padding):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn_dim = cnn_channels[-1]\n",
    "\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "\n",
    "        for i in range(len(cnn_channels)-2):\n",
    "            cnn_in = cnn_channels[i]\n",
    "            cnn_out = cnn_channels[i+1]\n",
    "            conv_block = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=cnn_in, out_channels=cnn_out, kernel_size=conv_kernel, padding=cnn_padding),\n",
    "                nn.BatchNorm2d(cnn_out),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=pool_kernel))\n",
    "            self.conv_layers.append(conv_block)\n",
    "\n",
    "        # final layer of CNN\n",
    "        final_in = cnn_channels[-2]\n",
    "        final_out = cnn_channels[-1]\n",
    "\n",
    "        conv_final = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=final_in, out_channels=final_out, kernel_size=conv_kernel, padding=cnn_padding),\n",
    "            nn.BatchNorm2d(final_out),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool2d((1, 1))  # Output size: [batch, 64, 1, 1]\n",
    "        )\n",
    "        self.conv_layers.append(conv_final)\n",
    "        \n",
    "    def forward(self, mfccs):\n",
    "\n",
    "        # expected shape (batch_size, in_channel, height, width) -> unsqeeze\n",
    "        mfccs = mfccs.unsqueeze(1)\n",
    "\n",
    "        for layer in self.conv_layers:\n",
    "            mfccs = layer(mfccs)\n",
    "        cnn_out = mfccs.view(mfccs.size(0), -1)\n",
    "        \n",
    "        return cnn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "49d26e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for CNN training alone\n",
    "class CNN_classifer(nn.Module):\n",
    "    def __init__(self, cnn_out, out_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn_layer = cnn_out\n",
    "        self.fc = nn.Linear(self.cnn_out.cnn_dim, out_dim)\n",
    "\n",
    "    def forward(self, mfccs):\n",
    "\n",
    "        cnn_out = self.cnn_layer(mfccs)\n",
    "        output = self.fc(cnn_out)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f942586f",
   "metadata": {},
   "source": [
    "#### Emsemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbba4a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpoofEnsemble(nn.Module):\n",
    "    def __init__(self, lstm_ffn_branch, cnn_branch, output_dim):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm_ffn_branch = lstm_ffn_branch\n",
    "        self.cnn_branch = cnn_branch\n",
    "\n",
    "        lstm_ffn_dim = lstm_ffn_branch.lstm_ffn_dim\n",
    "        cnn_dim = cnn_branch.cnn_dim\n",
    "        self.fc = nn.Linear(lstm_ffn_dim + cnn_dim, output_dim)\n",
    "        \n",
    "    def forward(self, pitch_hnrs, pitchhnr_lengths, global_features, mfccs):\n",
    "      \n",
    "        lstm_ffn_out = self.lstm_ffn_branch(pitch_hnrs, pitchhnr_lengths, global_features)\n",
    "        \n",
    "        # Get the output from the second branch\n",
    "        cnn_out = self.cnn_branch(mfccs)\n",
    "        \n",
    "        # Concatenate all features\n",
    "        combined_features = torch.cat((lstm_ffn_out, cnn_out), dim=1)\n",
    "        \n",
    "        # Final classification\n",
    "        output = self.fc(combined_features)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0e0862",
   "metadata": {},
   "source": [
    "### Initiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3c5ff2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device count: 4\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "76bcf403",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = torch.tensor([weight_bonafide, weight_spoof], dtype=torch.float32).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e02bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_ffn_out= LSTM_FFN_branch(lstm_input_dim=config.lstm_input_dim, lstm_hidden_dim=config.lstm_hidden_dim, lstm_n_layers=config.lstm_n_layers, bidirectional=config.bidirectional,\n",
    "                 ffn_input_dim=config.ffn_input_dim, ffn_hidden_dim=config.ffn_hidden_dim, dropout=config.dropout_rate).to(DEVICE)\n",
    "cnn_out = CNN_branch(cnn_channels=config.cnn_channels, conv_kernel=config.conv_kernel, pool_kernel=config.pool_kernel, cnn_padding=config.cnn_padding).to(DEVICE)\n",
    "\n",
    "if config.model==\"SpoofEnsmble\":\n",
    "    model = SpoofEnsemble(lstm_ffn_branch=lstm_ffn_out, cnn_branch=cnn_out, output_dim=2).to(DEVICE)\n",
    "elif config.model==\"LSTM_FFN_classifier\":\n",
    "    model = LSTM_FFN_classifer(lstm_ffn_out=lstm_ffn_out, output_dim=2).to(DEVICE)\n",
    "elif config.model==\"CNN_classifier\":\n",
    "    model = CNN_classifer(cnn_out=cnn_out, out_dim=2).to(DEVICE)\n",
    "else:\n",
    "    print(\"WARNING: invalid model name.\")\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean', weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "# print(f\"DEBUG: Initial FFN_Linear WEIGHTS:\\n{model.ffn_linear.weight.detach().cpu().numpy()}\")\n",
    "# print(f\"DEBUG: Initial FFN_Linear BIAS:\\n{model.ffn_linear.bias.detach().cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed0614e",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7882ce3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(data_loader, model, criterion):\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode (disables dropout, etc.)\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    scores_bonafide = []\n",
    "    scores_spoof = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculations during evaluation\n",
    "        for batch_labels, batch_lengths, batch_pitchhnr, batch_global, batch_mfcc in data_loader:\n",
    "            \n",
    "            batch_labels = batch_labels.to(DEVICE)\n",
    "            batch_pitchhnr = batch_pitchhnr.to(DEVICE)\n",
    "            batch_global = batch_global.to(DEVICE)\n",
    "            batch_mfcc = batch_mfcc.to(DEVICE)\n",
    "\n",
    "            # Forward pass: Get model outputs (logits)\n",
    "            logits = model(batch_pitchhnr, batch_lengths, batch_global, batch_mfcc)\n",
    "            \n",
    "            # Calculate loss for the current batch\n",
    "            loss = criterion(logits, batch_labels)\n",
    "            total_loss += loss.item() * batch_labels.size(0) # Accumulate loss, weighted by batch size\n",
    "\n",
    "            # for EER\n",
    "            probabilities = torch.softmax(logits, dim=1)\n",
    "            \n",
    "            for i in range(len(batch_labels)):\n",
    "                current_label = batch_labels[i]\n",
    "                current_score = probabilities[i]\n",
    "\n",
    "                if current_label == LABEL_BONAFIDE:\n",
    "                    scores_bonafide.append(current_score[LABEL_BONAFIDE].cpu())     # numpy is cpu only, need to move tensor from gpu\n",
    "                elif current_label == LABEL_SPOOF:\n",
    "                    scores_spoof.append(current_score[LABEL_BONAFIDE].cpu())\n",
    "            \n",
    "            total_samples += batch_labels.size(0) # Count number of samples in this batch\n",
    "\n",
    "    average_loss = total_loss / total_samples if total_samples > 0 else 0.0\n",
    "\n",
    "    scores_bonafide_np = np.array(scores_bonafide)    \n",
    "    scores_spoof_np = np.array(scores_spoof)\n",
    "    eer, threshold = em.compute_eer(scores_bonafide_np, scores_spoof_np)\n",
    "\n",
    "    all_scores = np.concatenate((scores_bonafide_np, scores_spoof_np))\n",
    "    labels_true = np.concatenate((np.ones_like(scores_bonafide_np), np.zeros_like(scores_spoof_np)))\n",
    "    labels_pred = (all_scores >= threshold).astype(int)\n",
    "    \n",
    "    return average_loss, eer, threshold, labels_true, labels_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05672159",
   "metadata": {},
   "source": [
    "### The training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dca569",
   "metadata": {},
   "source": [
    ">note: in wandb, scalers logs for every epoch, plots get overwritten (but still saved in artifacts?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c10873ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, dev_dataloader, criterion, optimizer, num_epochs,\n",
    "                min_eer, best_model_filename):\n",
    "\n",
    "    print(f\"Training started on device: {DEVICE}\")\n",
    "    model.to(DEVICE) \n",
    "\n",
    "    # Initial metric dictionary for the progress bar\n",
    "    metric_dict = {'train_loss': 'N/A', 'val_loss': 'N/A', 'val_eer': 'N/A', 'val_threshold': 'N/A'}\n",
    "\n",
    "    # Evaluate on validation set first to get a baseline\n",
    "    print(\"Evaluating on validation set before training...\")\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    val_loss_initial, val_eer_initial, threshold_initial, labels_true, labels_pred = evaluate_classifier(dev_dataloader, model, criterion)\n",
    "    metric_dict.update({'val_loss': f'{val_loss_initial:.3f}', 'val_eer': f'{val_eer_initial*100:.2f}%', 'val_threshold': f'{threshold_initial*100:.2f}%'})\n",
    "    print(f\"Initial Validation - Loss: {val_loss_initial:.4f}, EER: {val_eer_initial*100:.2f}%, Threshold: {threshold_initial*100:.2f}%\")\n",
    "\n",
    "    # Progress bar setup\n",
    "    total_steps = num_epochs * len(train_dataloader)\n",
    "    pbar = tqdm(total=total_steps, initial=0, postfix=metric_dict, unit=\"batch\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode (enables dropout, etc.)\n",
    "        pbar.set_description(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        \n",
    "        running_train_loss = 0.0\n",
    "        num_train_batches = 0\n",
    "\n",
    "        for batch_labels, batch_lengths, batch_pitchhnr, batch_global, batch_mfcc in train_dataloader:\n",
    "            # Move data to the specified device\n",
    "            # batch_lengths are used by pack_padded_sequence which expects them on CPU\n",
    "            batch_labels = batch_labels.to(DEVICE)\n",
    "            batch_pitchhnr = batch_pitchhnr.to(DEVICE)\n",
    "            batch_global = batch_global.to(DEVICE)\n",
    "            batch_mfcc = batch_mfcc.to(DEVICE)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass: Get model outputs (logits)\n",
    "            logits = model(batch_pitchhnr, batch_lengths, batch_global, batch_mfcc)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(logits, batch_labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            # --- FOR GRADIENT CLIPPING ---\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.01) # Start with max_norm=1.0 or 0.5\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update statistics for progress bar and logging\n",
    "            running_train_loss += loss.item()\n",
    "            num_train_batches += 1\n",
    "            \n",
    "            pbar.update(1) # Increment progress bar by one batch\n",
    "            metric_dict.update({'train_loss': f'{loss.item():.3f}'}) # Current batch loss\n",
    "            pbar.set_postfix(metric_dict)\n",
    "        \n",
    "        # Calculate average training loss for the epoch\n",
    "        avg_epoch_train_loss = running_train_loss / num_train_batches if num_train_batches > 0 else 0.0\n",
    "        metric_dict.update({'train_loss': f'{avg_epoch_train_loss:.3f}'}) # Average epoch loss\n",
    "        \n",
    "        # Evaluate on validation set after each epoch\n",
    "        avg_val_loss, val_eer, val_threshold, labels_true, labels_pred = evaluate_classifier(dev_dataloader, model, criterion)\n",
    "        \n",
    "        # Update with latest validation metrics\n",
    "        metric_dict.update({'val_loss': f'{avg_val_loss:.3f}', 'val_eer': f'{val_eer*100:.2f}%', 'val_threshold': f'{val_threshold*100:.2f}%'})\n",
    "        pbar.set_postfix(metric_dict)\n",
    "        \n",
    "        # Optional: Print epoch summary\n",
    "        print(f\"\\nEpoch {epoch+1} Summary: Avg Train Loss: {avg_epoch_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, EER: {val_eer*100:.2f}%, Threshold: {val_threshold*100:.2f}%\")\n",
    "\n",
    "        # log the train\\dev loss and the eer & threshold\n",
    "        run.log({\"train_loss\": avg_epoch_train_loss, \"dev_loss\": avg_val_loss, \n",
    "                   \"dev_eer\": val_eer, \"dev_threshold\":val_threshold, \"epoch\": epoch + 1})\n",
    "        \n",
    "        # update min eer and optimal model\n",
    "        if val_eer < min_eer:\n",
    "            min_eer = val_eer\n",
    "            torch.save(model.state_dict(), best_model_filename)\n",
    "            print(f\"Epoch {epoch+1}: New best model saved to '{best_model_filename}' with EER: {min_eer:.4f}\")\n",
    "\n",
    "            run.summary['best_validation_eer'] = min_eer\n",
    "            run.summary['best_eer_epoch'] = epoch + 1\n",
    "            run.summary['validation_loss_at_best_eer'] = avg_val_loss\n",
    "\n",
    "            # log the report and confusion matrix\n",
    "            class_names = ['SPOOF', 'BONAFIDE']     #NOTE: the order matters, need to match labels\n",
    "            report_columns =  [\"Class\", \"Precision\", \"Recall\", \"F1-score\", \"Support\"]\n",
    "            class_report = classification_report(labels_true, labels_pred, labels=[0, 1],\n",
    "                                        target_names=class_names).splitlines()\n",
    "            report_table = []\n",
    "            for line in class_report[2:(len(class_names)+2)]:\n",
    "                report_table.append(line.split())\n",
    "            run.log({\"Confusion Matix\": wandb.plot.confusion_matrix(y_true=labels_true, preds=labels_pred, class_names=class_names),\n",
    "                    \"Classification Report\": wandb.Table(data=report_table, columns=report_columns)})\n",
    "\n",
    "    pbar.close()\n",
    "    print(\"Training finished.\")\n",
    "    return min_eer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e00816c",
   "metadata": {},
   "source": [
    ">note: only partially deterministic for adaptivemaxpooling does not support the feature yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d2c89dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Trial with Seed: 42 ---\n",
      "Training started on device: cuda\n",
      "Evaluating on validation set before training...\n",
      "Initial Validation - Loss: 0.6786, EER: 59.34%, Threshold: 48.94%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c755efd1509e4784aa824e7c6746dd5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1425 [00:00<?, ?batch/s, train_loss=N/A, val_eer=59.34%, val_loss=0.679, val_threshold=48.9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users1/liqe/.conda/envs/pytorch0/lib/python3.11/site-packages/torch/autograd/__init__.py:266: UserWarning: adaptive_max_pool2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:79.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary: Avg Train Loss: 0.1268, Val Loss: 0.9103, EER: 3.53%, Threshold: 99.96%\n",
      "Epoch 1: New best model saved to 'best_model' with EER: 0.0353\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = config.epochs\n",
    "min_eer = float('inf')\n",
    "best_model_filename = 'best_model'\n",
    "\n",
    "for seed in config.seeds:\n",
    "    print(f\"\\n--- Starting Trial with Seed: {seed} ---\")\n",
    "    set_seed(seed)\n",
    "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    min_eer = train_model(model, train_dataloader, dev_dataloader, criterion, optimizer, NUM_EPOCHS,\n",
    "                min_eer, best_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd7ece4",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e2840319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging the best model (best_model) to W&B Artifacts...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model logged as W&B Artifact.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>dev_eer</td><td></td></tr><tr><td>dev_loss</td><td></td></tr><tr><td>dev_threshold</td><td></td></tr><tr><td>epoch</td><td></td></tr><tr><td>train_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_eer_epoch</td><td>1</td></tr><tr><td>best_validation_eer</td><td>0.03533</td></tr><tr><td>dev_eer</td><td>0.03533</td></tr><tr><td>dev_loss</td><td>0.91034</td></tr><tr><td>dev_threshold</td><td>0.99956</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>train_loss</td><td>0.12676</td></tr><tr><td>validation_loss_at_best_eer</td><td>0.91034</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_modulelist_cnn</strong> at: <a href='https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake/runs/ydn2vorz' target=\"_blank\">https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake/runs/ydn2vorz</a><br> View project at: <a href='https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake' target=\"_blank\">https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake</a><br>Synced 5 W&B file(s), 2 media file(s), 6 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250629_214528-ydn2vorz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B run finished.\n"
     ]
    }
   ],
   "source": [
    "if min_eer != float('inf'):\n",
    "    print(f\"Logging the best model ({best_model_filename}) to W&B Artifacts...\")\n",
    "    best_model_artifact = wandb.Artifact(\n",
    "        name=f\"{run.id}-best-model\", # Using run ID for uniqueness\n",
    "        type=\"model\",\n",
    "        description=f\"Best model according to EER ({min_eer:.4f}) achieved at epoch {run.summary.get('best_eer_epoch', 'N/A')}.\",\n",
    "        metadata={\"best_eer\": min_eer, \"epoch_of_best_eer\": run.summary.get('best_eer_epoch', 'N/A')}\n",
    "    )\n",
    "    best_model_artifact.add_file(best_model_filename) # Add the saved file\n",
    "    wandb.run.log_artifact(best_model_artifact, aliases=[\"best_eer_model\"]) # Add an alias\n",
    "    print(\"Best model logged as W&B Artifact.\")\n",
    "else:\n",
    "    print(\"No model was saved as best_eer did not improve from its initial value.\")\n",
    "\n",
    "run.finish()\n",
    "\n",
    "print(\"W&B run finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc0200f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
