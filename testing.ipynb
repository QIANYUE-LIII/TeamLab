{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f8e6394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import eval_metrics as em\n",
    "import os\n",
    "import eval_metrics as em\n",
    "from sklearn.metrics import classification_report\n",
    "from models import LSTM_FFN_branch, CNN_branch, SpoofEnsemble, LSTM_FFN_classifer, CNN_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138da182",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116d8225",
   "metadata": {},
   "source": [
    "#### get configs from the training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af8c77ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'SpoofEnsemble', 'seeds': [0, 7, 42], 'epochs': 70, 'dataset': 'ASVSpoof19_LA', 'feature': 'MFCC&Prosody', 'ffn_dims': [11, 64], 'batch_size': 32, 'attack_type': 'all', 'cnn_padding': 1, 'conv_kernel': [3, 3], 'pool_kernel': [2, 2], 'cnn_channels': [1, 32, 64, 128], 'dropout_rate': 0.3, 'oversampling': True, 'bidirectional': False, 'learning_rate': 0.0005, 'loss_function': 'weighted_CE', 'lstm_n_layers': 1, 'lstm_input_dim': 2, 'lstm_hidden_dim': 64}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "api = wandb.Api()\n",
    "#NOTE: remember to change\n",
    "# # 1. ensemble-all-11\n",
    "# run_path = \"qianyue-university-of-stuttgart/teamlab_deepfake/runs/8v8y4zrh\"\n",
    "# 08\n",
    "run_path = \"qianyue-university-of-stuttgart/teamlab_deepfake/runs/h456ty5q\"\n",
    "train_run = api.run(run_path)\n",
    "train_config = train_run.config\n",
    "\n",
    "print(train_config)\n",
    "print(type(train_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b65bac7",
   "metadata": {},
   "source": [
    "#### config setting for the current run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a640b86d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/users1/liqe/TeamLab_phonetics/TeamLab/wandb/run-20250708_092932-orvktkst</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake/runs/orvktkst' target=\"_blank\">EvaluationEnsemble_all_dev</a></strong> to <a href='https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake' target=\"_blank\">https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake/runs/orvktkst' target=\"_blank\">https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake/runs/orvktkst</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_data': 'dev', 'model': 'SpoofEnsemble', 'dataset': 'ASVSpoof19_LA', 'feature': 'MFCC&Prosody', 'attack_type': 'all', 'loss_function': 'weighted_CE', 'scheduler': False, 'scheduler_factor': 0.5, 'scheduler_patience': 4, 'epochs': 70, 'batch_size': 32, 'oversampling': True, 'learning_rate': 0.0005, 'dropout_rate': 0.3, 'lstm_input_dim': 2, 'lstm_hidden_dim': 64, 'bidirectional': False, 'lstm_n_layers': 1, 'ffn_dims': [11, 64], 'cnn_channels': [1, 32, 64, 128], 'conv_kernel': [3, 3], 'pool_kernel': [2, 2], 'cnn_padding': 1}\n"
     ]
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project = \"teamlab_deepfake\",\n",
    "    job_type = \"evaluation\",\n",
    "    name = \"EvaluationEnsemble_all_dev\",\n",
    "    notes = None,\n",
    "    config = {\n",
    "            # for testing\n",
    "            \"test_data\": \"dev\", #dev/test\n",
    "            # general\n",
    "            \"model\": train_config.get('model'),\n",
    "            \"dataset\": train_config.get('dataset'),\n",
    "            \"feature\": train_config.get('feature'),\n",
    "            \"attack_type\": train_config.get('attack_type'),\n",
    "            \"loss_function\": train_config.get('loss_function'),\n",
    "            \"scheduler\": train_config.get('scheduler', False),\n",
    "            \"scheduler_factor\": train_config.get('scheduler_factor', 0.5),\n",
    "            \"scheduler_patience\": train_config.get('scheduler_patience', 4),\n",
    "            \"epochs\": train_config.get('epochs'),\n",
    "            \"batch_size\": train_config.get('batch_size'),\n",
    "            \"oversampling\": train_config.get('oversampling'),\n",
    "            \"learning_rate\": train_config.get('learning_rate'),\n",
    "            \"dropout_rate\": train_config.get('dropout_rate'),\n",
    "            # lstm layer\n",
    "            \"lstm_input_dim\": train_config.get('lstm_input_dim'),\n",
    "            \"lstm_hidden_dim\": train_config.get('lstm_hidden_dim'),\n",
    "            \"bidirectional\": train_config.get('bidirectional'),\n",
    "            \"lstm_n_layers\": train_config.get('lstm_n_layers'),\n",
    "            # ffn layer\n",
    "            \"ffn_dims\": train_config.get('ffn_dims'),\n",
    "            # cnn_layer\n",
    "            \"cnn_channels\": train_config.get('cnn_channels'),\n",
    "            \"conv_kernel\": train_config.get('conv_kernel'),\n",
    "            \"pool_kernel\": train_config.get('pool_kernel'),\n",
    "            \"cnn_padding\": train_config.get('cnn_padding')\n",
    "    },\n",
    ")\n",
    "\n",
    "config=run.config\n",
    "            \n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84335c8e",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee249393",
   "metadata": {},
   "outputs": [],
   "source": [
    "PITCH_COLUMN = 'PITCH'\n",
    "HNR_COLUMN = 'HNR'\n",
    "JITTER_COLUMN = 'JITTER'\n",
    "SHIMMER_COLUMN = 'SHIMMER'\n",
    "MFCC_COLUMN = 'MFCC'\n",
    "LABEL_COLUMN = 'LABEL'\n",
    "ATTACK_TYPE_COLUMN = 'ATTACK_TYPE'\n",
    "AUDIO_ID_COLUMN = 'AUDIO_ID'\n",
    "                           \n",
    "NAN_REPLACEMENT_VALUE = 0.0  \n",
    "PADDING_VALUE = 0.0         \n",
    "LABEL_BONAFIDE = 1\n",
    "LABEL_SPOOF = 0\n",
    "\n",
    "if config.test_data == \"test\":\n",
    "    test_features_path = '/home/users1/liqe/TeamLab_phonetics/merged_eval_com.pkl'   #NOTE: tbc\n",
    "elif config.test_data == \"dev\":\n",
    "    test_features_path = '/home/users1/liqe/TeamLab_phonetics/merged_dev_com.pkl'\n",
    "else:\n",
    "    print(\"WARNING: invalid test data.\")\n",
    "df_test = pd.read_pickle(test_features_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd382b26",
   "metadata": {},
   "source": [
    "#### Dataprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95a882b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process dataset\n",
    "# If stored in a seperate .py -> need to inherit from the training code + attack type&id\n",
    "class ASVDataset(Dataset):\n",
    "    def __init__(self, dataframe, pitch_col, hnr_col, jitter_col, shimmer_col, mfcc_col, label_col, \n",
    "                 attack_type_col, audio_id_col, nan_replacement=NAN_REPLACEMENT_VALUE):\n",
    "        \n",
    "        self.labels = []\n",
    "        self.attack_type = []\n",
    "        self.audio_id = []\n",
    "        self.processed_pitchhnr = []\n",
    "        self.global_features = []\n",
    "        self.processed_mfcc = []\n",
    "        \n",
    "        print(f\"Attempting to process {len(dataframe)} entries from DataFrame\")\n",
    "        found_count = 0\n",
    "        # Iterate through the DataFrame, process and pad the features\n",
    "        for index, row in dataframe.iterrows():  \n",
    "            if not np.isnan(row[label_col]):\n",
    "                self.labels.append(row[label_col])\n",
    "                self.attack_type.append(row[attack_type_col])\n",
    "                self.audio_id.append(row[audio_id_col])\n",
    "\n",
    "                pitch_sequence_raw = row[pitch_col]\n",
    "                processed_pitch = np.nan_to_num(pitch_sequence_raw, nan=nan_replacement)\n",
    "                \n",
    "                hnr_sequence_raw = row[hnr_col]\n",
    "                processed_hnr = np.nan_to_num(hnr_sequence_raw, nan=nan_replacement)\n",
    "\n",
    "                ### NOTE:need to pad the two sequences to the same length\n",
    "                max_length = max(len(processed_pitch), len(processed_hnr))\n",
    "                if len(processed_pitch) > len(processed_hnr):\n",
    "                    padding = np.zeros(max_length - len(processed_hnr), dtype=processed_hnr.dtype)\n",
    "                    processed_hnr = np.concatenate((processed_hnr, padding))\n",
    "                else:\n",
    "                    padding = np.zeros(max_length - len(processed_pitch), dtype=processed_pitch.dtype)\n",
    "                    processed_pitch = np.concatenate((processed_pitch, padding))\n",
    "\n",
    "                combined_features = np.stack((processed_pitch, processed_hnr), axis=-1) \n",
    "                self.processed_pitchhnr.append(torch.tensor(combined_features, dtype=torch.float32))\n",
    "\n",
    "                # process and combine jitter and shimmer to one sequence\n",
    "                processed_jitter = np.nan_to_num(row[jitter_col], nan=nan_replacement)\n",
    "                processed_shimmer = np.nan_to_num(row[shimmer_col], nan=nan_replacement)\n",
    "                jitter_shimmer = np.concatenate((processed_jitter, processed_shimmer))\n",
    "                self.global_features.append(torch.tensor(jitter_shimmer, dtype=torch.float32))\n",
    "                \n",
    "                # process mfcc\n",
    "                mfcc = row[mfcc_col]\n",
    "                # NOTE: need transpose for padding (time, feature_dim)\n",
    "                self.processed_mfcc.append(torch.tensor(mfcc, dtype=torch.float32).T)\n",
    "\n",
    "                found_count += 1\n",
    "        \n",
    "        self.labels = torch.tensor(self.labels, dtype=torch.long)\n",
    "        print(f\"Successfully processed {found_count} samples out of {len(dataframe)} DataFrame entries.\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of matched samples in the dataset.\"\"\"\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns one sample from the dataset: a preprocessed pitch sequence and its label.\n",
    "        \"\"\"\n",
    "        label = self.labels[idx]\n",
    "        audio_id = self.audio_id[idx]\n",
    "        attack_type = self.attack_type[idx]\n",
    "        pitch_hnr = self.processed_pitchhnr[idx]\n",
    "        global_feature = self.global_features[idx]\n",
    "        mfcc = self.processed_mfcc[idx]\n",
    "        return label, pitch_hnr, global_feature, mfcc, audio_id, attack_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "813ea461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, padding_value=PADDING_VALUE):\n",
    "    \"\"\"\n",
    "    Pads sequences within a batch to the same length.\n",
    "    \"\"\"\n",
    "    labels = [item[0] for item in batch]\n",
    "    pitch_hnrs = [item[1] for item in batch]\n",
    "    global_features = [item[2] for item in batch]\n",
    "    mfccs = [item[3] for item in batch]\n",
    "    audio_ids = [item[4] for item in batch]\n",
    "    attack_types = [item[5] for item in batch]\n",
    "\n",
    "    labels = torch.stack(labels)\n",
    "\n",
    "    pitchhnr_lengths = torch.tensor([len(seq) for seq in pitch_hnrs], dtype=torch.long)\n",
    "    padded_pitchhnrs = pad_sequence(pitch_hnrs, batch_first=True, padding_value=padding_value)\n",
    "    if padded_pitchhnrs.ndim == 2:     # lstm expects: [batch_size, sequence_length, feature_size]\n",
    "        padded_pitchhnrs = padded_pitchhnrs.unsqueeze(2)\n",
    "\n",
    "    global_features = torch.stack(global_features)\n",
    "\n",
    "    padded_mfccs = pad_sequence(mfccs, batch_first=True, padding_value=padding_value)\n",
    "\n",
    "    return labels, audio_ids, attack_types, pitchhnr_lengths, padded_pitchhnrs, global_features, padded_mfccs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d54343",
   "metadata": {},
   "source": [
    "#### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02eeadc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to process 24844 entries from DataFrame\n",
      "Successfully processed 24844 samples out of 24844 DataFrame entries.\n",
      "\n",
      "--- Batch 1 ---\n",
      "  Labels (first 5): tensor([1, 0, 1, 0, 0])\n",
      "  IDs (first 5): ['LA_D_7341689', 'LA_D_9918902', 'LA_D_1275044', 'LA_D_4897222', 'LA_D_5631993']\n",
      "  Types (first 5): ['-', 'A03', '-', 'A02', 'A03']\n",
      "  Padded Sequences Shape: torch.Size([32, 672, 2])\n",
      "  Original Lengths (first 5): tensor([565, 373, 356, 470, 499])\n",
      "  Global Shape: torch.Size([32, 11])\n",
      "  MFCC Shape: torch.Size([32, 211, 60])\n"
     ]
    }
   ],
   "source": [
    "pitch_dataset_test = ASVDataset(dataframe=df_test,  \n",
    "                                    pitch_col=PITCH_COLUMN,\n",
    "                                    hnr_col=HNR_COLUMN,\n",
    "                                    jitter_col=JITTER_COLUMN,\n",
    "                                    shimmer_col=SHIMMER_COLUMN,\n",
    "                                    mfcc_col=MFCC_COLUMN,\n",
    "                                    label_col=LABEL_COLUMN,\n",
    "                                    attack_type_col=ATTACK_TYPE_COLUMN,\n",
    "                                    audio_id_col=AUDIO_ID_COLUMN,\n",
    "                                    nan_replacement=NAN_REPLACEMENT_VALUE)\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    pitch_dataset_test, batch_size=config.batch_size, shuffle=False, collate_fn=collate_fn, num_workers=8\n",
    ")\n",
    "\n",
    "## For inspection\n",
    "for i, batch_data in enumerate(test_dataloader):\n",
    "    # batch_data is a tuple\n",
    "    batch_labels, batch_ids, batch_types, batch_lengths, batch_pitchhnr, batch_global, batch_mfcc = batch_data\n",
    "    print(f\"\\n--- Batch {i+1} ---\")\n",
    "    print(f\"  Labels (first 5): {batch_labels[:5]}\")\n",
    "    print(f\"  IDs (first 5): {batch_ids[:5]}\")\n",
    "    print(f\"  Types (first 5): {batch_types[:5]}\")\n",
    "    print(f\"  Padded Sequences Shape: {batch_pitchhnr.shape}\")\n",
    "    print(f\"  Original Lengths (first 5): {batch_lengths[:5]}\")\n",
    "    print(f\"  Global Shape: {batch_global.shape}\")\n",
    "    print(f\"  MFCC Shape: {batch_mfcc.shape}\")\n",
    "    \n",
    "\n",
    "    if i == 0: # Break after the first batch for inspection\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8fcbe2",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "032f345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_explain(model, test_loader, device, num_examples=1):\n",
    "    model.eval()\n",
    "    results_data = []\n",
    "    explain_data = []\n",
    "    \n",
    "    # --- 1. GATHER MODEL PREDICTIONS ---\n",
    "    print(\"Gathering model predictions from the test set...\")\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            batch_labels, batch_ids, batch_types, batch_lengths, batch_pitchhnrs, batch_globals, batch_mfccs = batch\n",
    "            \n",
    "            batch_labels = batch_labels.to(device)\n",
    "            batch_mfccs = batch_mfccs.to(device)\n",
    "            batch_pitchhnrs = batch_pitchhnrs.to(device)\n",
    "            batch_globals = batch_globals.to(device)\n",
    "            \n",
    "            logits = model(batch_pitchhnrs, batch_lengths, batch_globals, batch_mfccs)\n",
    "\n",
    "            scores = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "\n",
    "            for i in range(len(scores)):\n",
    "                results_data.append({\n",
    "                    'audio_id': batch_ids[i],\n",
    "                    'attack_type': batch_types[i],\n",
    "                    'label_true': batch_labels[i].item(),    # from tensor to scaler\n",
    "                    'score': scores[i][LABEL_BONAFIDE],\n",
    "                })\n",
    "    \n",
    "\n",
    "    results_df = pd.DataFrame(results_data)\n",
    "    \n",
    "\n",
    "    # --- 2. CALCULATE OVERALL EER (THE OPERATIONAL METRIC) ---\n",
    "    print(\"\\n--- Overall Performance ---\")\n",
    "    bonafide_rows = results_df[results_df['label_true'] == LABEL_BONAFIDE]\n",
    "    spoof_rows = results_df[results_df['label_true'] == LABEL_SPOOF]\n",
    "    \n",
    "    scores_bonafide_np = bonafide_rows['score'].to_numpy()\n",
    "    scores_spoof_overall_np = spoof_rows['score'].to_numpy()\n",
    "    \n",
    "    # This is your main, global EER and threshold\n",
    "    overall_eer, overall_threshold = em.compute_eer(scores_bonafide_np, scores_spoof_overall_np)\n",
    "\n",
    "    run.log({\"eval_eer_overall\": overall_eer, \"eval_threshold_overall\": overall_threshold})\n",
    "    print(f\"Overall EER: {overall_eer*100:.2f}% at threshold {overall_threshold:.4f}\")\n",
    "\n",
    "    # --- 3. CALCULATE PER-ATTACK PERFORMANCE (THE DETAILED DIAGNOSIS) ---\n",
    "    print(\"\\n--- Per-Attack Performance Analysis ---\")\n",
    "    attack_analysis_results = []\n",
    "    unique_attacks = sorted(spoof_rows['attack_type'].unique())\n",
    "\n",
    "    # Calculate the fixed False Rejection Rate at the global threshold\n",
    "    frr_at_global_threshold = np.sum(scores_bonafide_np < overall_threshold) / len(scores_bonafide_np)\n",
    "    run.log({\"frr_at_global_threshold\": frr_at_global_threshold})\n",
    "    print(f\"FRR at Global Threshold ({overall_threshold:.4f}): {frr_at_global_threshold*100:.2f}%\")\n",
    "\n",
    "    for attack_type in unique_attacks:\n",
    "        current_attack_rows = spoof_rows[spoof_rows['attack_type'] == attack_type]\n",
    "        scores_current_attack = current_attack_rows['score'].to_numpy()\n",
    "        \n",
    "        if len(scores_current_attack) == 0:\n",
    "            continue\n",
    "\n",
    "        # Analysis A: What is the BEST POSSIBLE EER for this attack?\n",
    "        optimal_eer, optimal_threshold = em.compute_eer(scores_bonafide_np, scores_current_attack)\n",
    "        \n",
    "        # Analysis B: What is the ACTUAL error rate for this attack using the GLOBAL threshold?\n",
    "        false_acceptances = np.sum(scores_current_attack >= overall_threshold)\n",
    "        far_at_global_threshold = false_acceptances / len(scores_current_attack)\n",
    "        \n",
    "        # Store raw numeric values for logging and correct sorting in W&B UI\n",
    "        attack_analysis_results.append({\n",
    "            \"attack_type\": attack_type,\n",
    "            \"optimal_eer\": optimal_eer,\n",
    "            \"optimal_threshold\": optimal_threshold,\n",
    "            \"far_at_global_threshold\": far_at_global_threshold,\n",
    "            \"num_examples\": len(scores_current_attack)\n",
    "        })\n",
    "        # Use formatted strings only for the console printout\n",
    "        print(f\"  - {attack_type}: Optimal EER={optimal_eer*100:.2f}% | FAR @ Global Threshold={far_at_global_threshold*100:.2f}%\")\n",
    "        \n",
    "        # Log both metrics to W&B for easier plotting\n",
    "        run.log({\n",
    "            f\"eer_by_attack/{attack_type}\": optimal_eer,\n",
    "            f\"far_at_global_threshold/{attack_type}\": far_at_global_threshold\n",
    "        })\n",
    "\n",
    "    # Log the summary table of per-attack analysis\n",
    "    per_attack_df = pd.DataFrame(attack_analysis_results)\n",
    "    \n",
    "    # Explicitly convert columns to a numeric type before logging to ensure correct sorting\n",
    "    for col in [\"optimal_eer\", \"optimal_threshold\", \"far_at_global_threshold\"]:\n",
    "        if col in per_attack_df.columns:\n",
    "            per_attack_df[col] = pd.to_numeric(per_attack_df[col])\n",
    "\n",
    "    run.log({\"per_attack_analysis_table\": wandb.Table(dataframe=per_attack_df)})\n",
    "    \n",
    "    # --- 2. LOG QUANTITATIVE RESULTS ---\n",
    "    print(\"\\nLogging overall quantitative metrics to W&B...\")\n",
    "    \n",
    "    # Calculate predictions directly on the DataFrame to ensure correct alignment\n",
    "    results_df['prediction'] = (results_df['score'] >= overall_threshold).astype(int)\n",
    "\n",
    "    # Sort the DataFrame by score in descending order before logging\n",
    "    results_df_sorted = results_df.sort_values(by='score', ascending=False)\n",
    "    \n",
    "    # Log the sorted detailed results table\n",
    "    run.log({\"test_results_table\": wandb.Table(dataframe=results_df_sorted)})\n",
    "\n",
    "    # Create the true and predicted labels for the confusion matrix\n",
    "    labels_true = results_df['label_true'].to_numpy()\n",
    "    labels_pred = results_df['prediction'].to_numpy()\n",
    "    \n",
    "    class_names = ['SPOOF', 'BONAFIDE']\n",
    "    report_columns = [\"Class\", \"Precision\", \"Recall\", \"F1-score\", \"Support\"]\n",
    "    class_report = classification_report(labels_true, labels_pred, labels=[0, 1], target_names=class_names).splitlines()\n",
    "    report_table = []\n",
    "    for line in class_report[2:(len(class_names)+2)]:\n",
    "        report_table.append(line.split())\n",
    "    \n",
    "    run.log({\n",
    "        \"Confusion Matrix\": wandb.plot.confusion_matrix(y_true=labels_true, preds=labels_pred, class_names=class_names),\n",
    "        \"Classification Report\": wandb.Table(data=report_table, columns=report_columns)\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f8fa3a",
   "metadata": {},
   "source": [
    "### Initiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e415f0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device count: 9\n",
      "Using CUDA device: NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device_index = 4\n",
    "    torch.cuda.set_device(device_index)\n",
    "    DEVICE = torch.device('cuda')\n",
    "    print(f\"Using CUDA device: {torch.cuda.get_device_name(DEVICE)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "    DEVICE = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e60a17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "faf45b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_model():\n",
    "    lstm_ffn_out= LSTM_FFN_branch(lstm_input_dim=config.lstm_input_dim, lstm_hidden_dim=config.lstm_hidden_dim, lstm_n_layers=config.lstm_n_layers, bidirectional=config.bidirectional,\n",
    "                    ffn_dims=config.ffn_dims).to(DEVICE)\n",
    "    cnn_out = CNN_branch(cnn_channels=config.cnn_channels, conv_kernel=config.conv_kernel, pool_kernel=config.pool_kernel, cnn_padding=config.cnn_padding).to(DEVICE)\n",
    "\n",
    "    if config.model==\"SpoofEnsemble\":\n",
    "        model = SpoofEnsemble(lstm_ffn_branch=lstm_ffn_out, cnn_branch=cnn_out, output_dim=2, dropout=config.dropout_rate).to(DEVICE)\n",
    "    elif config.model==\"LSTM_FFN_classifier\":\n",
    "        model = LSTM_FFN_classifer(lstm_ffn_out=lstm_ffn_out, output_dim=2, dropout=config.dropout_rate).to(DEVICE)\n",
    "    elif config.model==\"CNN_classifier\":\n",
    "        model = CNN_classifer(cnn_out=cnn_out, output_dim=2, dropout=config.dropout_rate).to(DEVICE)\n",
    "    else:\n",
    "        print(\"WARNING: invalid model name.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b3af53",
   "metadata": {},
   "source": [
    "### Setup and Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74ff8b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from /home/users1/liqe/TeamLab_phonetics/TeamLab/artifacts/h456ty5q-best-model:v0/best_model.\n",
      "Gathering model predictions from the test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 777/777 [00:12<00:00, 62.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Overall Performance ---\n",
      "Overall EER: 0.43% at threshold 0.0011\n",
      "\n",
      "--- Per-Attack Performance Analysis ---\n",
      "FRR at Global Threshold (0.0011): 0.43%\n",
      "  - A01: Optimal EER=0.11% | FAR @ Global Threshold=0.00%\n",
      "  - A02: Optimal EER=0.13% | FAR @ Global Threshold=0.00%\n",
      "  - A03: Optimal EER=0.19% | FAR @ Global Threshold=0.03%\n",
      "  - A04: Optimal EER=0.27% | FAR @ Global Threshold=0.16%\n",
      "  - A05: Optimal EER=0.59% | FAR @ Global Threshold=1.00%\n",
      "  - A06: Optimal EER=0.62% | FAR @ Global Threshold=1.43%\n",
      "\n",
      "Logging overall quantitative metrics to W&B...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eer_by_attack/A01</td><td>▁</td></tr><tr><td>eer_by_attack/A02</td><td>▁</td></tr><tr><td>eer_by_attack/A03</td><td>▁</td></tr><tr><td>eer_by_attack/A04</td><td>▁</td></tr><tr><td>eer_by_attack/A05</td><td>▁</td></tr><tr><td>eer_by_attack/A06</td><td>▁</td></tr><tr><td>eval_eer_overall</td><td>▁</td></tr><tr><td>eval_threshold_overall</td><td>▁</td></tr><tr><td>far_at_global_threshold/A01</td><td>▁</td></tr><tr><td>far_at_global_threshold/A02</td><td>▁</td></tr><tr><td>far_at_global_threshold/A03</td><td>▁</td></tr><tr><td>far_at_global_threshold/A04</td><td>▁</td></tr><tr><td>far_at_global_threshold/A05</td><td>▁</td></tr><tr><td>far_at_global_threshold/A06</td><td>▁</td></tr><tr><td>frr_at_global_threshold</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eer_by_attack/A01</td><td>0.00113</td></tr><tr><td>eer_by_attack/A02</td><td>0.00126</td></tr><tr><td>eer_by_attack/A03</td><td>0.00192</td></tr><tr><td>eer_by_attack/A04</td><td>0.00272</td></tr><tr><td>eer_by_attack/A05</td><td>0.0059</td></tr><tr><td>eer_by_attack/A06</td><td>0.00623</td></tr><tr><td>eval_eer_overall</td><td>0.00431</td></tr><tr><td>eval_threshold_overall</td><td>0.00112</td></tr><tr><td>far_at_global_threshold/A01</td><td>0</td></tr><tr><td>far_at_global_threshold/A02</td><td>0</td></tr><tr><td>far_at_global_threshold/A03</td><td>0.00027</td></tr><tr><td>far_at_global_threshold/A04</td><td>0.00161</td></tr><tr><td>far_at_global_threshold/A05</td><td>0.00996</td></tr><tr><td>far_at_global_threshold/A06</td><td>0.01426</td></tr><tr><td>frr_at_global_threshold</td><td>0.00432</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">EvaluationEnsemble_all_dev</strong> at: <a href='https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake/runs/orvktkst' target=\"_blank\">https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake/runs/orvktkst</a><br> View project at: <a href='https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake' target=\"_blank\">https://wandb.ai/qianyue-university-of-stuttgart/teamlab_deepfake</a><br>Synced 5 W&B file(s), 4 media file(s), 10 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250708_092932-orvktkst/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation complete. Results logged to W&B.\n"
     ]
    }
   ],
   "source": [
    "model = initiate_model() \n",
    "\n",
    "# NOTE: remember to change\n",
    "# artifact = run.use_artifact('qianyue-university-of-stuttgart/teamlab_deepfake/ey187sci-best-model:v0', type='model')\n",
    "# artifact_dir = artifact.download()\n",
    "# model_path = os.path.join(artifact_dir, 'best_model')\n",
    "model_path = '/home/users1/liqe/TeamLab_phonetics/TeamLab/artifacts/h456ty5q-best-model:v0/best_model'\n",
    "\n",
    "try:\n",
    "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "    print(f\"Model loaded from {model_path}.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"WARNING: Model path not found at '{model_path}'. Using randomly initialized model.\")\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "# --- 3. Log Model as a W&B Artifact ---\n",
    "if os.path.exists(model_path):\n",
    "    model_artifact = wandb.Artifact(\n",
    "        name=f\"model-{run.id}\", \n",
    "        type=\"model\",\n",
    "        description=\"Trained model checkpoint for spoof detection.\"\n",
    "    )\n",
    "    model_artifact.add_file(model_path)\n",
    "    run.log_artifact(model_artifact)\n",
    "\n",
    "evaluate_and_explain(model, test_dataloader, DEVICE)\n",
    "\n",
    "run.finish()\n",
    "print(\"\\nEvaluation complete. Results logged to W&B.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59df24a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
