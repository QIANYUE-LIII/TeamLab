{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f93cc234-1bee-470f-8974-7c632f313d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /home/users1/qianqu/.conda/lib/python3.11/site-packages (0.20.1)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/users1/qianqu/.conda/lib/python3.11/site-packages (from wandb) (8.2.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/users1/qianqu/.conda/lib/python3.11/site-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: packaging in /home/users1/qianqu/.conda/lib/python3.11/site-packages (from wandb) (25.0)\n",
      "Requirement already satisfied: platformdirs in /home/users1/qianqu/.conda/lib/python3.11/site-packages (from wandb) (4.3.8)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /home/users1/qianqu/.conda/lib/python3.11/site-packages (from wandb) (6.31.1)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/users1/qianqu/.conda/lib/python3.11/site-packages (from wandb) (7.0.0)\n",
      "Requirement already satisfied: pydantic<3 in /home/users1/qianqu/.conda/lib/python3.11/site-packages (from wandb) (2.11.7)\n",
      "Requirement already satisfied: pyyaml in /home/users1/qianqu/.conda/lib/python3.11/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/users1/qianqu/.conda/lib/python3.11/site-packages (from wandb) (2.32.4)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/users1/qianqu/.conda/lib/python3.11/site-packages (from wandb) (2.30.0)\n",
      "Requirement already satisfied: setproctitle in /home/users1/qianqu/.conda/lib/python3.11/site-packages (from wandb) (1.3.6)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in /home/users1/qianqu/.conda/lib/python3.11/site-packages (from wandb) (4.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/users1/qianqu/.conda/lib/python3.11/site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/users1/qianqu/.conda/lib/python3.11/site-packages (from pydantic<3->wandb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/users1/qianqu/.conda/lib/python3.11/site-packages (from pydantic<3->wandb) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/users1/qianqu/.conda/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/users1/qianqu/.conda/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/users1/qianqu/.conda/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/users1/qianqu/.conda/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2025.6.15)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/users1/qianqu/.conda/lib/python3.11/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/users1/qianqu/.conda/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3827d48e-fdfb-4018-8c67-5a4369279a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nbformat in /home/users1/qianqu/.conda/lib/python3.11/site-packages (5.10.4)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /home/users1/qianqu/.conda/lib/python3.11/site-packages (from nbformat) (2.21.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /home/users1/qianqu/.conda/lib/python3.11/site-packages (from nbformat) (4.24.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/users1/qianqu/.conda/lib/python3.11/site-packages (from nbformat) (5.8.1)\n",
      "Requirement already satisfied: traitlets>=5.1 in /home/users1/qianqu/.conda/lib/python3.11/site-packages (from nbformat) (5.14.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/users1/qianqu/.conda/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/users1/qianqu/.conda/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/users1/qianqu/.conda/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/users1/qianqu/.conda/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat) (0.25.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/users1/qianqu/.conda/lib/python3.11/site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (4.3.8)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /home/users1/qianqu/.conda/lib/python3.11/site-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat) (4.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e6425958-ca6e-42a4-aab0-67bfb20cac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26bfb025-9e77-481e-be4f-4cee44c6574f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a021bf38-b046-453a-94bf-9831579d40b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from eer import compute_eer\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c6e1d99-8224-4003-a4ed-dcf01663ebdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9af20f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19f5a858-3157-457d-9d37-3e89dab12ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(X_path, y_path):\n",
    "    with open(X_path, 'rb') as f:\n",
    "        X_train = pickle.load(f)\n",
    "    with open(y_path, 'rb') as f:\n",
    "        y_train = pickle.load(f)   \n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "79522879-f215-4b87-874c-25c3c7d9ac7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = get_data(X_path ='../data/dataset/mfcc_train_60_cnn/X_train_60_cnn.pkl', y_path = '../data/dataset/mfcc_train_60_cnn/y_train_60_cnn.pkl')\n",
    "X_dev, y_dev = get_data(X_path ='../data/dataset/mfcc_dev_60_cnn/X_dev_60_cnn.pkl', y_path = '../data/dataset/mfcc_dev_60_cnn/y_dev_60_cnn.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2faa4f2f-94b6-45e4-bc92-ecf9944b720e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 75)\n",
      "(60, 88)\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0].shape)\n",
    "print(X_train[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "66c91d3c-500b-415e-b8a1-2294f65f44a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_and_tensorize(X, y, max_len):\n",
    "    # (n_mfcc, T)\n",
    "    max_len = max([x.shape[1] for x in X])  \n",
    "    X_padded = [ np.pad(x, ((0, 0), (0, max_len - x.shape[1])), mode='constant') for x in X]\n",
    "    X_tensor = torch.tensor(np.stack(X_padded), dtype=torch.float32).unsqueeze(1)  # (N, 1, n_mfcc, T)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "    return X_tensor, y_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c82141ae-f742-4b3d-8932-484e47c481e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(max([x.shape[1] for x in X_train]), max([x.shape[1] for x in X_dev]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1976b861-fc1a-4392-bd3a-9438261fd8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413\n"
     ]
    }
   ],
   "source": [
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "47028a00-e270-41d9-b17e-d3a0e86388f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final, y_train_final = pad_and_tensorize(X_train, y_train, max_len)\n",
    "X_dev_final, y_dev_final = pad_and_tensorize(X_dev, y_dev, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "09f2b2d1-d3ff-423f-8bdc-408bd5b87c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNSpoofDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNSpoofDetector, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3,3), padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3,3), padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3,3), padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.pool3 = nn.AdaptiveMaxPool2d((1, 1))  # Output size: [batch, 64, 1, 1]\n",
    "        \n",
    "        self.fc = nn.Linear(64, 1)  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f918b6b0-88bf-4fcb-84b9-767e9858c2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train, X_dev, y_dev, run_name, config):\n",
    "    wandb.finish()\n",
    "    os.environ[\"WANDB_START_METHOD\"] = \"thread\"\n",
    "    # Initialize wandb experiment and log hyperparameters\n",
    "    wandb.init(project=\"asvspoof-baseline\", name=run_name, config=config, reinit=True)\n",
    "    config = wandb.config  # Use wandb.config for convenient access\n",
    "    \n",
    "    # ===== Set device and initialize model =====\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = CNNSpoofDetector().to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "\n",
    "    # ===== Prepare DataLoaders =====\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=config.batch_size, shuffle=True)\n",
    "    dev_loader = DataLoader(TensorDataset(X_dev, y_dev), batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "    # ===== Training loop =====\n",
    "    best_eer = float(\"inf\")  # Track best EER score\n",
    "    best_model_path = 'best_model.pth'\n",
    "    num_epochs = config.epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "        # Move data to device\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()           # Clear gradients\n",
    "            outputs = model(X_batch).squeeze()  # Forward pass and remove singleton dimension\n",
    "            loss = criterion(outputs, y_batch)  # Compute binary cross-entropy loss\n",
    "            loss.backward()                # Backpropagate\n",
    "            optimizer.step()               # Update model weights\n",
    "\n",
    "            running_loss += loss.item()    # Accumulate batch loss\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss:.4f}\")\n",
    "\n",
    "        # ------------------ Evaluate EER on development set ------------------\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        all_outputs = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():  # No gradient calculation needed during evaluation\n",
    "            for X_dev, y_dev in dev_loader:\n",
    "                X_dev, y_dev = X_dev.to(device), y_dev.to(device).float()\n",
    "                outputs = model(X_dev).squeeze()  # Forward pass\n",
    "                all_outputs.extend(outputs.cpu().numpy())  # Save outputs\n",
    "                all_labels.extend(y_dev.cpu().numpy())     # Save true labels\n",
    "\n",
    "        all_outputs = np.array(all_outputs)\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_probs = 1 / (1 + np.exp(-all_outputs))  # Sigmoid\n",
    "        bonafide_probs = all_probs[all_labels == 1]\n",
    "        spoof_probs    = all_probs[all_labels == 0]\n",
    "    \n",
    "\n",
    "        # Compute Equal Error Rate and the threshold that achieves it\n",
    "        eer, threshold = compute_eer(np.array(bonafide_probs), np.array(spoof_probs))\n",
    "        print(f\"Epoch {epoch+1} -- Dev EER: {eer:.4f}, Threshold: {threshold:.4f}\")\n",
    "\n",
    "        # Log metrics to wandb\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"loss\": running_loss,\n",
    "            \"dev_eer\": float(eer),\n",
    "            \"threshold\": float(threshold)\n",
    "        })\n",
    "\n",
    "        if eer < best_eer:\n",
    "            best_eer = eer\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"✅ Saved new best model with EER: {eer:.4f}\")\n",
    "        print(f\"✅ Training complete. Best EER: {best_eer:.4f}\")\n",
    "    wandb.finish()  # Finalize wandb run\n",
    "    return best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c3b96a3d-14a3-47a1-9bd7-5adf2304722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_run_name(config, model_name=\"cnn\"):\n",
    "    \"\"\"\n",
    "    Generate a descriptive wandb run name based on model name, key hyperparameters, and timestamp.\n",
    "    \n",
    "    Example output:\n",
    "        cnn-lr0.001-bs32-ep10-20250622-1730\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "    \n",
    "    # Optional: format lr nicely\n",
    "    lr_str = f\"{config['lr']:.0e}\" if config['lr'] < 1e-2 else str(config['lr'])\n",
    "\n",
    "    run_name = f\"{model_name}-lr{lr_str}-bs{config['batch_size']}-ep{config['epochs']}-{timestamp}\"\n",
    "    return run_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f1026b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"lr\": 1e-3,\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 200,\n",
    "    \"best_model_path\": \"best_model.pth\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7b2c4852-564b-4315-8ba9-c03e80d4c2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = generate_run_name(config, model_name=\"cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e964ddaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn-lr1e-03-bs32-ep200-20250623-0613\n"
     ]
    }
   ],
   "source": [
    "print(run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de30ed7-6e6d-4f30-9cba-7f2176c22045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/users1/qianqu/ASVspoof_project/notebooks/wandb/run-20250623_061341-d7m2d3wh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/qianqianrumail-university-of-stuttgart/asvspoof-baseline/runs/d7m2d3wh' target=\"_blank\">cnn-lr1e-03-bs32-ep200-20250623-0613</a></strong> to <a href='https://wandb.ai/qianqianrumail-university-of-stuttgart/asvspoof-baseline' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/qianqianrumail-university-of-stuttgart/asvspoof-baseline' target=\"_blank\">https://wandb.ai/qianqianrumail-university-of-stuttgart/asvspoof-baseline</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/qianqianrumail-university-of-stuttgart/asvspoof-baseline/runs/d7m2d3wh' target=\"_blank\">https://wandb.ai/qianqianrumail-university-of-stuttgart/asvspoof-baseline/runs/d7m2d3wh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 107.0072\n",
      "Epoch 1 -- Dev EER: 0.0667, Threshold: 0.1032\n",
      "✅ Saved new best model with EER: 0.0667\n",
      "✅ Training complete. Best EER: 0.0667\n",
      "Epoch 2/200, Loss: 41.9766\n",
      "Epoch 2 -- Dev EER: 0.0503, Threshold: 0.3996\n",
      "✅ Saved new best model with EER: 0.0503\n",
      "✅ Training complete. Best EER: 0.0503\n",
      "Epoch 3/200, Loss: 24.8991\n",
      "Epoch 3 -- Dev EER: 0.0483, Threshold: 0.0349\n",
      "✅ Saved new best model with EER: 0.0483\n",
      "✅ Training complete. Best EER: 0.0483\n",
      "Epoch 4/200, Loss: 18.3727\n",
      "Epoch 4 -- Dev EER: 0.0360, Threshold: 0.3394\n",
      "✅ Saved new best model with EER: 0.0360\n",
      "✅ Training complete. Best EER: 0.0360\n",
      "Epoch 5/200, Loss: 13.5709\n",
      "Epoch 5 -- Dev EER: 0.0361, Threshold: 0.0071\n",
      "✅ Training complete. Best EER: 0.0360\n",
      "Epoch 6/200, Loss: 10.3497\n",
      "Epoch 6 -- Dev EER: 0.0271, Threshold: 0.1600\n",
      "✅ Saved new best model with EER: 0.0271\n",
      "✅ Training complete. Best EER: 0.0271\n",
      "Epoch 7/200, Loss: 9.1370\n",
      "Epoch 7 -- Dev EER: 0.0349, Threshold: 0.0029\n",
      "✅ Training complete. Best EER: 0.0271\n",
      "Epoch 8/200, Loss: 8.8245\n",
      "Epoch 8 -- Dev EER: 0.0407, Threshold: 0.0588\n",
      "✅ Training complete. Best EER: 0.0271\n",
      "Epoch 9/200, Loss: 5.2622\n",
      "Epoch 9 -- Dev EER: 0.0322, Threshold: 0.0087\n",
      "✅ Training complete. Best EER: 0.0271\n",
      "Epoch 10/200, Loss: 6.1171\n",
      "Epoch 10 -- Dev EER: 0.0338, Threshold: 0.2432\n",
      "✅ Training complete. Best EER: 0.0271\n",
      "Epoch 11/200, Loss: 5.0877\n",
      "Epoch 11 -- Dev EER: 0.0275, Threshold: 0.0032\n",
      "✅ Training complete. Best EER: 0.0271\n",
      "Epoch 12/200, Loss: 1.5106\n",
      "Epoch 12 -- Dev EER: 0.0243, Threshold: 0.0006\n",
      "✅ Saved new best model with EER: 0.0243\n",
      "✅ Training complete. Best EER: 0.0243\n",
      "Epoch 13/200, Loss: 5.0159\n",
      "Epoch 13 -- Dev EER: 0.0624, Threshold: 0.0553\n",
      "✅ Training complete. Best EER: 0.0243\n",
      "Epoch 14/200, Loss: 3.8174\n",
      "Epoch 14 -- Dev EER: 0.0243, Threshold: 0.0025\n",
      "✅ Training complete. Best EER: 0.0243\n",
      "Epoch 15/200, Loss: 0.8317\n",
      "Epoch 15 -- Dev EER: 0.0330, Threshold: 0.0048\n",
      "✅ Training complete. Best EER: 0.0243\n",
      "Epoch 16/200, Loss: 3.0219\n",
      "Epoch 16 -- Dev EER: 0.0294, Threshold: 0.0552\n",
      "✅ Training complete. Best EER: 0.0243\n",
      "Epoch 17/200, Loss: 3.8438\n",
      "Epoch 17 -- Dev EER: 0.0333, Threshold: 0.0018\n",
      "✅ Training complete. Best EER: 0.0243\n",
      "Epoch 18/200, Loss: 2.5599\n",
      "Epoch 18 -- Dev EER: 0.0247, Threshold: 0.0639\n",
      "✅ Training complete. Best EER: 0.0243\n",
      "Epoch 19/200, Loss: 6.3712\n",
      "Epoch 19 -- Dev EER: 0.0254, Threshold: 0.0018\n",
      "✅ Training complete. Best EER: 0.0243\n",
      "Epoch 20/200, Loss: 0.5026\n",
      "Epoch 20 -- Dev EER: 0.0251, Threshold: 0.0031\n",
      "✅ Training complete. Best EER: 0.0243\n",
      "Epoch 21/200, Loss: 0.2980\n",
      "Epoch 21 -- Dev EER: 0.0223, Threshold: 0.0006\n",
      "✅ Saved new best model with EER: 0.0223\n",
      "✅ Training complete. Best EER: 0.0223\n",
      "Epoch 22/200, Loss: 2.0686\n",
      "Epoch 22 -- Dev EER: 0.0283, Threshold: 0.0002\n",
      "✅ Training complete. Best EER: 0.0223\n",
      "Epoch 23/200, Loss: 4.5390\n",
      "Epoch 23 -- Dev EER: 0.0224, Threshold: 0.0167\n",
      "✅ Training complete. Best EER: 0.0223\n",
      "Epoch 24/200, Loss: 0.5844\n",
      "Epoch 24 -- Dev EER: 0.0255, Threshold: 0.0080\n",
      "✅ Training complete. Best EER: 0.0223\n",
      "Epoch 25/200, Loss: 0.1384\n",
      "Epoch 25 -- Dev EER: 0.0224, Threshold: 0.0003\n",
      "✅ Training complete. Best EER: 0.0223\n",
      "Epoch 26/200, Loss: 0.0641\n",
      "Epoch 26 -- Dev EER: 0.0212, Threshold: 0.0012\n",
      "✅ Saved new best model with EER: 0.0212\n",
      "✅ Training complete. Best EER: 0.0212\n",
      "Epoch 27/200, Loss: 0.0376\n",
      "Epoch 27 -- Dev EER: 0.0208, Threshold: 0.0021\n",
      "✅ Saved new best model with EER: 0.0208\n",
      "✅ Training complete. Best EER: 0.0208\n",
      "Epoch 28/200, Loss: 0.0272\n",
      "Epoch 28 -- Dev EER: 0.0216, Threshold: 0.0015\n",
      "✅ Training complete. Best EER: 0.0208\n",
      "Epoch 29/200, Loss: 0.0222\n",
      "Epoch 29 -- Dev EER: 0.0228, Threshold: 0.0006\n",
      "✅ Training complete. Best EER: 0.0208\n",
      "Epoch 30/200, Loss: 0.0153\n",
      "Epoch 30 -- Dev EER: 0.0232, Threshold: 0.0004\n",
      "✅ Training complete. Best EER: 0.0208\n",
      "Epoch 31/200, Loss: 0.0131\n",
      "Epoch 31 -- Dev EER: 0.0212, Threshold: 0.0005\n",
      "✅ Training complete. Best EER: 0.0208\n",
      "Epoch 32/200, Loss: 0.0103\n",
      "Epoch 32 -- Dev EER: 0.0220, Threshold: 0.0001\n",
      "✅ Training complete. Best EER: 0.0208\n",
      "Epoch 33/200, Loss: 8.1315\n",
      "Epoch 33 -- Dev EER: 0.0235, Threshold: 0.0005\n",
      "✅ Training complete. Best EER: 0.0208\n",
      "Epoch 34/200, Loss: 1.0983\n",
      "Epoch 34 -- Dev EER: 0.0318, Threshold: 0.0029\n",
      "✅ Training complete. Best EER: 0.0208\n",
      "Epoch 35/200, Loss: 0.2640\n",
      "Epoch 35 -- Dev EER: 0.0247, Threshold: 0.0014\n",
      "✅ Training complete. Best EER: 0.0208\n",
      "Epoch 36/200, Loss: 0.0908\n",
      "Epoch 36 -- Dev EER: 0.0196, Threshold: 0.0012\n",
      "✅ Saved new best model with EER: 0.0196\n",
      "✅ Training complete. Best EER: 0.0196\n",
      "Epoch 37/200, Loss: 0.0522\n",
      "Epoch 37 -- Dev EER: 0.0204, Threshold: 0.0016\n",
      "✅ Training complete. Best EER: 0.0196\n",
      "Epoch 38/200, Loss: 0.0282\n",
      "Epoch 38 -- Dev EER: 0.0219, Threshold: 0.0008\n",
      "✅ Training complete. Best EER: 0.0196\n",
      "Epoch 39/200, Loss: 0.0225\n",
      "Epoch 39 -- Dev EER: 0.0224, Threshold: 0.0004\n",
      "✅ Training complete. Best EER: 0.0196\n",
      "Epoch 40/200, Loss: 0.0142\n",
      "Epoch 40 -- Dev EER: 0.0208, Threshold: 0.0004\n",
      "✅ Training complete. Best EER: 0.0196\n",
      "Epoch 41/200, Loss: 0.0097\n",
      "Epoch 41 -- Dev EER: 0.0212, Threshold: 0.0008\n",
      "✅ Training complete. Best EER: 0.0196\n",
      "Epoch 42/200, Loss: 0.0066\n",
      "Epoch 42 -- Dev EER: 0.0200, Threshold: 0.0003\n",
      "✅ Training complete. Best EER: 0.0196\n",
      "Epoch 43/200, Loss: 6.1755\n",
      "Epoch 43 -- Dev EER: 0.0279, Threshold: 0.0022\n",
      "✅ Training complete. Best EER: 0.0196\n",
      "Epoch 44/200, Loss: 0.9714\n",
      "Epoch 44 -- Dev EER: 0.0216, Threshold: 0.0016\n",
      "✅ Training complete. Best EER: 0.0196\n",
      "Epoch 45/200, Loss: 0.1588\n",
      "Epoch 45 -- Dev EER: 0.0224, Threshold: 0.0006\n",
      "✅ Training complete. Best EER: 0.0196\n",
      "Epoch 46/200, Loss: 0.1321\n",
      "Epoch 46 -- Dev EER: 0.0224, Threshold: 0.0035\n",
      "✅ Training complete. Best EER: 0.0196\n",
      "Epoch 47/200, Loss: 0.0342\n",
      "Epoch 47 -- Dev EER: 0.0220, Threshold: 0.0005\n",
      "✅ Training complete. Best EER: 0.0196\n",
      "Epoch 48/200, Loss: 0.0248\n",
      "Epoch 48 -- Dev EER: 0.0216, Threshold: 0.0003\n",
      "✅ Training complete. Best EER: 0.0196\n",
      "Epoch 49/200, Loss: 0.0145\n",
      "Epoch 49 -- Dev EER: 0.0208, Threshold: 0.0007\n",
      "✅ Training complete. Best EER: 0.0196\n",
      "Epoch 50/200, Loss: 0.0113\n",
      "Epoch 50 -- Dev EER: 0.0216, Threshold: 0.0004\n",
      "✅ Training complete. Best EER: 0.0196\n",
      "Epoch 51/200, Loss: 0.0071\n",
      "Epoch 51 -- Dev EER: 0.0212, Threshold: 0.0002\n",
      "✅ Training complete. Best EER: 0.0196\n",
      "Epoch 52/200, Loss: 2.7708\n",
      "Epoch 52 -- Dev EER: 0.0313, Threshold: 0.0048\n",
      "✅ Training complete. Best EER: 0.0196\n",
      "Epoch 53/200, Loss: 1.4260\n",
      "Epoch 53 -- Dev EER: 0.0216, Threshold: 0.0008\n",
      "✅ Training complete. Best EER: 0.0196\n",
      "Epoch 54/200, Loss: 1.1418\n",
      "Epoch 54 -- Dev EER: 0.0260, Threshold: 0.0048\n",
      "✅ Training complete. Best EER: 0.0196\n",
      "Epoch 55/200, Loss: 0.6277\n",
      "Epoch 55 -- Dev EER: 0.0204, Threshold: 0.0018\n",
      "✅ Training complete. Best EER: 0.0196\n",
      "Epoch 56/200, Loss: 0.0860\n",
      "Epoch 56 -- Dev EER: 0.0212, Threshold: 0.0012\n",
      "✅ Training complete. Best EER: 0.0196\n",
      "Epoch 57/200, Loss: 0.0250\n",
      "Epoch 57 -- Dev EER: 0.0208, Threshold: 0.0007\n",
      "✅ Training complete. Best EER: 0.0196\n",
      "Epoch 58/200, Loss: 0.0201\n",
      "Epoch 58 -- Dev EER: 0.0208, Threshold: 0.0008\n",
      "✅ Training complete. Best EER: 0.0196\n",
      "Epoch 59/200, Loss: 0.0170\n",
      "Epoch 59 -- Dev EER: 0.0201, Threshold: 0.0007\n",
      "✅ Training complete. Best EER: 0.0196\n",
      "Epoch 60/200, Loss: 0.0093\n",
      "Epoch 60 -- Dev EER: 0.0204, Threshold: 0.0005\n",
      "✅ Training complete. Best EER: 0.0196\n",
      "Epoch 61/200, Loss: 0.0079\n",
      "Epoch 61 -- Dev EER: 0.0192, Threshold: 0.0002\n",
      "✅ Saved new best model with EER: 0.0192\n",
      "✅ Training complete. Best EER: 0.0192\n",
      "Epoch 62/200, Loss: 0.0058\n",
      "Epoch 62 -- Dev EER: 0.0208, Threshold: 0.0003\n",
      "✅ Training complete. Best EER: 0.0192\n",
      "Epoch 63/200, Loss: 0.0044\n",
      "Epoch 63 -- Dev EER: 0.0196, Threshold: 0.0005\n",
      "✅ Training complete. Best EER: 0.0192\n",
      "Epoch 64/200, Loss: 0.0026\n",
      "Epoch 64 -- Dev EER: 0.0200, Threshold: 0.0001\n",
      "✅ Training complete. Best EER: 0.0192\n",
      "Epoch 65/200, Loss: 2.8286\n",
      "Epoch 65 -- Dev EER: 0.0400, Threshold: 0.0023\n",
      "✅ Training complete. Best EER: 0.0192\n",
      "Epoch 66/200, Loss: 2.4754\n",
      "Epoch 66 -- Dev EER: 0.0247, Threshold: 0.0048\n",
      "✅ Training complete. Best EER: 0.0192\n",
      "Epoch 67/200, Loss: 0.1265\n",
      "Epoch 67 -- Dev EER: 0.0255, Threshold: 0.0018\n",
      "✅ Training complete. Best EER: 0.0192\n",
      "Epoch 68/200, Loss: 0.0405\n",
      "Epoch 68 -- Dev EER: 0.0231, Threshold: 0.0018\n",
      "✅ Training complete. Best EER: 0.0192\n",
      "Epoch 69/200, Loss: 0.0260\n",
      "Epoch 69 -- Dev EER: 0.0224, Threshold: 0.0006\n",
      "✅ Training complete. Best EER: 0.0192\n",
      "Epoch 70/200, Loss: 0.0199\n",
      "Epoch 70 -- Dev EER: 0.0220, Threshold: 0.0003\n",
      "✅ Training complete. Best EER: 0.0192\n",
      "Epoch 71/200, Loss: 0.0133\n",
      "Epoch 71 -- Dev EER: 0.0212, Threshold: 0.0005\n",
      "✅ Training complete. Best EER: 0.0192\n",
      "Epoch 72/200, Loss: 0.0105\n",
      "Epoch 72 -- Dev EER: 0.0208, Threshold: 0.0002\n",
      "✅ Training complete. Best EER: 0.0192\n",
      "Epoch 73/200, Loss: 0.0083\n",
      "Epoch 73 -- Dev EER: 0.0205, Threshold: 0.0009\n",
      "✅ Training complete. Best EER: 0.0192\n",
      "Epoch 74/200, Loss: 0.0059\n",
      "Epoch 74 -- Dev EER: 0.0204, Threshold: 0.0001\n",
      "✅ Training complete. Best EER: 0.0192\n",
      "Epoch 75/200, Loss: 0.0030\n",
      "Epoch 75 -- Dev EER: 0.0208, Threshold: 0.0002\n",
      "✅ Training complete. Best EER: 0.0192\n",
      "Epoch 76/200, Loss: 4.5636\n",
      "Epoch 76 -- Dev EER: 0.0287, Threshold: 0.0002\n",
      "✅ Training complete. Best EER: 0.0192\n",
      "Epoch 77/200, Loss: 0.8295\n",
      "Epoch 77 -- Dev EER: 0.0228, Threshold: 0.0011\n",
      "✅ Training complete. Best EER: 0.0192\n",
      "Epoch 78/200, Loss: 0.0822\n",
      "Epoch 78 -- Dev EER: 0.0231, Threshold: 0.0003\n",
      "✅ Training complete. Best EER: 0.0192\n",
      "Epoch 79/200, Loss: 0.0241\n",
      "Epoch 79 -- Dev EER: 0.0224, Threshold: 0.0003\n",
      "✅ Training complete. Best EER: 0.0192\n",
      "Epoch 80/200, Loss: 0.0212\n",
      "Epoch 80 -- Dev EER: 0.0231, Threshold: 0.0003\n",
      "✅ Training complete. Best EER: 0.0192\n",
      "Epoch 81/200, Loss: 0.0130\n",
      "Epoch 81 -- Dev EER: 0.0224, Threshold: 0.0002\n",
      "✅ Training complete. Best EER: 0.0192\n",
      "Epoch 82/200, Loss: 0.0132\n",
      "Epoch 82 -- Dev EER: 0.0197, Threshold: 0.0001\n",
      "✅ Training complete. Best EER: 0.0192\n",
      "Epoch 83/200, Loss: 0.0074\n",
      "Epoch 83 -- Dev EER: 0.0196, Threshold: 0.0002\n",
      "✅ Training complete. Best EER: 0.0192\n",
      "Epoch 84/200, Loss: 0.0063\n",
      "Epoch 84 -- Dev EER: 0.0215, Threshold: 0.0003\n",
      "✅ Training complete. Best EER: 0.0192\n",
      "Epoch 85/200, Loss: 0.0042\n",
      "Epoch 85 -- Dev EER: 0.0204, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0192\n",
      "Epoch 86/200, Loss: 0.0037\n",
      "Epoch 86 -- Dev EER: 0.0224, Threshold: 0.0026\n",
      "✅ Training complete. Best EER: 0.0192\n",
      "Epoch 87/200, Loss: 5.3818\n",
      "Epoch 87 -- Dev EER: 0.0208, Threshold: 0.0006\n",
      "✅ Training complete. Best EER: 0.0192\n",
      "Epoch 88/200, Loss: 0.1708\n",
      "Epoch 88 -- Dev EER: 0.0231, Threshold: 0.0006\n",
      "✅ Training complete. Best EER: 0.0192\n",
      "Epoch 89/200, Loss: 0.0645\n",
      "Epoch 89 -- Dev EER: 0.0228, Threshold: 0.0001\n",
      "✅ Training complete. Best EER: 0.0192\n",
      "Epoch 90/200, Loss: 0.0232\n",
      "Epoch 90 -- Dev EER: 0.0220, Threshold: 0.0004\n",
      "✅ Training complete. Best EER: 0.0192\n",
      "Epoch 91/200, Loss: 0.0183\n",
      "Epoch 91 -- Dev EER: 0.0219, Threshold: 0.0003\n",
      "✅ Training complete. Best EER: 0.0192\n",
      "Epoch 92/200, Loss: 0.0117\n",
      "Epoch 92 -- Dev EER: 0.0216, Threshold: 0.0002\n",
      "✅ Training complete. Best EER: 0.0192\n",
      "Epoch 93/200, Loss: 0.0091\n",
      "Epoch 93 -- Dev EER: 0.0208, Threshold: 0.0005\n",
      "✅ Training complete. Best EER: 0.0192\n",
      "Epoch 94/200, Loss: 0.0068\n",
      "Epoch 94 -- Dev EER: 0.0209, Threshold: 0.0002\n",
      "✅ Training complete. Best EER: 0.0192\n",
      "Epoch 95/200, Loss: 0.0067\n",
      "Epoch 95 -- Dev EER: 0.0204, Threshold: 0.0011\n",
      "✅ Training complete. Best EER: 0.0192\n",
      "Epoch 96/200, Loss: 0.0045\n",
      "Epoch 96 -- Dev EER: 0.0184, Threshold: 0.0001\n",
      "✅ Saved new best model with EER: 0.0184\n",
      "✅ Training complete. Best EER: 0.0184\n",
      "Epoch 97/200, Loss: 6.7086\n",
      "Epoch 97 -- Dev EER: 0.0224, Threshold: 0.0003\n",
      "✅ Training complete. Best EER: 0.0184\n",
      "Epoch 98/200, Loss: 0.9038\n",
      "Epoch 98 -- Dev EER: 0.0211, Threshold: 0.0004\n",
      "✅ Training complete. Best EER: 0.0184\n",
      "Epoch 99/200, Loss: 0.1178\n",
      "Epoch 99 -- Dev EER: 0.0213, Threshold: 0.0001\n",
      "✅ Training complete. Best EER: 0.0184\n",
      "Epoch 100/200, Loss: 0.0306\n",
      "Epoch 100 -- Dev EER: 0.0196, Threshold: 0.0003\n",
      "✅ Training complete. Best EER: 0.0184\n",
      "Epoch 101/200, Loss: 0.0226\n",
      "Epoch 101 -- Dev EER: 0.0196, Threshold: 0.0002\n",
      "✅ Training complete. Best EER: 0.0184\n",
      "Epoch 102/200, Loss: 0.0150\n",
      "Epoch 102 -- Dev EER: 0.0192, Threshold: 0.0003\n",
      "✅ Training complete. Best EER: 0.0184\n",
      "Epoch 103/200, Loss: 0.0114\n",
      "Epoch 103 -- Dev EER: 0.0184, Threshold: 0.0002\n",
      "✅ Training complete. Best EER: 0.0184\n",
      "Epoch 104/200, Loss: 0.0116\n",
      "Epoch 104 -- Dev EER: 0.0188, Threshold: 0.0002\n",
      "✅ Training complete. Best EER: 0.0184\n",
      "Epoch 105/200, Loss: 0.0068\n",
      "Epoch 105 -- Dev EER: 0.0176, Threshold: 0.0002\n",
      "✅ Saved new best model with EER: 0.0176\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 106/200, Loss: 0.0050\n",
      "Epoch 106 -- Dev EER: 0.0177, Threshold: 0.0003\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 107/200, Loss: 0.0052\n",
      "Epoch 107 -- Dev EER: 0.0177, Threshold: 0.0001\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 108/200, Loss: 2.7309\n",
      "Epoch 108 -- Dev EER: 0.0372, Threshold: 0.0019\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 109/200, Loss: 1.1622\n",
      "Epoch 109 -- Dev EER: 0.0220, Threshold: 0.0002\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 110/200, Loss: 0.0489\n",
      "Epoch 110 -- Dev EER: 0.0215, Threshold: 0.0001\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 111/200, Loss: 0.0330\n",
      "Epoch 111 -- Dev EER: 0.0215, Threshold: 0.0005\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 112/200, Loss: 0.0145\n",
      "Epoch 112 -- Dev EER: 0.0212, Threshold: 0.0002\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 113/200, Loss: 0.0105\n",
      "Epoch 113 -- Dev EER: 0.0204, Threshold: 0.0003\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 114/200, Loss: 0.0106\n",
      "Epoch 114 -- Dev EER: 0.0189, Threshold: 0.0007\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 115/200, Loss: 0.0070\n",
      "Epoch 115 -- Dev EER: 0.0188, Threshold: 0.0002\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 116/200, Loss: 0.0050\n",
      "Epoch 116 -- Dev EER: 0.0200, Threshold: 0.0002\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 117/200, Loss: 0.0035\n",
      "Epoch 117 -- Dev EER: 0.0188, Threshold: 0.0001\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 118/200, Loss: 0.0031\n",
      "Epoch 118 -- Dev EER: 0.0185, Threshold: 0.0003\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 119/200, Loss: 6.8875\n",
      "Epoch 119 -- Dev EER: 0.0220, Threshold: 0.0522\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 120/200, Loss: 0.4754\n",
      "Epoch 120 -- Dev EER: 0.0196, Threshold: 0.0003\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 121/200, Loss: 0.0325\n",
      "Epoch 121 -- Dev EER: 0.0196, Threshold: 0.0006\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 122/200, Loss: 0.0219\n",
      "Epoch 122 -- Dev EER: 0.0195, Threshold: 0.0002\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 123/200, Loss: 0.0155\n",
      "Epoch 123 -- Dev EER: 0.0192, Threshold: 0.0002\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 124/200, Loss: 0.0092\n",
      "Epoch 124 -- Dev EER: 0.0193, Threshold: 0.0001\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 125/200, Loss: 0.0138\n",
      "Epoch 125 -- Dev EER: 0.0184, Threshold: 0.0004\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 126/200, Loss: 3.6875\n",
      "Epoch 126 -- Dev EER: 0.0231, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 127/200, Loss: 0.1436\n",
      "Epoch 127 -- Dev EER: 0.0223, Threshold: 0.0002\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 128/200, Loss: 0.0193\n",
      "Epoch 128 -- Dev EER: 0.0235, Threshold: 0.0001\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 129/200, Loss: 0.0160\n",
      "Epoch 129 -- Dev EER: 0.0220, Threshold: 0.0003\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 130/200, Loss: 0.0091\n",
      "Epoch 130 -- Dev EER: 0.0224, Threshold: 0.0001\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 131/200, Loss: 0.0085\n",
      "Epoch 131 -- Dev EER: 0.0224, Threshold: 0.0002\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 132/200, Loss: 0.0057\n",
      "Epoch 132 -- Dev EER: 0.0228, Threshold: 0.0001\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 133/200, Loss: 0.0048\n",
      "Epoch 133 -- Dev EER: 0.0220, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 134/200, Loss: 0.0066\n",
      "Epoch 134 -- Dev EER: 0.0216, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 135/200, Loss: 0.0023\n",
      "Epoch 135 -- Dev EER: 0.0219, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 136/200, Loss: 0.0021\n",
      "Epoch 136 -- Dev EER: 0.0200, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 137/200, Loss: 1.4748\n",
      "Epoch 137 -- Dev EER: 0.0267, Threshold: 0.0002\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 138/200, Loss: 2.4580\n",
      "Epoch 138 -- Dev EER: 0.0204, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 139/200, Loss: 0.0709\n",
      "Epoch 139 -- Dev EER: 0.0193, Threshold: 0.0005\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 140/200, Loss: 0.0172\n",
      "Epoch 140 -- Dev EER: 0.0184, Threshold: 0.0001\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 141/200, Loss: 0.0109\n",
      "Epoch 141 -- Dev EER: 0.0181, Threshold: 0.0003\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 142/200, Loss: 0.0114\n",
      "Epoch 142 -- Dev EER: 0.0196, Threshold: 0.0001\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 143/200, Loss: 0.0074\n",
      "Epoch 143 -- Dev EER: 0.0185, Threshold: 0.0001\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 144/200, Loss: 0.0056\n",
      "Epoch 144 -- Dev EER: 0.0184, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 145/200, Loss: 0.0035\n",
      "Epoch 145 -- Dev EER: 0.0192, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 146/200, Loss: 0.0029\n",
      "Epoch 146 -- Dev EER: 0.0188, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 147/200, Loss: 0.0024\n",
      "Epoch 147 -- Dev EER: 0.0188, Threshold: 0.0001\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 148/200, Loss: 0.0025\n",
      "Epoch 148 -- Dev EER: 0.0180, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 149/200, Loss: 0.0012\n",
      "Epoch 149 -- Dev EER: 0.0184, Threshold: 0.0001\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 150/200, Loss: 0.0007\n",
      "Epoch 150 -- Dev EER: 0.0184, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 151/200, Loss: 0.0007\n",
      "Epoch 151 -- Dev EER: 0.0181, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 152/200, Loss: 0.0004\n",
      "Epoch 152 -- Dev EER: 0.0180, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 153/200, Loss: 2.8822\n",
      "Epoch 153 -- Dev EER: 0.0267, Threshold: 0.0004\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 154/200, Loss: 0.0522\n",
      "Epoch 154 -- Dev EER: 0.0220, Threshold: 0.0001\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 155/200, Loss: 0.0141\n",
      "Epoch 155 -- Dev EER: 0.0219, Threshold: 0.0001\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 156/200, Loss: 0.0118\n",
      "Epoch 156 -- Dev EER: 0.0209, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 157/200, Loss: 0.0081\n",
      "Epoch 157 -- Dev EER: 0.0215, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 158/200, Loss: 0.0072\n",
      "Epoch 158 -- Dev EER: 0.0196, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 159/200, Loss: 0.0036\n",
      "Epoch 159 -- Dev EER: 0.0200, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 160/200, Loss: 0.0024\n",
      "Epoch 160 -- Dev EER: 0.0196, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 161/200, Loss: 0.0067\n",
      "Epoch 161 -- Dev EER: 0.0231, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 162/200, Loss: 2.7363\n",
      "Epoch 162 -- Dev EER: 0.0236, Threshold: 0.0002\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 163/200, Loss: 0.3239\n",
      "Epoch 163 -- Dev EER: 0.0216, Threshold: 0.0001\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 164/200, Loss: 0.0321\n",
      "Epoch 164 -- Dev EER: 0.0207, Threshold: 0.0001\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 165/200, Loss: 0.0101\n",
      "Epoch 165 -- Dev EER: 0.0200, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 166/200, Loss: 0.0073\n",
      "Epoch 166 -- Dev EER: 0.0203, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 167/200, Loss: 0.0071\n",
      "Epoch 167 -- Dev EER: 0.0192, Threshold: 0.0002\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 168/200, Loss: 0.0037\n",
      "Epoch 168 -- Dev EER: 0.0200, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 169/200, Loss: 0.0032\n",
      "Epoch 169 -- Dev EER: 0.0200, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 170/200, Loss: 0.0018\n",
      "Epoch 170 -- Dev EER: 0.0197, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 171/200, Loss: 0.0016\n",
      "Epoch 171 -- Dev EER: 0.0200, Threshold: 0.0001\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 172/200, Loss: 0.0009\n",
      "Epoch 172 -- Dev EER: 0.0204, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 173/200, Loss: 0.7826\n",
      "Epoch 173 -- Dev EER: 0.0415, Threshold: 0.0001\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 174/200, Loss: 3.9880\n",
      "Epoch 174 -- Dev EER: 0.0283, Threshold: 0.0003\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 175/200, Loss: 0.0789\n",
      "Epoch 175 -- Dev EER: 0.0240, Threshold: 0.0001\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 176/200, Loss: 0.0270\n",
      "Epoch 176 -- Dev EER: 0.0231, Threshold: 0.0003\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 177/200, Loss: 0.0216\n",
      "Epoch 177 -- Dev EER: 0.0255, Threshold: 0.0001\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 178/200, Loss: 0.0128\n",
      "Epoch 178 -- Dev EER: 0.0224, Threshold: 0.0002\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 179/200, Loss: 0.0152\n",
      "Epoch 179 -- Dev EER: 0.0228, Threshold: 0.0001\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 180/200, Loss: 0.0047\n",
      "Epoch 180 -- Dev EER: 0.0220, Threshold: 0.0001\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 181/200, Loss: 0.0040\n",
      "Epoch 181 -- Dev EER: 0.0215, Threshold: 0.0001\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 182/200, Loss: 0.0030\n",
      "Epoch 182 -- Dev EER: 0.0224, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 183/200, Loss: 0.0024\n",
      "Epoch 183 -- Dev EER: 0.0224, Threshold: 0.0001\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 184/200, Loss: 0.0022\n",
      "Epoch 184 -- Dev EER: 0.0209, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 185/200, Loss: 0.0019\n",
      "Epoch 185 -- Dev EER: 0.0193, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 186/200, Loss: 0.0010\n",
      "Epoch 186 -- Dev EER: 0.0200, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 187/200, Loss: 0.0005\n",
      "Epoch 187 -- Dev EER: 0.0200, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 188/200, Loss: 0.0005\n",
      "Epoch 188 -- Dev EER: 0.0197, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 189/200, Loss: 1.7490\n",
      "Epoch 189 -- Dev EER: 0.0345, Threshold: 0.1173\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 190/200, Loss: 0.9950\n",
      "Epoch 190 -- Dev EER: 0.0243, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 191/200, Loss: 0.0576\n",
      "Epoch 191 -- Dev EER: 0.0247, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 192/200, Loss: 0.0117\n",
      "Epoch 192 -- Dev EER: 0.0216, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 193/200, Loss: 0.0062\n",
      "Epoch 193 -- Dev EER: 0.0220, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 194/200, Loss: 0.0026\n",
      "Epoch 194 -- Dev EER: 0.0224, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 195/200, Loss: 0.0022\n",
      "Epoch 195 -- Dev EER: 0.0221, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 196/200, Loss: 0.0025\n",
      "Epoch 196 -- Dev EER: 0.0220, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 197/200, Loss: 0.0020\n",
      "Epoch 197 -- Dev EER: 0.0216, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 198/200, Loss: 0.0011\n",
      "Epoch 198 -- Dev EER: 0.0216, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 199/200, Loss: 0.0011\n",
      "Epoch 199 -- Dev EER: 0.0216, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n",
      "Epoch 200/200, Loss: 0.0008\n",
      "Epoch 200 -- Dev EER: 0.0208, Threshold: 0.0000\n",
      "✅ Training complete. Best EER: 0.0176\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>dev_eer</td><td>▄▃▃▂▄▂▂▂▂▂▂▄▂▂▃▂▁█▃▂▂▂▁▁▂▁▁▂▁▁▁▁▂▂█▂▂▂▂▂</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇██</td></tr><tr><td>loss</td><td>█▄▄▃▃▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▂▂▁▁▃▁▁▂▂▂▁▁▁▁▁▂▂▁▁</td></tr><tr><td>threshold</td><td>█▂▅▄▆▅▄▃▆▅▅▁▅▅▃▂▂▁▂▂▁▂▄▂▂▂▂▁▁▁▂▁▁▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>dev_eer</td><td>0.02081</td></tr><tr><td>epoch</td><td>200</td></tr><tr><td>loss</td><td>0.00078</td></tr><tr><td>threshold</td><td>1e-05</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cnn-lr1e-03-bs32-ep200-20250623-0613</strong> at: <a href='https://wandb.ai/qianqianrumail-university-of-stuttgart/asvspoof-baseline/runs/d7m2d3wh' target=\"_blank\">https://wandb.ai/qianqianrumail-university-of-stuttgart/asvspoof-baseline/runs/d7m2d3wh</a><br> View project at: <a href='https://wandb.ai/qianqianrumail-university-of-stuttgart/asvspoof-baseline' target=\"_blank\">https://wandb.ai/qianqianrumail-university-of-stuttgart/asvspoof-baseline</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250623_061341-d7m2d3wh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_model_path = train_model(X_train_final, y_train_final, X_dev_final, y_dev_final, run_name, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "23194773-14cf-4079-a1c6-92e906326096",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNSpoofDetector().to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(TensorDataset(X_train_final, y_train_final), batch_size=32, shuffle=True)\n",
    "dev_loader = DataLoader(TensorDataset(X_dev_final, y_dev_final), batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e305aec6-2105-4e49-8b9b-09f08c18b392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the CNN model and move it to the appropriate device (CPU or GPU)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Use GPU if available, otherwise fallback to CPU\n",
    "# model = CNNSpoofDetector().to(device)\n",
    "\n",
    "# # Define the loss function: binary cross-entropy with logits (for 0 vs. 1 classification)\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# # Use Adam optimizer for model training\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "# best_eer = float('inf')\n",
    "# best_model_path = 'best_model.pth'\n",
    "\n",
    "# # Loop over the number of training epochs\n",
    "# num_epochs = 5\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()  # Set the model to training mode\n",
    "#     running_loss = 0.0  # Accumulate loss over batches\n",
    "\n",
    "#     for X_batch, y_batch in train_loader:\n",
    "#         # Move data to device\n",
    "#         X_batch, y_batch = X_batch.to(device), y_batch.to(device).float()\n",
    "\n",
    "#         optimizer.zero_grad()           # Clear gradients\n",
    "#         outputs = model(X_batch).squeeze()  # Forward pass and remove singleton dimension\n",
    "#         loss = criterion(outputs, y_batch)  # Compute binary cross-entropy loss\n",
    "#         loss.backward()                # Backpropagate\n",
    "#         optimizer.step()               # Update model weights\n",
    "\n",
    "#         running_loss += loss.item()    # Accumulate batch loss\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss:.4f}\")\n",
    "\n",
    "#     # ------------------ Evaluate EER on development set ------------------\n",
    "#     model.eval()  # Set model to evaluation mode\n",
    "#     all_outputs = []\n",
    "#     all_labels = []\n",
    "\n",
    "#     with torch.no_grad():  # No gradient calculation needed during evaluation\n",
    "#         for X_dev, y_dev in dev_loader:\n",
    "#             X_dev, y_dev = X_dev.to(device), y_dev.to(device).float()\n",
    "#             outputs = model(X_dev).squeeze()  # Forward pass\n",
    "#             all_outputs.extend(outputs.cpu().numpy())  # Save outputs\n",
    "#             all_labels.extend(y_dev.cpu().numpy())     # Save true labels\n",
    "\n",
    "#     all_outputs = np.array(all_outputs)\n",
    "#     all_labels = np.array(all_labels)\n",
    "#     all_probs = 1 / (1 + np.exp(-all_outputs))  # Sigmoid\n",
    "#     bonafide_probs = all_probs[all_labels == 1]\n",
    "#     spoof_probs    = all_probs[all_labels == 0]\n",
    "    \n",
    "\n",
    "#     # Compute Equal Error Rate and the threshold that achieves it\n",
    "#     eer, threshold = compute_eer(np.array(bonafide_probs), np.array(spoof_probs))\n",
    "#     print(f\"Epoch {epoch+1} -- Dev EER: {eer:.4f}, Threshold: {threshold:.4f}\")\n",
    "\n",
    "#     if eer < best_eer:\n",
    "#         best_eer = eer\n",
    "#         torch.save(model.state_dict(), best_model_path)\n",
    "#         print(f\"✅ Saved new best model with EER: {eer:.4f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3427ef6-8c05-401c-803c-7590ed54dad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3953488/307811798.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final evaluation -- EER: 0.0176, Threshold: 0.0002\n"
     ]
    }
   ],
   "source": [
    "model = CNNSpoofDetector().to(device)\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.eval()\n",
    "\n",
    "all_outputs, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for X_dev, y_dev in dev_loader:\n",
    "        X_dev, y_dev = X_dev.to(device), y_dev.to(device).float()\n",
    "        outputs = model(X_dev).squeeze()\n",
    "        all_outputs.extend(outputs.cpu().numpy())\n",
    "        all_labels.extend(y_dev.cpu().numpy())\n",
    "\n",
    "all_outputs = np.array(all_outputs)\n",
    "all_labels = np.array(all_labels)\n",
    "all_probs = 1 / (1 + np.exp(-all_outputs))  # Sigmoid\n",
    "bonafide_probs = all_probs[all_labels == 1]\n",
    "spoof_probs = all_probs[all_labels == 0]\n",
    "\n",
    "eer, threshold = compute_eer(np.array(bonafide_probs), np.array(spoof_probs))\n",
    "print(f\"Final evaluation -- EER: {eer:.4f}, Threshold: {threshold:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ee578c69-2e30-4628-8c77-68de24becaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_eer_0341_path = 'cnn_mfcc_eer_0.0341_0617.pth'\n",
    "# torch.save(model.state_dict(), model_eer_0341_path)\n",
    "# print(f\"✅ model has saved as {model_eer_0341_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b55ed118-fd8c-48bb-b234-0c018885fc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      0.99     22296\n",
      "         1.0       0.99      0.87      0.93      2548\n",
      "\n",
      "    accuracy                           0.99     24844\n",
      "   macro avg       0.99      0.94      0.96     24844\n",
      "weighted avg       0.99      0.99      0.99     24844\n",
      "\n",
      "Confusion Matrix:\n",
      "[[22279    17]\n",
      " [  326  2222]]\n"
     ]
    }
   ],
   "source": [
    "# Turn logits into binary predictions\n",
    "y_pred = (np.array(all_outputs) > threshold).astype(int)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(all_labels, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70122f6-67db-4f3c-be8e-ac088a2fdc79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
